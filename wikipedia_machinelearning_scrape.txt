"Machine learning is a subfield of computer science[1] (more particularly soft computing and granular computing) that evolved from the study of pattern recognition, computational intelligence, and computational learning theory in artificial intelligence.[1] In 1959, Arthur Samuel defined machine learning as a ""Field of study that gives computers the ability to learn without being explicitly programmed"".[2] Machine learning explores the study and construction of algorithms that can learn from and perform predictive analysis on data.[3] Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs,[4]:2 rather than following strictly static program instructions.","Machine learning is closely related to (and often overlaps with) computational statistics; a discipline which also focuses in prediction-making through the use of computers. It has strong ties to mathematical optimization, matrix theory, linear algebra, and copulas, which delivers methods, theory and application domains to the field. Machine learning is employed in a range of computing tasks where designing and programming explicit algorithms is unfeasible. Example applications include spam filtering, optical character recognition (OCR),[5] search engines and computer vision. Machine learning is sometimes conflated with data mining,[6] where the latter sub-field focuses more on exploratory data analysis and is known as unsupervised learning.[4]:vii[7]","Within the field of data analytics, machine learning is a method used to devise complex models and algorithms that lend themselves to prediction - in commercial use, this is known as predictive analytics. These analytical models allow researchers, data scientists, engineers, and analysts to ""produce reliable, repeatable decisions and results"" and uncover ""hidden insights"" through learning from historical relationships and trends in the data.[8]",,,"Tom M. Mitchell provided a widely quoted, more formal definition: ""A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E.""[9] This definition is notable for its defining machine learning in fundamentally operational rather than cognitive terms, thus following Alan Turing's proposal in his paper ""Computing Machinery and Intelligence"" that the question ""Can machines think?"" be replaced with the question ""Can machines do what we (as thinking entities) can do?""[10]",,"Machine learning tasks are typically classified into three broad categories, depending on the nature of the learning ""signal"" or ""feedback"" available to a learning system. These are[11]","Between supervised and unsupervised learning is semi-supervised learning, where the teacher gives an incomplete training signal: a training set with some (often many) of the target outputs missing. Transduction is a special case of this principle where the entire set of problem instances is known at learning time, except that part of the targets are missing.","Among other categories of machine learning problems, learning to learn learns its own inductive bias based on previous experience. Developmental learning, elaborated for robot learning, generates its own sequences (also called curriculum) of learning situations to cumulatively acquire repertoires of novel skills through autonomous self-exploration and social interaction with human teachers and using guidance mechanisms such as active learning, maturation, motor synergies, and imitation.",Another categorization of machine learning tasks arises when one considers the desired output of a machine-learned system:[4]:3,"As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed ""neural networks""; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.[11]:488","However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[11]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[12] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[11]:708–710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as ""connectionism"", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[11]:25","Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[12] It also benefited from the increasing availability of digitized information, and the possibility to distribute that via the Internet.",Machine learning and data mining often employ the same methods and overlap significantly. They can be roughly distinguished as follows:,"The two areas overlap in many ways: data mining uses many machine learning methods, but often with a slightly different goal in mind. On the other hand, machine learning also employs data mining methods as ""unsupervised learning"" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in Knowledge Discovery and Data Mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.","Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[13]","Machine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[14] He also suggested the term data science as a placeholder to call the overall field.[14]","Leo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model,[15] wherein 'algorithmic model' means more or less the machine learning algorithms like Random forest.","Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[16]","A core objective of a learner is to generalize from its experience.[17][18] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.","The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.","For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[19]","In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.","Decision tree learning uses a decision tree as a predictive model, which maps observations about an item to conclusions about the item's target value.",Association rule learning is a method for discovering interesting relations between variables in large databases.,"An artificial neural network (ANN) learning algorithm, usually called ""neural network"" (NN), is a learning algorithm that is inspired by the structure and functional aspects of biological neural networks. Computations are structured in terms of an interconnected group of artificial neurons, processing information using a connectionist approach to computation. Modern neural networks are non-linear statistical data modeling tools. They are usually used to model complex relationships between inputs and outputs, to find patterns in data, or to capture the statistical structure in an unknown joint probability distribution between observed variables.",Falling hardware prices and the development of GPUs for personal use in the last few years have contributed to the development of the concept of Deep learning which consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[20],"Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.","Support vector machines (SVMs) are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.","Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to some predesignated criterion or criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated for example by internal compactness (similarity between members of the same cluster) and separation between different clusters. Other methods are based on estimated density and graph connectivity. Clustering is a method of unsupervised learning, and a common technique for statistical data analysis.","A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independencies via a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning.","Reinforcement learning is concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. Reinforcement learning algorithms attempt to find a policy that maps states of the world to the actions the agent ought to take in those states. Reinforcement learning differs from the supervised learning problem in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected.","Several learning algorithms, mostly unsupervised learning algorithms, aim at discovering better representations of the inputs provided during training. Classical examples include principal components analysis and cluster analysis. Representation learning algorithms often attempt to preserve the information in their input but transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions, allowing to reconstruct the inputs coming from the unknown data generating distribution, while not being necessarily faithful for configurations that are implausible under that distribution.","Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse (has many zeros). Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into (high-dimensional) vectors.[21] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[22]","In this problem, the learning machine is given pairs of examples that are considered similar and pairs of less similar objects. It then needs to learn a similarity function (or a distance metric function) that can predict if new objects are similar. It is sometimes used in Recommendation systems.","In this method, a datum is represented as a linear combination of basis functions, and the coefficients are assumed to be sparse. Let x be a d-dimensional datum, D be a d by n matrix, where each column of D represents a basis function. r is the coefficient to represent x using D. Mathematically, sparse dictionary learning means solving 



x
≈
D
r


{\displaystyle x\approx Dr}

 where r is sparse. Generally speaking, n is assumed to be larger than d to allow the freedom for a sparse representation.",Learning a dictionary along with sparse representations is strongly NP-hard and also difficult to solve approximately.[23] A popular heuristic method for sparse dictionary learning is K-SVD.,"Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine which classes a previously unseen datum belongs to. Suppose a dictionary for each class has already been built. Then a new datum is associated with the class such that it's best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[24]","A genetic algorithm (GA) is a search heuristic that mimics the process of natural selection, and uses methods such as mutation and crossover to generate new genotype in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms found some uses in the 1980s and 1990s.[25][26] Vice versa, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[27]",Applications for machine learning include:,"In 2006, the online movie company Netflix held the first ""Netflix Prize"" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[28] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (""everything is a recommendation"") and they changed their recommendation engine accordingly.[29]",In 2010 The Wall Street Journal wrote about money management firm Rebellion Research's use of machine learning to predict economic movements. The article describes Rebellion Research's prediction of the financial crisis and economic recovery.[30],"In 2014 it has been reported that a machine learning algorithm has been applied in Art History to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.[31]","Machine Learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use, thus digitizing cultural prejudices such as institutional racism and classism.[32] Responsible collection of data thus is a critical part of machine learning. See Machine ethics for additional information.",Software suites containing a variety of machine learning algorithms include the following:
Data mining facilities are included in some of the Category:Data analysis software and Category:Statistical software products.,"This category has the following 8 subcategories, out of 8 total.
","The following 48 pages are in this category, out of 48 total. This list may not reflect recent changes (learn more).
"
"This category has the following 29 subcategories, out of 29 total.
","The following 200 pages are in this category, out of 284 total. This list may not reflect recent changes (learn more).
"
"Decision theory is the study of optimal actions, as determined by considering the probability and utility of different outcomes.","This category has the following 16 subcategories, out of 16 total.
","The following 200 pages are in this category, out of 222 total. This list may not reflect recent changes (learn more).
"
"The following 40 pages are in this category, out of 40 total. This list may not reflect recent changes (learn more).
"
"The following 32 pages are in this category, out of 32 total. This list may not reflect recent changes (learn more).
"
This category are for articles about artificial neural networks (ANN).,"This category has the following 2 subcategories, out of 2 total.
","The following 123 pages are in this category, out of 123 total. This list may not reflect recent changes (learn more).
"
"The following 11 pages are in this category, out of 11 total. This list may not reflect recent changes (learn more).
"
"This category is about statistical classification algorithms. For more information, see Statistical classification.","This category has the following 3 subcategories, out of 3 total.
","The following 80 pages are in this category, out of 80 total. This list may not reflect recent changes (learn more).
"
"This category has the following 2 subcategories, out of 2 total.
","The following 16 pages are in this category, out of 16 total. This list may not reflect recent changes (learn more).
"
"The following 20 pages are in this category, out of 20 total. This list may not reflect recent changes (learn more).
"
"Academic conferences related to artificial intelligence, machine learning and pattern recognition.","The following 14 pages are in this category, out of 14 total. This list may not reflect recent changes (learn more).
"
Some products in Category:Data analysis software and Category:Statistical software also include data mining and machine learning facilities.,"This category has only the following subcategory.
","The following 79 pages are in this category, out of 79 total. This list may not reflect recent changes (learn more).
"
"This category has only the following subcategory.
","The following 4 pages are in this category, out of 4 total. This list may not reflect recent changes (learn more).
"
"The following 13 pages are in this category, out of 13 total. This list may not reflect recent changes (learn more).
"
"The following 24 pages are in this category, out of 24 total. This list may not reflect recent changes (learn more).
"
"Ensemble learning is a type of machine learning that studies algorithms and architectures that build collections, or ensembles, of statistical classifiers that are more accurate than a single classifier.","The following 14 pages are in this category, out of 14 total. This list may not reflect recent changes (learn more).
"
"An evolutionary algorithm (EA) is a heuristic optimization algorithm using techniques inspired by mechanisms from organic evolution such as mutation, recombination, and natural selection to find an optimal configuration for a specific system within specific constraints.","This category has the following 4 subcategories, out of 4 total.
","The following 40 pages are in this category, out of 40 total. This list may not reflect recent changes (learn more).
","The following 2 files are in this category, out of 2 total.
"
"The following 5 pages are in this category, out of 5 total. This list may not reflect recent changes (learn more).
"
This page lists categories and articles related to kernel methods for machine learning.,"This category has only the following subcategory.
","The following 15 pages are in this category, out of 15 total. This list may not reflect recent changes (learn more).
"
"This category has only the following subcategory.
","The following 25 pages are in this category, out of 25 total. This list may not reflect recent changes (learn more).
"
"Learning-based methods in computer vision make use of training data to build systems for visual analysis. For example, one may train a system for detecting faces using training images of faces. Training data is often given in the forms of image or video collections, together with target labels. Such data is often fed into a machine learning algorithm, that will learn to predict such labels given novel images or video. Learning-based methods have been used for a variety of computer vision tasks, including low-level problems such as image-denoising, and high-level tasks such as object recognition and scene classification.","The following 4 pages are in this category, out of 4 total. This list may not reflect recent changes (learn more).
"
"The following 7 pages are in this category, out of 7 total. This list may not reflect recent changes (learn more).
"
"The following 8 pages are in this category, out of 8 total. This list may not reflect recent changes (learn more).
"
"This category has only the following subcategory.
","The following 54 pages are in this category, out of 54 total. This list may not reflect recent changes (learn more).
"
"This portal navigation can be placed on articles, and is commonly placed in the See also section of an article on Wikipedia, using the following code:","The following 2 pages are in this category, out of 2 total. This list may not reflect recent changes (learn more).
"
Category for machine learning tasks,"The following 5 pages are in this category, out of 5 total. This list may not reflect recent changes (learn more).
"
"This category has the following 2 subcategories, out of 2 total.
","The following 51 pages are in this category, out of 51 total. This list may not reflect recent changes (learn more).
"
Researchers who study machine learning.,"The following 66 pages are in this category, out of 66 total. This list may not reflect recent changes (learn more).
"
"This category has only the following subcategory.
","The following 35 pages are in this category, out of 35 total. This list may not reflect recent changes (learn more).
"
"This category has only the following subcategory.
","The following 3 pages are in this category, out of 3 total. This list may not reflect recent changes (learn more).
"
"The following 8 pages are in this category, out of 8 total. This list may not reflect recent changes (learn more).
"
The accuracy paradox for predictive analytics states that predictive models with a given level of accuracy may have greater predictive power than models with higher accuracy. It may be better to avoid the accuracy metric in favor of other metrics such as precision and recall. [1],"Accuracy is often the starting point for analyzing the quality of a predictive model, as well as an obvious criterion for prediction. Accuracy measures the ratio of correct predictions to the total number of cases evaluated. It may seem obvious that the ratio of correct predictions to cases should be a key metric. A predictive model may have high accuracy, but be useless.","In an example predictive model for an insurance fraud application, all cases that are predicted as high-risk by the model will be investigated. To evaluate the performance of the model, the insurance company has created a sample data set of 10,000 claims. All 10,000 cases in the validation sample have been carefully checked and it is known which cases are fraudulent. To analyze the quality of the model, the insurance uses the table of confusion. The definition of accuracy, the table of confusion for model M1Fraud, and the calculation of accuracy for model M1Fraud is shown below.","




A

(
M
)
=



T
N
+
T
P


T
N
+
F
P
+
F
N
+
T
P





{\displaystyle \mathrm {A} (M)={\frac {TN+TP}{TN+FP+FN+TP}}}

 where",Formula 1: Definition of Accuracy,Table 1: Table of Confusion for Fraud Model M1Fraud.,"




A

(
M
)
=



9
,
700
+
100


9
,
700
+
150
+
50
+
100



=
98.0
%


{\displaystyle \mathrm {A} (M)={\frac {9,700+100}{9,700+150+50+100}}=98.0\%}

",Formula 2: Accuracy for model M1Fraud,"With an accuracy of 98.0% model M1Fraud appears to perform fairly well. The paradox lies in the fact that accuracy can be easily improved to 98.5% by always predicting ""no fraud"". The table of confusion and the accuracy for this trivial “always predict negative” model M2Fraud and the accuracy of this model are shown below.",Table 2: Table of Confusion for Fraud Model M2Fraud.,"




A

(
M
)
=



9
,
850
+
0


9
,
850
+
150
+
0
+
0



=
98.5
%


{\displaystyle \mathrm {A} (M)={\frac {9,850+0}{9,850+150+0+0}}=98.5\%}

",Formula 3: Accuracy for model M2Fraud,"Model M2Fraudreduces the rate of inaccurate predictions from 2% to 1.5%. This is an apparent improvement of 25%. The new model M2Fraud shows fewer incorrect predictions and markedly improved accuracy, as compared to the original model M1Fraud, but is obviously useless.",The alternative model M2Fraud does not offer any value to the company for preventing fraud. The less accurate model is more useful than the more accurate model.,"Model improvements should not be measured in terms of accuracy gains. It may be going too far to say that accuracy is irrelevant, but caution is advised when using accuracy in the evaluation of predictive models."
Action model learning (sometimes abbreviated action learning) is an area of machine learning concerned with creation and modification of software agent's knowledge about effects and preconditions of the actions that can be executed within its environment. This knowledge is usually represented in logic-based action description language and used as the input for automated planners.,"Learning action models is important when goals change. When an agent acted for a while, it can use its accumulated knowledge about actions in the domain to make better decisions. Thus, learning action models differs from reinforcement learning. It enables reasoning about actions instead of expensive trials in the world.[1] Action model learning is a form of inductive reasoning, where new knowledge is generated based on agent's observations. It differs from standard supervised learning in that correct input/output pairs are never presented, nor imprecise action models explicitly corrected.","Usual motivation for action model learning is the fact that manual specification of action models for planners is often a difficult, time consuming, and error-prone task (especially in complex environments).",,,"Given a training set 



E


{\displaystyle E}

 consisting of examples 



e
=
(
s
,
a
,

s
′

)


{\displaystyle e=(s,a,s')}

, where 



s
,

s
′



{\displaystyle s,s'}

 are observations of a world state from two consecutive time steps 



t
,

t
′



{\displaystyle t,t'}

 and 



a


{\displaystyle a}

 is an action instance observed in time step 



t


{\displaystyle t}

, the goal of action model learning in general is to construct an action model 



⟨
D
,
P
⟩


{\displaystyle \langle D,P\rangle }

, where 



D


{\displaystyle D}

 is a description of domain dynamics in action description formalism like STRIPS, ADL or PDDL and 



P


{\displaystyle P}

 is a probability function defined over the elements of 



D


{\displaystyle D}

. [2] However, many state of the art action learning methods assume determinism and do not induce 



P


{\displaystyle P}

. In addition to determinism, individual methods differ in how they deal with other attributes of domain (e.g. partial observability or sensoric noise).","Recent action learning methods take various approaches and employ a wide variety of tools from different areas of artificial intelligence and computational logic. As an example of a method based on propositional logic, we can mention SLAF (Simultaneous Learning and Filtering) algorithm,[1] which uses agent's observations to construct a long propositional formula over time and subsequently interprets it using a satisfiability (SAT) solver. Another technique, in which learning is converted into a satisfiability problem (weighted MAX-SAT in this case) and SAT solvers are used, is implemented in ARMS (Action-Relation Modeling System).[3] Two mutually similar, fully declarative approaches to action learning were based on logic programming paradigm Answer Set Programming (ASP)[4] and its extension, Reactive ASP.[5] In another example, bottom-up inductive logic programming approach was employed.[6] Several different solutions are not directly logic-based. For example, the action model learning using a perceptron algorithm [7] or the multi level greedy search over the space of possible action models.[8] In the older paper from 1992,[9] the action model learning was studied as an extension of reinforcement learning.","Most action learning research papers are published in journals and conferences focused on artificial intelligence in general (e.g. Journal of Artificial Intelligence Research (JAIR), Artificial Intelligence, Applied Artificial Intelligence (AAI) or AAAI conferences). Despite mutual relevance of the topics, action model learning is usually not addressed on planning conferences like ICAPS."
Active learning is a special case of semi-supervised machine learning in which a learning algorithm is able to interactively query the user (or some other information source) to obtain the desired outputs at new data points.[1] [2] In statistics literature it is sometimes also called optimal experimental design. [3],"There are situations in which unlabeled data is abundant but manually labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm be overwhelmed by uninformative examples. Recent developments are dedicated to hybrid active learning[4] and active learning in a single-pass (on-line) context,[5] combining concepts from the field of Machine Learning (e.g., conflict and ignorance) with adaptive, incremental learning policies in the field of Online machine learning.",,,"Let 



T


{\displaystyle T}

 be the total set of all data under consideration. For example, in a protein engineering problem, 



T


{\displaystyle T}

 would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity.","During each iteration, 



i


{\displaystyle i}

, 



T


{\displaystyle T}

 is broken up into three subsets","Most of the current research in active learning involves the best method to choose the data points for 




T

C
,
i




{\displaystyle T_{C,i}}

.",Algorithms for determining which data points should be labeled can be organized into a number of different categories:[1],A wide variety of algorithms have been studied that fall into these categories.[1][3],"Some active learning algorithms are built upon Support vector machines (SVMs) and exploit the structure of the SVM to determine which data points to label. Such methods usually calculate the margin, 



W


{\displaystyle W}

, of each unlabeled datum in 




T

U
,
i




{\displaystyle T_{U,i}}

 and treat 



W


{\displaystyle W}

 as an 



n


{\displaystyle n}

-dimensional distance from that datum to the separating hyperplane.","Minimum Marginal Hyperplane methods assume that the data with the smallest 



W


{\displaystyle W}

 are those that the SVM is most uncertain about and therefore should be placed in 




T

C
,
i




{\displaystyle T_{C,i}}

 to be labeled. Other similar methods, such as Maximum Marginal Hyperplane, choose data with the largest 



W


{\displaystyle W}

. Tradeoff methods choose a mix of the smallest and largest 



W


{\displaystyle W}

s.",
"Adversarial machine learning is a research field that lies at the intersection of machine learning and computer security. It aims to enable the safe adoption of machine learning techniques in adversarial settings like spam filtering, malware detection and biometric recognition.","The problem arises from the fact that machine learning techniques were originally designed for stationary environments in which the training and test data are assumed to be generated from the same (although possibly unknown) distribution. In the presence of intelligent and adaptive adversaries, however, this working hypothesis is likely to be violated to at least some degree (depending on the adversary). In fact, a malicious adversary can carefully manipulate the input data exploiting specific vulnerabilities of learning algorithms to compromise the whole system security.","Examples include: attacks in spam filtering, where spam messages are obfuscated through misspelling of bad words or insertion of good words;[1][2][3][4][5][6][7][8][9][10][11][12] attacks in computer security, e.g., to obfuscate malware code within network packets [13] or mislead signature detection;[14] attacks in biometric recognition, where fake biometric traits may be exploited to impersonate a legitimate user (biometric spoofing) [15] or to compromise users’ template galleries that are adaptively updated over time.[16][17]",,,"To understand the security properties of learning algorithms in adversarial settings, one should address the following main issues:[18][19][20][21]","This process amounts to simulating a proactive arms race (instead of a reactive one, as depicted in Figures 1 and 2, where system designers try to anticipate the adversary in order to understand whether there are potential vulnerabilities that should be fixed in advance; for instance, by means of specific countermeasures such as additional features or different learning algorithms. However proactive approaches are not necessarily superior to reactive ones. For instance, in,[22] the authors showed that under some circumstances, reactive approaches are more suitable for improving system security.",The first step of the above-sketched arms race is identifying potential attacks against machine learning algorithms. A substantial amount of work has been done in this direction.[18][19][20][21][23][24],"Attacks against (supervised) machine learning algorithms have been categorized along three primary axes:[21][23][24] their influence on the classifier, the security violation they cause, and their specificity.","This taxonomy has been extended into a more comprehensive threat model that allows one to make explicit assumptions on the adversary’s goal, knowledge of the attacked system, capability of manipulating the input data and/or the system components, and on the corresponding (potentially, formally-defined) attack strategy. Details can be found here.[18][19] Two of the main attack scenarios identified according to this threat model are sketched below.","Evasion attacks [18][19][25][26] are the most prevalent type of attack that may be encountered in adversarial settings during system operation. For instance, spammers and hackers often attempt to evade detection by obfuscating the content of spam emails and malware code. In the evasion setting, malicious samples are modified at test time to evade detection; that is, to be misclassified as legitimate. No influence over the training data is assumed. A clear example of evasion is image-based spam in which the spam content is embedded within an attached image to evade the textual analysis performed by anti-spam filters. Another example of evasion is given by spoofing attacks against biometric verification systems.[15][16]","Machine learning algorithms are often re-trained on data collected during operation to adapt to changes in the underlying data distribution. For instance, intrusion detection systems (IDSs) are often re-trained on a set of samples collected during network operation. Within this scenario, an attacker may poison the training data by injecting carefully designed samples to eventually compromise the whole learning process. Poisoning may thus be regarded as an adversarial contamination of the training data. Examples of poisoning attacks against machine learning algorithms (including learning in the presence of worst-case adversarial label flips in the training data) can be found in.[14][16][18][19][21][23][24][27][28][29][30][31][32][33][34]","Clustering algorithms have been increasingly adopted in security applications to find dangerous or illicit activities. For instance, clustering of malware and computer viruses aims to identify and categorize different existing malware families, and to generate specific signatures for their detection by anti-viruses, or signature-based intrusion detection systems like Snort. However, clustering algorithms have not been originally devised to deal with deliberate attack attempts that are designed to subvert the clustering process itself. Whether clustering can be safely adopted in such settings thus remains questionable. Preliminary work reporting some vulnerability of clustering can be found in.[35][36][37][38][39]","A number of defense mechanisms against evasion, poisoning and privacy attacks have been proposed in the field of adversarial machine learning, including:",1. The definition of secure learning algorithms;[6][7][8][9][40][41][42][43],2. The use of multiple classifier systems;[3][4][5][10][44][45][46],3. The use of randomization or disinformation to mislead the attacker while acquiring knowledge of the system;[4][21][23][24],4. The study of privacy-preserving learning.[19][47],5. Ladder algorithm for Kaggle-style competitions.[48],"Some software libraries are available, mainly for testing purposes and research."
AIXI ['ai̯k͡siː] is a theoretical mathematical formalism for artificial general intelligence. It combines Solomonoff induction with sequential decision theory. AIXI was first proposed by Marcus Hutter in 2000[1] and the results below are proved in Hutter's 2005 book Universal Artificial Intelligence.[2],"AIXI is a reinforcement learning agent; it maximizes the expected total rewards received from the environment. Intuitively, it simultaneously considers every computable hypothesis. In each time step, it looks at every possible program and evaluates how many rewards that program generates depending on the next action taken. The promised rewards are then weighted by the subjective belief that this program constitutes the true environment. This belief is computed from the length of the program: longer programs are considered less likely, in line with Occam's razor. AIXI then selects the action that has the highest expected total reward in the weighted sum of all these programs.",,,"The AIXI agent interacts sequentially with some (stochastic and unknown to AIXI) environment 



μ


{\displaystyle \mu }

. In step t, the agent outputs an action 




a

t




{\displaystyle a_{t}}

 and the environment responds with an observation 




o

t




{\displaystyle o_{t}}

 and a reward 




r

t




{\displaystyle r_{t}}

 distributed according to the conditional probability 



μ
(

o

t



r

t



|


a

1



o

1



r

1


.
.
.

a

t
−
1



o

t
−
1



r

t
−
1



a

t


)


{\displaystyle \mu (o_{t}r_{t}|a_{1}o_{1}r_{1}...a_{t-1}o_{t-1}r_{t-1}a_{t})}

. Then this cycle repeats for t + 1. The agent tries to maximize cumulative future reward 




r

t


+
…
+

r

m




{\displaystyle r_{t}+\ldots +r_{m}}

 for a fixed lifetime m.","Given a current time t and history 




a

1



o

1



r

1


.
.
.

a

t
−
1



o

t
−
1



r

t
−
1




{\displaystyle a_{1}o_{1}r_{1}...a_{t-1}o_{t-1}r_{t-1}}

, the action AIXI outputs is defined as[3]","where U denotes a monotone universal Turing machine, and q ranges over all programs on the universal machine U.",The parameters to AIXI are the universal Turing machine and the agent's lifetime m. The latter dependence can be removed by the use of discounting.,AIXI's performance is measured by the expected total number of rewards it receives. AIXI has been proven to be optimal in the following ways.[2],"However, AIXI does have limitations. It is restricted to maximizing rewards based on percepts as opposed to external states. It also assumes it interacts with the environment solely through action and percept channels, preventing it from considering the possibility of being damaged or modified. It also assumes the environment is computable.[4] Since AIXI is incomputable, it assigns zero probability to its own existence.","Like Solomonoff induction, AIXI is incomputable. However, there are computable approximations of it. One such approximation is AIXItl, which performs as least as well as the provably best time t and space l limited agent.[2] Another approximation to AIXI with a restricted environment class is MC-AIXI(FAC-CTW), which has had some success playing simple games such as partially observable Pac-Man.[5][6]"
"Algorithmic inference gathers new developments in the statistical inference methods made feasible by the powerful computing devices widely available to any data analyst. Cornerstones in this field are computational learning theory, granular computing, bioinformatics, and, long ago, structural probability (Fraser 1966). The main focus is on the algorithms which compute statistics rooting the study of a random phenomenon, along with the amount of data they must feed on to produce reliable results. This shifts the interest of mathematicians from the study of the distribution laws to the functional properties of the statistics, and the interest of computer scientists from the algorithms for processing data to the information they process.",,,"Concerning the identification of the parameters of a distribution law, the mature reader may recall lengthy disputes in the mid 20th century about the interpretation of their variability in terms of fiducial distribution (Fisher 1956), structural probabilities (Fraser 1966), priors/posteriors (Ramsey 1925), and so on. From an epistemology viewpoint, this entailed a companion dispute as to the nature of probability: is it a physical feature of phenomena to be described through random variables or a way of synthesizing data about a phenomenon? Opting for the latter, Fisher defines a fiducial distribution law of parameters of a given random variable that he deduces from a sample of its specifications. With this law he computes, for instance “the probability that μ (mean of a Gaussian variable – our note) is less than any assigned value, or the probability that it lies between any assigned values, or, in short, its probability distribution, in the light of the sample observed”.","Fisher fought hard to defend the difference and superiority of his notion of parameter distribution in comparison to analogous notions, such as Bayes' posterior distribution, Fraser's constructive probability and Neyman's confidence intervals. For half a century, Neyman's confidence intervals won out for all practical purposes, crediting the phenomenological nature of probability. With this perspective, when you deal with a Gaussian variable, its mean μ is fixed by the physical features of the phenomenon you are observing, where the observations are random operators, hence the observed values are specifications of a random sample. Because of their randomness, you may compute from the sample specific intervals containing the fixed μ with a given probability that you denote confidence.","Let X be a Gaussian variable[1] with parameters 



μ


{\displaystyle \mu }

 and 




σ

2




{\displaystyle \sigma ^{2}}

 and 



{

X

1


,
…
,

X

m


}


{\displaystyle \{X_{1},\ldots ,X_{m}\}}

 a sample drawn from it. Working with statistics",and,"is the sample mean, we recognize that","follows a Student's t distribution (Wilks 1962) with parameter (degrees of freedom) m − 1, so that","Gauging T between two quantiles and inverting its expression as a function of 



μ


{\displaystyle \mu }

 you obtain confidence intervals for 



μ


{\displaystyle \mu }

.",With the sample specification:,"having size m = 10, you compute the statistics 




s

μ


=
43.37


{\displaystyle s_{\mu }=43.37}

 and 




s


σ

2




=
46.07


{\displaystyle s_{\sigma ^{2}}=46.07}

, and obtain a 0.90 confidence interval for 



μ


{\displaystyle \mu }

 with extremes (3.03, 5.65).","From a modeling perspective the entire dispute looks like a chicken-egg dilemma: either fixed data by first and probability distribution of their properties as a consequence, or fixed properties by first and probability distribution of the observed data as a corollary. The classic solution has one benefit and one drawback. The former was appreciated particularly back when people still did computations with sheet and pencil. Per se, the task of computing a Neyman confidence interval for the fixed parameter θ is hard: you don’t know θ, but you look for disposing around it an interval with a possibly very low probability of failing. The analytical solution is allowed for a very limited number of theoretical cases. Vice versa a large variety of instances may be quickly solved in an approximate way via the central limit theorem in terms of confidence interval around a Gaussian distribution – that's the benefit. The drawback is that the central limit theorem is applicable when the sample size is sufficiently large. Therefore, it is less and less applicable with the sample involved in modern inference instances. The fault is not in the sample size on its own part. Rather, this size is not sufficiently large because of the complexity of the inference problem.","With the availability of large computing facilities, scientists refocused from isolated parameters inference to complex functions inference, i.e. re sets of highly nested parameters identifying functions. In these cases we speak about learning of functions (in terms for instance of regression, neuro-fuzzy system or computational learning) on the basis of highly informative samples. A first effect of having a complex structure linking data is the reduction of the number of sample degrees of freedom, i.e. the burning of a part of sample points, so that the effective sample size to be considered in the central limit theorem is too small. Focusing on the sample size ensuring a limited learning error with a given confidence level, the consequence is that the lower bound on this size grows with complexity indices such as VC dimension or detail of a class to which the function we want to learn belongs.","A sample of 1,000 independent bits is enough to ensure an absolute error of at most 0.081 on the estimation of the parameter p of the underlying Bernoulli variable with a confidence of at least 0.99. The same size cannot guarantee a threshold less than 0.088 with the same confidence 0.99 when the error is identified with the probability that a 20-year-old man living in New York does not fit the ranges of height, weight and waistline observed on 1,000 Big Apple inhabitants. The accuracy shortage occurs because both the VC dimension and the detail of the class of parallelepipeds, among which the one observed from the 1,000 inhabitants' ranges falls, are equal to 6.","With insufficiently large samples, the approach: fixed sample – random properties suggests inference procedures in three steps:","a sampling mechanism 



(
U
,

g

(
a
,
k
)


)


{\displaystyle (U,g_{(a,k)})}

 for X with seed U reads:","or, equivalently, 




g

(
a
,
k
)


(
u
)
=
k

u

−
1

/

a


.


{\displaystyle g_{(a,k)}(u)=ku^{-1/a}.}

","With these relations we may inspect the values of the parameters that could have generated a sample with the observed statistic from a particular setting of the seeds representing the seed of the sample. Hence, to the population of sample seeds corresponds a population of parameters. In order to ensure this population clean properties, it is enough to draw randomly the seed values and involve either sufficient statistics or, simply, well-behaved statistics w.r.t. the parameters, in the master equations.","For example, the statistics 




s

1


=

∑

i
=
1


m


log
⁡

x

i




{\displaystyle s_{1}=\sum _{i=1}^{m}\log x_{i}}

 and 




s

2


=

min

i
=
1
,
…
,
m


{

x

i


}


{\displaystyle s_{2}=\min _{i=1,\ldots ,m}\{x_{i}\}}

 prove to be sufficient for parameters a and k of a Pareto random variable X. Thanks to the (equivalent form of the) sampling mechanism 




g

(
a
,
k
)




{\displaystyle g_{(a,k)}}

 we may read them as",respectively.,"where 




s

1




{\displaystyle s_{1}}

 and 




s

2




{\displaystyle s_{2}}

 are the observed statistics and 




u

1


,
…
,

u

m




{\displaystyle u_{1},\ldots ,u_{m}}

 a set of uniform seeds. Transferring to the parameters the probability (density) affecting the seeds, you obtain the distribution law of the random parameters A and K compatible with the statistics you have observed.","Compatibility denotes parameters of compatible populations, i.e. of populations that could have generated a sample giving rise to the observed statistics. You may formalize this notion as follows:","For a random variable and a sample drawn from it a compatible distribution is a distribution having the same sampling mechanism 






M



X


=
(
Z
,

g

θ


)


{\displaystyle {\mathcal {M}}_{X}=(Z,g_{\boldsymbol {\theta }})}

 of X with a value 




θ



{\displaystyle {\boldsymbol {\theta }}}

 of the random parameter 




Θ



{\displaystyle \mathbf {\Theta } }

 derived from a master equation rooted on a well-behaved statistic s.",You may find the distribution law of the Pareto parameters A and K as an implementation example of the population bootstrap method as in the figure on the left.,"Implementing the twisting argument method, you get the distribution law 




F

M


(
μ
)


{\displaystyle F_{M}(\mu )}

 of the mean M of a Gaussian variable X on the basis of the statistic 




s

M


=

∑

i
=
1


m



x

i




{\displaystyle s_{M}=\sum _{i=1}^{m}x_{i}}

 when 




Σ

2




{\displaystyle \Sigma ^{2}}

 is known to be equal to 




σ

2




{\displaystyle \sigma ^{2}}

 (Apolloni, Malchiodi & Gaito 2006). Its expression is:","shown in the figure on the right, where 



Φ


{\displaystyle \Phi }

 is the cumulative distribution function of a standard normal distribution.","Computing a confidence interval for M given its distribution function is straightforward: we need only find two quantiles (for instance 



δ

/

2


{\displaystyle \delta /2}

 and 



1
−
δ

/

2


{\displaystyle 1-\delta /2}

 quantiles in case we are interested in a confidence interval of level δ symmetric in the tail's probabilities) as indicated on the left in the diagram showing the behavior of the two bounds for different values of the statistic sm.","The Achilles heel of Fisher's approach lies in the joint distribution of more than one parameter, say mean and variance of a Gaussian distribution. On the contrary, with the last approach (and above-mentioned methods: population bootstrap and twisting argument) we may learn the joint distribution of many parameters. For instance, focusing on the distribution of two or many more parameters, in the figures below we report two confidence regions where the function to be learnt falls with a confidence of 90%. The former concerns the probability with which an extended support vector machine attributes a binary label 1 to the points of the 



(
x
,
y
)


{\displaystyle (x,y)}

 plane. The two surfaces are drawn on the basis of a set of sample points in turn labelled according to a specific distribution law (Apolloni et al. 2008). The latter concerns the confidence region of the hazard rate of breast cancer recurrence computed from a censored sample (Apolloni, Malchiodi & Gaito 2006).",
"AlphaGo is a computer program developed by Google DeepMind in London to play the board game Go.[1] In October 2015, it became the first Computer Go program to beat a professional human Go player without handicaps on a full-sized 19×19 board.[2][3] In March 2016, it beat Lee Sedol in a five-game match, the first time a computer Go program has beaten a 9-dan professional without handicaps.[4] Although it lost to Lee Sedol in the fourth game, Lee resigned the final game, giving a final score of 4 games to 1 in favour of AlphaGo. In recognition of beating Lee Sedol, AlphaGo was awarded an honorary 9-dan by the Korea Baduk Association.","AlphaGo's algorithm uses a Monte Carlo tree search to find its moves based on knowledge previously ""learned"" by machine learning, specifically by an artificial neural network (a deep learning method) by extensive training, both from human and computer play.",,,"Go is considered much more difficult for computers to win than other games such as chess, because its much larger branching factor makes it prohibitively difficult to use traditional AI methods such as Alpha–beta pruning, Tree traversal and heuristic search.[2][5]","Almost two decades after IBM's computer Deep Blue beat world chess champion Garry Kasparov in the 1997 match, the strongest Go programs using artificial intelligence techniques only reached about amateur 5-dan level,[6] and still could not beat a professional Go player without handicaps.[2][3][7] In 2012, the software program Zen, running on a four PC cluster, beat Masaki Takemiya (9p) two times at five and four stones handicap.[8] In 2013, Crazy Stone beat Yoshio Ishida (9p) at four-stones handicap.[9]","According to AlphaGo's David Silver, the AlphaGo research project was formed around 2014 to test how well a neural network using deep learning can compete at Go.[10] AlphaGo represents a significant improvement over previous Go programs. In 500 games against other available Go programs, including Crazy Stone and Zen,[11] AlphaGo running on a single computer won all but one.[12] In a similar matchup, AlphaGo running on multiple computers won all 500 games played against other Go programs, and 77% of games played against AlphaGo running on a single computer. The distributed version in October 2015 was using 1,202 CPUs and 176 GPUs.[6]","In October 2015, the distributed version of AlphaGo defeated the European Go champion Fan Hui,[13] a 2-dan (out of 9 dan possible) professional, five to zero.[3][14] This was the first time a computer Go program had beaten a professional human player on a full-sized board without handicap.[15] The announcement of the news was delayed until 27 January 2016 to coincide with the publication of a paper in the journal Nature[6] describing the algorithms used.[3]","AlphaGo played South Korean professional Go player Lee Sedol, ranked 9-dan, one of the best players at Go,[7][needs update] with five games taking place at the Four Seasons Hotel in Seoul, South Korea on 9, 10, 12, 13, and 15 March 2016,[16][17] which were video-streamed live.[18] Aja Huang, a DeepMind team member and amateur 6-dan Go player, placed stones on the Go board for AlphaGo, which ran through Google's cloud computing with its servers located in the United States.[19] The match used Chinese rules with a 7.5-point komi, and each side had two hours of thinking time plus three 60-second byoyomi periods.[20] The version of AlphaGo playing against Lee used a similar amount of computing power as was used in the Fan Hui match.[21]","At the time of play, Lee Sedol had the second-highest number of Go international championship victories in the world.[22] While there is no single official method of ranking in international Go, some sources ranked Lee Sedol as the fourth-best player in the world at the time.[23][24] AlphaGo was not specifically trained to face Lee.[25]","The first three games were won by AlphaGo following resignations by Lee Sedol.[26][27] However, Lee Sedol beat AlphaGo in the fourth game, winning by resignation at move 180. AlphaGo then continued to achieve a fourth win, winning the fifth game by resignation.[28]","The prize was $1 million USD. Since AlphaGo won four out of five and thus the series, the prize will be donated to charities, including UNICEF.[29] Lee Sedol received $150,000 for participating in all five games and an additional $20,000 for his win.[20]","On June 29th, at a presentation held at a University in the Netherlands, Aja Huang, one of the Deep Mind team, revealed that it had rectified the problem that occurred during the 4th game of the match between AlphaGo and Lee Sedol, and that after move 78 (which was dubbed the ""hand of God"" by many professionals""), it would play accurately and maintain Black's advantage, since before the error which resulted in the loss, AlphaGo was leading throughout the game and Lee's move was not credited as the one which won the game, but caused the program's computing powers to be diverted and confused. Aja Huang explained that AlphaGo's policy network of finding the most accurate move order and continuation did not precisely guide AlphaGo to make the correct continuation after move 78, since its value network did not determine Lee Sedol's 78th move as being the most likely, and therefore when the move was made AlphaGo could not make the right adjustment to the logical continuation..[30]","On June 4, 2016, at a news conference during the 37th World Amateur Go Championship in Wuxi, Yang Jun'an, the CPC party chief of China Qiyuan, and also an executive of the International Go Federation, claimed that AlphaGo would have a match against Chinese Go player Ke Jie, possibly within this year.[31] However, Demis Hassabis responded that DeepMind had not decided yet what to do next with AlphaGo.[32]","An early version of AlphaGo was tested on hardware with various numbers of CPUs and GPUs, running in asynchronous or distributed mode. Two seconds of thinking time was given to each move. The resulting Elo ratings are listed below.[6] In the matches with more time per move higher ratings are achieved.","In May 2016, Google unveiled its own proprietary hardware ""tensor processing units"", which it stated had already been deployed in multiple internal projects at Google, including the AlphaGo match against Lee Sedol.[33][34]","As of 2016, AlphaGo's algorithm uses a combination of machine learning and tree search techniques, combined with extensive training, both from human and computer play. It uses Monte Carlo tree search, guided by a ""value network"" and a ""policy network,"" both implemented using deep neural network technology.[2][6] A limited amount of game-specific feature detection pre-processing (for example, to highlight whether a move matches a nakade pattern) is applied to the input before it is sent to the neural networks.[6]","The system's neural networks were initially bootstrapped from human gameplay expertise. AlphaGo was initially trained to mimic human play by attempting to match the moves of expert players from recorded historical games, using a database of around 30 million moves.[13] Once it had reached a certain degree of proficiency, it was trained further by being set to play large numbers of games against other instances of itself, using reinforcement learning to improve its play.[2] To avoid ""disrespectfully"" wasting its opponent's time, the program is specifically programmed to resign if its assessment of win probability falls beneath a certain threshold; for the March 2016 match against Lee, the resignation threshold was set to 20%.[35]","Toby Manning, the match referee for AlphaGo vs. Fan Hui, has described the program's style as ""conservative"".[36] During AlphaGo's match against Lee Sedol, Korean commentators exclaimed the AI's playstyle greatly resembled that of the legendary player Lee Changho.[citation needed] This similarity can be attributed to the fact that like Lee Changho,[citation needed] AlphaGo's playstyle also strongly favors greater probability of winning by fewer points over lesser probability of winning by more points.[10]",AlphaGo's March 2016 victory was a major milestone in artificial intelligence research.[37] Go had previously been regarded as a hard problem in machine learning that was expected to be out of reach for the technology of the time.[37][38][39] Most experts thought a Go program as powerful as AlphaGo was at least five years away;[40] some experts thought that it would take at least another decade before computers would beat Go champions.[6][41][42] Most observers at the beginning of the 2016 matches expected Lee to beat AlphaGo.[37],"With games such as checkers (that has been ""solved"" by the Chinook draughts player team), chess, and now Go won by computers, victories at popular board games can no longer serve as major milestones for artificial intelligence in the way that they used to. Deep Blue's Murray Campbell called AlphaGo's victory ""the end of an era... board games are more or less done and it's time to move on.""[37]","When compared with Deep Blue or with Watson, AlphaGo's underlying algorithms are potentially more general-purpose, and may be evidence that the scientific community is making progress toward artificial general intelligence.[10][43] Some commentators believe AlphaGo's victory makes for a good opportunity for society to start discussing preparations for the possible future impact of machines with general purpose intelligence. (As noted by entrepreneur Guy Suter, AlphaGo itself only knows how to play Go, and doesn't possess general purpose intelligence: ""[It] couldn't just wake up one morning and decide it wants to learn how to use firearms""[37]) In March 2016, AI researcher Stuart Russell stated that ""AI methods are progressing much faster than expected, (which) makes the question of the long-term outcome more urgent,"" adding that ""in order to ensure that increasingly powerful AI systems remain completely under human control... there is a lot of work to do.""[44] Some scholars, such as Stephen Hawking, warned (in May 2015 before the matches) that some future self-improving AI could gain actual general intelligence, leading to an unexpected AI takeover; other scholars disagree: AI expert Jean-Gabriel Ganascia believes that ""Things like 'common sense'... may never be reproducible"",[45] and says ""I don't see why we would speak about fears. On the contrary, this raises hopes in many domains such as health and space exploration.""[44] Computer scientist Richard Sutton ""I don't think people should be scared... but I do think people should be paying attention.""[46]","Go is a popular game in China, Japan and Korea, and the 2016 matches were watched by perhaps a hundred million people worldwide.[37][47] Many top Go players characterized AlphaGo's unorthodox plays as seemingly-questionable moves that initially befuddled onlookers, but made sense in hindsight:[41] ""All but the very best Go players craft their style by imitating top players. AlphaGo seems to have totally original moves it creates itself.""[37] AlphaGo appeared to have unexpectedly become much stronger, even when compared with its October 2015 match[48] where a computer had beat a Go professional for the first time ever without the advantage of a handicap.[49] The day after Lee's first defeat, Jeong Ahram, the lead Go correspondent for one of South Korea’s biggest daily newspapers, said ""Last night was very gloomy... Many people drank alcohol.""[50] The Korea Baduk Association, the organization that oversees Go professionals in South Korea, awarded AlphaGo an honorary 9-dan title for exhibiting creative skills and pushing forward the game's progress.[51]","China's Ke Jie, an 18-year-old generally recognized as the world's best Go player,[23][52] initially claimed that he would be able to beat AlphaGo, but declined to play against it for fear that it would ""copy my style"".[52] As the matches progressed, Ke Jie went back and forth, stating that ""it is highly likely that I (could) lose"" after analyzing the first three matches,[53] but regaining confidence after AlphaGo displayed flaws in the fourth match.[54]","Toby Manning, the referee of AlphaGo's match against Fan Hui, and Hajin Lee, secretary general of the International Go Federation, both reason that in the future, Go players will get help from computers to learn what they have done wrong in games and improve their skills.[49]","After game two, Lee said he felt ""speechless"": ""From the very beginning of the match, I could never manage an upper hand for one single move. It was AlphaGo's total victory.""[55] Lee apologized for his losses, stating after game three that ""I misjudged the capabilities of AlphaGo and felt powerless.""[37] He emphasized that the defeat was ""Lee Se-dol's defeat"" and ""not a defeat of mankind"".[25][45] Lee said his eventual loss to a machine was ""inevitable"" but stated that ""robots will never understand the beauty of the game the same way that we humans do.""[45] Lee called his game four victory a ""priceless win that I (would) not exchange for anything.""[25]","Facebook has also been working on their own Go-playing system darkforest, also based on combining machine learning and tree search.[36][56] Although a strong player against other computer Go programs, as of early 2016, it had not yet defeated a professional human player.[57] darkforest has lost to CrazyStone and Zen and is estimated to be of similar strength to CrazyStone and Zen.[58]","On March 1 a ""Deep Zen Go Project"" was announced between the developers of the computer go program Zen (Yoji Ojima, Hideki Kato), telecommunications and media company Dwango and a deep learning research team at Tokyo University (developers of Ponanza - a shogi AI that beat all the human pros). Japanese Go Association is also pledging their support. Their goal is to beat AlphaGo in 6 months to 1 year.[59]","AlphaGo (black) v. Fan Hui, Game 4 (8 October 2015), AlphaGo won by resignation.[6]"
"Apprenticeship learning, or apprenticeship via inverse reinforcement learning (AIRP), is a concept in the field of artificial intelligence and machine learning, developed by Pieter Abbeel, Associate Professor in Berkeley's EECS department, and Andrew Ng, Associate Professor in Stanford University's Computer Science Department. It was incepted in 2004. AIRP deals with ""Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform""[1]",AIRP concept is closely related to reinforcement learning (RL) that is a sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to maximize some notion of long-term reward. AIRP algorithms are used when the reward function is unknown. The algorithms use observations of the behavior of an expert to teach the agent the optimal actions in certain states of the environment.,"AIRP is a special case of the general area of learning from demonstration (LfD), where the goal is to learn a complex task by observing a set of expert traces (demonstrations). AIRP is the intersection of LfD and RL.","Apprenticeship learning has been used to model reward functions of highly dynamic scenarios where there is no obvious reward function intuitively. Take the task of driving for example, there are many different objectives working simultaneously - such as maintaining safe following distance, a good speed, not changing lanes too often, etc. This task, may seem easy at first glance, but a trivial reward function may not converge to the policy wanted.","One domain where apprenticeship learning has been used extensively is helicopter control. While simple trajectories can be intuitively derived, complicated tasks like aerobatics for shows has been successful. These include aerobatic maneuvers like - in-place flips, in-place rolls, loops, hurricanes and even auto-rotation landings. This work was developed by Pieter Abbeel, Adam Coates, and Andrew Ng - ""Autonomous Helicopter Aerobatics through Apprenticeship Learning""[2]"
"The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision.[1]",The bag-of-words model is commonly used in methods of document classification. where the (frequency of) occurrence of each word is used as a feature for training a classifier.,"An early reference to ""bag of words"" in a linguistic context can be found in Zellig Harris's 1954 article on Distributional Structure.[2]",,,The following models a text document using bag-of-words.,Here are two simple text documents:,"Based on these two text documents, a list is constructed as follows:","In practice, the Bag-of-words model is mainly used as a tool of feature generation. After transforming the text into a ""bag of words"", we can calculate various measures to characterize the text. The most common type of characteristics, or features calculated from the Bag-of-words model is term frequency, namely, the number of times a term appears in the text. For the example above, we can construct the following two lists to record the term frequencies of all the distinct words:","Each entry of the lists refers to count of the corresponding entry in the list (this is also the histogram representation). For example, in the first list (which represents document 1), the first two entries are ""1,2"". The first entry corresponds to the word ""John"" which is the first word in the list, and its value is ""1"" because ""John"" appears in the first document 1 time. Similarly, the second entry corresponds to the word ""likes"" which is the second word in the list, and its value is ""2"" because ""likes"" appears in the first document 2 times. This list (or vector) representation does not preserve the order of the words in the original sentences, which is just the main feature of the Bag-of-words model. This kind of representation has several successful applications, for example email filtering.[1]","However, term frequencies are not necessarily the best representation for the text. Common words like ""the"", ""a"", ""to"" are almost always have the highest term frequency in the text, thus having a high raw count does not necessarily means that the corresponding word is more important. To address this problem, one of the most popular way to ""normalize"" the term frequencies is to weight term by the inverse of document frequency, or tf–idf. Additionally, for the specific purpose of classification supervised alternatives have been developed that take into account the class label of a document.[3] Lastly, binary (presence/absence or 1/0) weighting is used in place of frequencies for some problems. (For instance, this option is implemented in the WEKA machine learning software system.)","Bag-of-word model is an orderless document representation—only the counts of words mattered. For instance, in the above example ""John likes to watch movies. Mary likes movies too"", the bag-of-words representation will not reveal the fact that a person name is always followed by the verb ""likes"" in this text. As an alternative, the n-gram model can be used to store this spatial information within the text. Applying to the same example above, a bigram model will parse the text into following units and store the term frequency of each unit as before.","Conceptually, we can view bag-of-word model as a special case of the n-gram model, with n=1. See language model for a more detailed discussion.","A common alternative to the use of dictionaries is the hashing trick, where words are directly mapped to indices with a hashing function.[4] By mapping words to indices directly with a hash function, no memory is required to store a dictionary. Hash collisions are typically dealt with by using freed-up memory to increase the number of hash buckets. In practice, hashing greatly simplifies the implementation of bag-of-words models and improves their scalability.","In Bayesian spam filtering, an e-mail message is modeled as an unordered collection of words selected from one of two probability distributions: one representing spam and one representing legitimate e-mail (""ham""). Imagine that there are two literal bags full of words. One bag is filled with words found in spam messages, and the other bag is filled with words found in legitimate e-mail. While any given word is likely to be found somewhere in both bags, the ""spam"" bag will contain spam-related words such as ""stock"", ""Viagra"", and ""buy"" much more frequently, while the ""ham"" bag will contain more words related to the user's friends or workplace.","To classify an e-mail message, the Bayesian spam filter assumes that the message is a pile of words that has been poured out randomly from one of the two bags, and uses Bayesian probability to determine which bag it is more likely to be."
"In computer science, a ball tree, balltree or metric tree,[1] is a space partitioning data structure for organizing points in a multi-dimensional space. The ball tree gets its name from the fact that it partitions data points into a nested set of hyperspheres known as ""balls"". The resulting data structure has characteristics that make it useful for a number of applications, most notably nearest neighbor search.",,,"A ball tree is a binary tree in which every node defines a D-dimensional hypersphere, or ball, containing a subset of the points to be searched. Each internal node of the tree partitions the data points into two disjoint sets which are associated with different balls. While the balls themselves may intersect, each point is assigned to one or the other ball in the partition according to its distance from the ball's center. Each leaf node in the tree defines a ball and enumerates all data points inside that ball.","Each node in the tree defines the smallest ball that contains all data points in its subtree. This gives rise to the useful property that, for a given test point t, the distance to any point in a ball B in the tree is greater than or equal to the distance from t to the ball. Formally: [2]","Where 




D

B


(
t
)


{\displaystyle D^{B}(t)}

 is the minimum possible distance from any point in the ball B to some point t.","Ball-trees are related to the M-tree, but only support binary splits, whereas in the M-tree each level splits m to 



2
m


{\displaystyle 2m}

 fold, thus leading to a less deep tree structure. The M-tree also keeps the distances from the parent node precomputed to speed up queries.","A number of ball tree construction algorithms are available.[3] The goal of such an algorithm is to produce a tree that will efficiently support queries of the desired type (e.g. nearest-neighbor) efficiently in the average case. The specific criteria of an ideal tree will depend on the type of question being answered and the distribution of the underlying data. However, a generally applicable measure of an efficient tree is one that minimizes the total volume of its internal nodes. Given the varied distributions of real-world data sets, this is a difficult task, but there are several heuristics that partition the data well in practice. In general, there is a tradeoff between the cost of constructing a tree and the efficiency achieved by this metric. [2]",This section briefly describes the simplest of these algorithms. A more in-depth discussion of five algoriths was given by Stephen Omohundro.[3],"The simplest such procedure is termed the ""k-d Construction Algorithm"", by analogy with the process used to construct k-d trees. This is an off-line algorithm, that is, an algorithm that operates on the entire data set at once. The tree is built top-down by recursively splitting the data points into two sets. Splits are chosen along the single dimension with the greatest spread of points, with the sets partitioned by the median value of all points along that dimension. Finding the split for each internal node requires linear time in the number of samples contained in that node, yielding an algorithm with time complexity 



O
(
n

log

n
)


{\displaystyle O(n\,\log \,n)}

, where n is the number of data points.","An important application of ball trees is expediting nearest neighbor search queries, in which the objective is to find the k points in the tree that are closest to a given test point by some distance metric (e.g. Euclidean distance). A simple search algorithm, sometimes called KNS1, exploits the distance property of the ball tree. In particular, if the algorithm is searching the data structure with a test point t, and has already seen some point p that is closest to t among the points have encountered so far, then any subtree whose ball is further from t than p can be ignored for the rest of the search.","The ball tree nearest-neighbor algorithm examines nodes in depth-first order, starting at the root. During the search, the algorithm maintains a max-first priority queue (often implemented with a heap), denoted Q here, of the k nearest points encountered so far. At each node B, it may perform one of three operations, before finally returning an updated version of the priority queue:",Performing the recursive search in the order described in point 3 above increases likelihood that the further child will be pruned entirely during the search.,"In comparison with several other data structures, ball trees have been shown to perform fairly well on the nearest-neighbor search problem, particularly as their number of dimensions grows.[1][4] However, the best nearest-neighbor data structure for a given application will depend on the dimensionality, number of data points, and underlying structure of the data."
"In probability and statistics, base rate generally refers to the (base) class probabilities unconditioned on featural evidence, frequently also known as prior probabilities. For example, if it were the case that 1% of the public were ""medical professionals"", and 99% of the public were not ""medical professionals"", then the base rate of medical professionals is simply 1%.","In the sciences, including medicine, the base rate is critical for comparison. It may at first seem impressive that 1000 people beat their winter cold while using 'Treatment X', until we look at the entire 'Treatment X' population and find that the base rate of success is actually only 1/100 (i.e. 100,000 people tried the treatment, but the other 99,000 people never really beat their winter cold). The treatment's effectiveness is clearer when such base rate information (i.e. ""1000 people... out of how many?"") is available. Note that controls may likewise offer further information for comparison; maybe the control groups, who were using no treatment at all, had their own base rate success of 5/100. Controls thus indicate that 'Treatment X' actually makes things worse, despite that initial proud claim about 1000 people.",The normative method for integrating base rates (prior probabilities) and featural evidence (likelihoods) is given by Bayes' rule.,"A large number of psychological studies have examined a phenomenon called base-rate neglect or base rate fallacy in which category base rates are not integrated with featural evidence in the normative manner. Mathematician Keith Devlin provides an illustration of the risks of this: He asks us to imagine that there is a type of cancer that afflicts 1% of all people. A doctor then says there is a test for that cancer which is about 80% reliable. He also says that the test provides a positive result for 100% of people who have the cancer, but it also results in a 'false positive' for 20% of people - who actually do not have the cancer. Now, if we test positive, we may be tempted to think it is 80% likely that we have the cancer. Devlin explains that, in fact, our odds are less than 5%. What is missing from the jumble of statistics is the most relevant base rate information. We should ask the doctor ""Out of the number of people who test positive at all (this is the base rate group that we care about), how many end up actually having the cancer?"".[1] Naturally, in assessing the probability that a given individual is a member of a particular class, we must account for other information besides the base rate. In particular, we must account for featural evidence. For example, when we see a person wearing a white doctor's coat and stethoscope, and prescribing medication, we have evidence which may allow us to conclude that the probability of this particular individual being a ""medical professional"" is considerably greater than the category base rate of 1%."
"In machine learning, kernel methods arise from the assumption of an inner product space or similarity structure on inputs. For some such methods, such as support vector machines (SVMs), the original formulation and its regularization were not Bayesian in nature. It is helpful to understand them from a Bayesian perspective. Because the kernels are not necessarily positive semidefinite, the underlying structure may not be inner product spaces, but instead more general reproducing kernel Hilbert spaces. In Bayesian probability kernel methods are a key component of Gaussian processes, where the kernel function is known as the covariance function. Kernel methods have traditionally been used in supervised learning problems where the input space is usually a space of vectors while the output space is a space of scalars. More recently these methods have been extended to problems that deal with multiple outputs such as in multi-task learning.[1]","In this article we analyze the connections between the regularization and the Bayesian point of view for kernel methods in the case of scalar outputs. A mathematical equivalence between the regularization and the Bayesian point of view is easily proved in cases where the reproducing kernel Hilbert space is finite-dimensional. The infinite-dimensional case raises subtle mathematical issues; we will consider here the finite-dimensional case. We start with a brief review of the main ideas underlying kernel methods for scalar learning, and briefly introduce the concepts of regularization and Gaussian processes. We then show how both points of view arrive at essentially equivalent estimators, and show the connection that ties them together.",,,"The classical supervised learning problem requires estimating the output for some new input point 





x

′



{\displaystyle \mathbf {x} '}

 by learning a scalar-valued estimator 






f
^



(


x

′

)


{\displaystyle {\hat {f}}(\mathbf {x} ')}

 on the basis of a training set 



S


{\displaystyle S}

 consisting of 



n


{\displaystyle n}

 input-output pairs, 



S
=
(

X

,

Y

)
=
(


x


1


,

y

1


)
,
…
,
(


x


n


,

y

n


)


{\displaystyle S=(\mathbf {X} ,\mathbf {Y} )=(\mathbf {x} _{1},y_{1}),\ldots ,(\mathbf {x} _{n},y_{n})}

.[2] Given a symmetric and positive bivariate function 



k
(
⋅
,
⋅
)


{\displaystyle k(\cdot ,\cdot )}

 called a kernel, one of the most popular estimators in machine learning is given by","






f
^



(


x

′

)
=


k


⊤


(

K

+
λ
n

I


)

−
1



Y

,


{\displaystyle {\hat {f}}(\mathbf {x} ')=\mathbf {k} ^{\top }(\mathbf {K} +\lambda n\mathbf {I} )^{-1}\mathbf {Y} ,}

",, , , , ,(1),"where 




K

≡
k
(

X

,

X

)


{\displaystyle \mathbf {K} \equiv k(\mathbf {X} ,\mathbf {X} )}

 is the kernel matrix with entries 





K


i
j


=
k
(


x


i


,


x


j


)


{\displaystyle \mathbf {K} _{ij}=k(\mathbf {x} _{i},\mathbf {x} _{j})}

, 




k

=
[
k
(


x


1


,


x

′

)
,
…
,
k
(


x


n


,


x

′

)

]

⊤




{\displaystyle \mathbf {k} =[k(\mathbf {x} _{1},\mathbf {x} '),\ldots ,k(\mathbf {x} _{n},\mathbf {x} ')]^{\top }}

, and 




Y

=
[

y

1


,
…
,

y

n



]

⊤




{\displaystyle \mathbf {Y} =[y_{1},\ldots ,y_{n}]^{\top }}

. We will see how this estimator can be derived both from a regularization and a Bayesian perspective.","The main assumption in the regularization perspective is that the set of functions 





F




{\displaystyle {\mathcal {F}}}

 is assumed to belong to a reproducing kernel Hilbert space 






H



k




{\displaystyle {\mathcal {H}}_{k}}

.[2][3][4][5]","A reproducing kernel Hilbert space (RKHS) 






H



k




{\displaystyle {\mathcal {H}}_{k}}

 is a Hilbert space of functions defined by a symmetric, positive-definite function 



k
:


X


×


X


→

R



{\displaystyle k:{\mathcal {X}}\times {\mathcal {X}}\rightarrow \mathbb {R} }

 called the reproducing kernel such that the function 



k
(

x

,
⋅
)


{\displaystyle k(\mathbf {x} ,\cdot )}

 belongs to 






H



k




{\displaystyle {\mathcal {H}}_{k}}

 for all 




x

∈


X




{\displaystyle \mathbf {x} \in {\mathcal {X}}}

.[6][7][8] There are three main properties make an RKHS appealing:","1. The reproducing property, which gives name to the space,","where 



⟨
⋅
,
⋅

⟩

k




{\displaystyle \langle \cdot ,\cdot \rangle _{k}}

 is the inner product in 






H



k




{\displaystyle {\mathcal {H}}_{k}}

.","2. Functions in an RKHS are in the closure of the linear combination of the kernel at given points,",This allows the construction in a unified framework of both linear and generalized linear models.,3. The squared norm in an RKHS can be written as,and could be viewed as measuring the complexity of the function.,The estimator is derived as the minimizer of the regularized functional,"





1
n



∑

i
=
1


n


(
f
(


x


i


)
−

y

i



)

2


+
λ
∥
f

∥

k


2


,


{\displaystyle {\frac {1}{n}}\sum _{i=1}^{n}(f(\mathbf {x} _{i})-y_{i})^{2}+\lambda \|f\|_{k}^{2},}

",, , , , ,(2),"where 



f
∈



H



k




{\displaystyle f\in {\mathcal {H}}_{k}}

 and 



∥
⋅

∥

k




{\displaystyle \|\cdot \|_{k}}

 is the norm in 






H



k




{\displaystyle {\mathcal {H}}_{k}}

. The first term in this functional, which measures the average of the squares of the errors between the 



f
(


x


i


)


{\displaystyle f(\mathbf {x} _{i})}

 and the 




y

i




{\displaystyle y_{i}}

, is called the empirical risk and represents the cost we pay by predicting 



f
(


x


i


)


{\displaystyle f(\mathbf {x} _{i})}

 for the true value 




y

i




{\displaystyle y_{i}}

. The second term in the functional is the squared norm in a RKHS multiplied by a weight 



λ


{\displaystyle \lambda }

 and serves the purpose of stabilizing the problem[3][5] as well as of adding a trade-off between fitting and complexity of the estimator.[2] The weight 



λ


{\displaystyle \lambda }

, called the regularizer, determines the degree to which instability and complexity of the estimator should be penalized (higher penalty for increasing value of 



λ


{\displaystyle \lambda }

).","The explicit form of the estimator in equation (1) is derived in two steps. First, the representer theorem[9][10][11] states that the minimizer of the functional (2) can always be written as a linear combination of the kernels centered at the training-set points,","






f
^



(


x

′

)
=

∑

i
=
1


n



c

i


k
(


x


i


,


x

′

)
=


k


⊤



c

,


{\displaystyle {\hat {f}}(\mathbf {x} ')=\sum _{i=1}^{n}c_{i}k(\mathbf {x} _{i},\mathbf {x} ')=\mathbf {k} ^{\top }\mathbf {c} ,}

",, , , , ,(3),"for some 




c

∈


R


n




{\displaystyle \mathbf {c} \in \mathbb {R} ^{n}}

. The explicit form of the coefficients 




c

=
[

c

1


,
…
,

c

n



]

⊤




{\displaystyle \mathbf {c} =[c_{1},\ldots ,c_{n}]^{\top }}

 can be found by substituting for 



f
(
⋅
)


{\displaystyle f(\cdot )}

 in the functional (2). For a function of the form in equation (3), we have that",We can rewrite the functional (2) as,"This functional is convex in 




c



{\displaystyle \mathbf {c} }

 and therefore we can find its minimum by setting the gradient with respect to 




c



{\displaystyle \mathbf {c} }

 to zero,","Substituting this expression for the coefficients in equation (3), we obtain the estimator stated previously in equation (1),",The notion of a kernel plays a crucial role in Bayesian probability as the covariance function of a stochastic process called the Gaussian process.,"As part of the Bayesian framework, the Gaussian process specifies the prior distribution that describes the prior beliefs about the properties of the function being modeled. These beliefs are updated after taking into account observational data by means of a likelihood function that relates the prior beliefs to the observations. Taken together, the prior and likelihood lead to an updated distribution called the posterior distribution that is customarily used for predicting test cases.","A Gaussian process (GP) is a stochastic process in which any finite number of random variables that are sampled follow a joint Normal distribution.[12] The mean vector and covariance matrix of the Gaussian distribution completely specify the GP. GPs are usually used as a priori distribution for functions, and as such the mean vector and covariance matrix can be viewed as functions, where the covariance function is also called the kernel of the GP. Let a function 



f


{\displaystyle f}

 follow a Gaussian process with mean function 



m


{\displaystyle m}

 and kernel function 



k


{\displaystyle k}

,","In terms of the underlying Gaussian distribution, we have that for any finite set 




X

=
{


x


i



}

i
=
1


n




{\displaystyle \mathbf {X} =\{\mathbf {x} _{i}\}_{i=1}^{n}}

 if we let 



f
(

X

)
=
[
f
(


x


1


)
,
…
,
f
(


x


n


)

]

⊤




{\displaystyle f(\mathbf {X} )=[f(\mathbf {x} _{1}),\ldots ,f(\mathbf {x} _{n})]^{\top }}

 then","where 




m

=
m
(

X

)
=
[
m
(


x


1


)
,
…
,
m
(


x


N


)

]

⊤




{\displaystyle \mathbf {m} =m(\mathbf {X} )=[m(\mathbf {x} _{1}),\ldots ,m(\mathbf {x} _{N})]^{\top }}

 is the mean vector and 




K

=
k
(

X

,

X

)


{\displaystyle \mathbf {K} =k(\mathbf {X} ,\mathbf {X} )}

 is the covariance matrix of the multivariate Gaussian distribution.","In a regression context, the likelihood function is usually assumed to be a Gaussian distribution and the observations to be independent and identically distributed (iid),","This assumption corresponds to the observations being corrupted with zero-mean Gaussian noise with variance 




σ

2




{\displaystyle \sigma ^{2}}

. The iid assumption makes it possible to factorize the likelihood function over the data points given the set of inputs 




X



{\displaystyle \mathbf {X} }

 and the variance of the noise 




σ

2




{\displaystyle \sigma ^{2}}

, and thus the posterior distribution can be computed analytically. For a test input vector 





x

′



{\displaystyle \mathbf {x} '}

, given the training data 



S
=
{

X

,

Y

}


{\displaystyle S=\{\mathbf {X} ,\mathbf {Y} \}}

, the posterior distribution is given by","where 




ϕ



{\displaystyle {\boldsymbol {\phi }}}

 denotes the set of parameters which include the variance of the noise 




σ

2




{\displaystyle \sigma ^{2}}

 and any parameters from the covariance function 



k


{\displaystyle k}

 and where","A connection between regularization theory and Bayesian theory can only be achieved in the case of finite dimensional RKHS. Under this assumption, regularization theory and Bayesian theory are connected through Gaussian process prediction.[3][12]","In the finite dimensional case, every RKHS can be described in terms of a feature map 



Φ
:


X


→


R


p




{\displaystyle \Phi :{\mathcal {X}}\rightarrow \mathbb {R} ^{p}}

 such that[2]","Functions in the RKHS with kernel 




K



{\displaystyle \mathbf {K} }

 can be then be written as",and we also have that,"We can now build a Gaussian process by assuming 




w

=
[

w

1


,
…
,

w

p



]

⊤




{\displaystyle \mathbf {w} =[w^{1},\ldots ,w^{p}]^{\top }}

 to be distributed according to a multivariate Gaussian distribution with zero mean and identity covariance matrix,",If we assume a Gaussian likelihood we have,"where 




f


w



(

X

)
=
(
⟨

w

,
Φ
(


x


1


)
⟩
,
…
,
⟨

w

,
Φ
(


x


n


⟩
)


{\displaystyle f_{\mathbf {w} }(\mathbf {X} )=(\langle \mathbf {w} ,\Phi (\mathbf {x} _{1})\rangle ,\ldots ,\langle \mathbf {w} ,\Phi (\mathbf {x} _{n}\rangle )}

. The resulting posterior distribution is the given by","We can see that a maximum posterior (MAP) estimate is equivalent to the minimization problem defining Tikhonov regularization, where in the Bayesian case the regularization parameter is related to the noise variance.","From a philosophical perspective, the loss function in a regularization setting plays a different role than the likelihood function in the Bayesian setting. Whereas the loss function measures the error that is incurred when predicting 



f
(

x

)


{\displaystyle f(\mathbf {x} )}

 in place of 



y


{\displaystyle y}

, the likelihood function measures how likely the observations are from the model that was assumed to be true in the generative process. From a mathematical perspective, however, the formulations of the regularization and Bayesian frameworks make the loss function and the likelihood function to have the same mathematical role of promoting the inference of functions 



f


{\displaystyle f}

 that approximate the labels 



y


{\displaystyle y}

 as much as possible."
Bayesian optimization is a sequential design strategy for global optimization of black-box functions[1] that doesn't require derivatives.,,,The term is generally attributed to Jonas Mockus and is coined in his work from a series of publications on global optimization in the 1970s and 1980s.[2][3][4],"Since the objective function is unknown, the Bayesian strategy is to treat it as a random function and place a prior over it. The prior captures our beliefs about the behaviour of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criteria) that determines what the next query point should be.","Examples of acquisition functions include probability of improvement, expected improvement, Bayesian expected losses, upper confidence bounds (UCB), Thompson sampling and mixtures of these.[5] They all trade-off exploration and exploitation so as to minimize the number of function queries. As such, Bayesian optimization is well suited for functions that are very expensive to evaluate.",The maximum of the acquisition function is typically found by resorting to discretization or by means of an auxiliary optimizer.,"The approach has been applied to solve a wide range of problems,[6] including learning to rank,[7] interactive animation,[8] robotics,[9][10][11] sensor networks,[12][13] automatic algorithm configuration,[14] automatic machine learning toolboxes,[15][16][17] reinforcement learning, planning, visual attention, architecture configuration in deep learning, etc."
"Bayesian Structural Time Series (BSTS) model is a machine learning technique used for feature selection, time series forecasting, nowcasting, inferring causal impact and other. The model is designed to work with time series data.","The model has also promising application in the field of analytical marketing. In particular, it can be used in order to asses how much different marketing campaigns have contributed to the change in web search volumes, product sales, brand popularity and other relevant indicators (difference-in-differences model is a usual alternative approach in this case).[1] ""In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including the time-varying influence of contemporaneous covariates, i.e., synthetic controls.""[1]",The model consists of three main parts:,"The model seems to discover not only correlations, but also causations in the underlying data.[1]","A possible drawback of the model can be its relatively complicated mathematical part. However, R-program has ready-to-use packages for calculating the BSTS model,[2][3] which do not require strong mathematical background from a researcher."
"In statistics and machine learning, the bias–variance tradeoff (or dilemma) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:","The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.","This tradeoff applies to all forms of supervised learning: classification, regression (function fitting),[1][2] and structured output learning. It has also been invoked to explain the effectiveness of heuristics in human learning.",,,"The bias–variance tradeoff is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately captures the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well, but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that don't tend to overfit, but may underfit their training data, failing to capture important regularities.","Models with low bias are usually more complex (e.g. higher-order regression polynomials), enabling them to represent the training set more accurately. In the process, however, they may also represent a large noise component in the training set, making their predictions less accurate - despite their added complexity. In contrast, models with higher bias tend to be relatively simple (low-order or even linear regression polynomials), but may produce lower variance predictions when applied beyond the training set.","Suppose that we have a training set consisting of a set of points 




x

1


,
…
,

x

n




{\displaystyle x_{1},\dots ,x_{n}}

 and real values 




y

i




{\displaystyle y_{i}}

 associated with each point 




x

i




{\displaystyle x_{i}}

. We assume that there is a functional, but noisy relation 



y
=
f
(
x
)
+
ϵ


{\displaystyle y=f(x)+\epsilon }

, where the noise, 



ϵ


{\displaystyle \epsilon }

, has zero mean and variance 




σ

2




{\displaystyle \sigma ^{2}}

.","We want to find a function 






f
^



(
x
)


{\displaystyle {\hat {f}}(x)}

, that approximates the true function 



f
(
x
)


{\displaystyle f(x)}

 as well as possible, by means of some learning algorithm. We make ""as well as possible"" precise by measuring the mean squared error between 



y


{\displaystyle y}

 and 






f
^



(
x
)


{\displaystyle {\hat {f}}(x)}

: we want 



(
y
−



f
^



(
x
)

)

2




{\displaystyle (y-{\hat {f}}(x))^{2}}

 to be minimal, both for 




x

1


,
…
,

x

n




{\displaystyle x_{1},\dots ,x_{n}}

 and for points outside of our sample. Of course, we cannot hope to do so perfectly, since the 




y

i




{\displaystyle y_{i}}

 contain noise 



ϵ


{\displaystyle \epsilon }

; this means we must be prepared to accept an irreducible error in any function we come up with.","Finding an 






f
^





{\displaystyle {\hat {f}}}

 that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function 






f
^





{\displaystyle {\hat {f}}}

 we select, we can decompose its expected error on an unseen sample 



x


{\displaystyle x}

 as follows:[3]:34[4]:223",Where:,and,"The expectation ranges over different choices of the training set 




x

1


,
…
,

x

n


,

y

1


,
…
,

y

n




{\displaystyle x_{1},\dots ,x_{n},y_{1},\dots ,y_{n}}

, all sampled from the same (conditional) distribution. The three terms represent:","The more complex the model 






f
^



(
x
)


{\displaystyle {\hat {f}}(x)}

 is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model ""move"" more to capture the data points, and hence its variance will be larger.","The derivation of the bias–variance decomposition for squared error proceeds as follows.[5][6] For notational convenience, abbreviate 



f
=
f
(
x
)


{\displaystyle f=f(x)}

 and 






f
^



=



f
^



(
x
)


{\displaystyle {\hat {f}}={\hat {f}}(x)}

. First, note that for any random variable 



X


{\displaystyle X}

, we have","Rearranging, we get:","Since 



f


{\displaystyle f}

 is deterministic","This, given 



y
=
f
+
ϵ


{\displaystyle y=f+\epsilon }

 and 




E

[
ϵ
]
=
0


{\displaystyle \mathrm {E} [\epsilon ]=0}

, implies 




E

[
y
]
=

E

[
f
+
ϵ
]
=

E

[
f
]
=
f


{\displaystyle \mathrm {E} [y]=\mathrm {E} [f+\epsilon ]=\mathrm {E} [f]=f}

.","Also, since 




V
a
r

[
ϵ
]
=

σ

2




{\displaystyle \mathrm {Var} [\epsilon ]=\sigma ^{2}}

","Thus, since 



ϵ


{\displaystyle \epsilon }

 and 






f
^





{\displaystyle {\hat {f}}}

 are independent, we can write","The bias–variance decomposition was originally formulated for least-squares regression. For the case of classification under the 0-1 loss (misclassification rate), it's possible to find a similar decomposition.[7][8] Alternatively, if the classification problem can be phrased as probabilistic classification, then the expected squared error of the predicted probabilities with respect to the true probabilities can be decomposed as before.[9]","Dimensionality reduction and feature selection can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance, e.g.:","One way of resolving the trade-off is to use mixture models and ensemble learning.[12][13] For example, boosting combines many ""weak"" (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines ""strong"" learners in a way that reduces their variance.","In the case of k-nearest neighbors regression, a closed-form expression exists that relates the bias–variance decomposition to the parameter k:[4]:37, 223","where 




N

1


(
x
)
,
…
,

N

k


(
x
)


{\displaystyle N_{1}(x),\dots ,N_{k}(x)}

 are the k nearest neighbors of x in the training set. The bias (first term) is a monotone rising function of k, while the variance (second term) drops off as k is increased. In fact, under ""reasonable assumptions"" the bias of the first-nearest neighbor (1-NN) estimator vanishes entirely as the size of the training set approaches infinity.[1]","While widely discussed in the context of machine learning, the bias-variance dilemma has been examined in the context of human cognition, most notably by Gerd Gigerenzer and co-workers in the context of learned heuristics. They have argued (see references below) that the human brain resolves the dilemma in the case of the typically sparse, poorly-characterised training-sets provided by experience by adopting high-bias/low variance heuristics. This reflects the fact that a zero-bias approach has poor generalisability to new situations, and also unreasonably presumes precise knowledge of the true state of the world. The resulting heuristics are relatively simple, but produce better inferences in a wider variety of situations.[14]","Geman et al.[1] argue that the bias-variance dilemma implies that abilities such as generic object recognition cannot be learned from scratch, but require a certain degree of “hard wiring” that is later tuned by experience. This is because model-free approaches to inference require impractically large training sets if they are to avoid high variance."
Binary or binomial classification is the task of classifying the elements of a given set into two groups on the basis of a classification rule. Some typical binary classification tasks are:,"An important point is that in many practical binary classification problems, the two groups are not symmetric – rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, a false positive (detecting a disease when it is not present) is considered differently from a false negative (not detecting a disease when it is present).","Sometimes, classification tasks are trivial. Given 100 balls, some of them red and some blue, a human with normal color vision can easily separate them into red ones and blue ones. However, some tasks, like those in practical medicine, and those interesting from the computer science point of view, are far from trivial, and may produce faulty results if executed imprecisely.",,,"Statistical classification is a problem studied in machine learning. It is a type of supervised learning, a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories. When there are only two categories the problem is known as statistical binary classification.",Some of the methods commonly used for binary classification are:,"Each classifier is best in only a select domain based upon the number of observations, the dimensionality of the feature vector, the noise in the data and many other factors. For example random forests perform better than SVM classifiers for 3D point clouds.[1] [2]","There are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine sensitivity and specificity are often used, while in information retrieval precision and recall are preferred. An important distinction is between metrics that are independent on the prevalence (how often each category occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties.","Given a classification of a specific data set, there are four basic data: the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These can be arranged into a 2×2 contingency table, with columns corresponding to actual value – condition positive (CP) or condition negative (CN) – and rows corresponding to classification value – test outcome positive or test outcome negative. There are eight basic ratios that one can compute from this table, which come in four complementary pairs (each pair summing to 1). These are obtained by dividing each of the four numbers by the sum of its row or column, yielding eight numbers, which can be referred to generically in the form ""true positive row ratio"" or ""false negative column ratio"", though there are conventional terms. There are thus two pairs of column ratios and two pairs of row ratios, and one can summarize these with four numbers by choosing one ratio from each pair – the other four numbers are the complements.","The column ratios are True Positive Rate (TPR, aka Sensitivity or recall), with complement the False Negative Rate (FNR); and True Negative Rate (TNR, aka Specificity, SPC), with complement False Positive Rate (FPR). These are the proportion of the population with the condition (resp., without the condition) for which the test is correct (or, complementarily, for which the test is incorrect); these are independent of prevalence.","The row ratios are Positive Predictive Value (PPV, aka precision), with complement the False Discovery Rate (FDR); and Negative Predictive Value (NPV), with complement the False Omission Rate (FOR). These are the proportion of the population with a given test result for which the test is correct (or, complementarily, for which the test is incorrect); these depend on prevalence.","In diagnostic testing, the main ratios used are the true column ratios – True Positive Rate and True Negative Rate – where they are known as sensitivity and specificity. In informational retrieval, the main ratios are the true positive ratios (row and column) – Positive Predictive Value and True Positive Rate – where they are known as precision and recall.","One can take ratios of a complementary pair of ratios, yielding four likelihood ratios (two column ratio of ratios, two row ratio of ratios). This is primarily done for the column (condition) ratios, yielding likelihood ratios in diagnostic testing. Taking the ratio of one of these groups of ratios yields a final ratio, the diagnostic odds ratio (DOR). This can also be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN); this has a useful interpretation – as an odds ratio – and is prevalence-independent.","There are a number of other metrics, most simply the accuracy or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; the complement is the Fraction Incorrect (FiC). The F-score combines precision and recall into one number via a choice of weighing, most simply equal weighing, as the balanced F-score (F1 score). Some metrics come from regression coefficients: the markedness and the informedness, and their geometric mean, the Matthews correlation coefficient. Other metrics include Youden's J statistic, the uncertainty coefficient, the Phi coefficient, and Cohen's kappa."," Tests whose results are of continuous values, such as most blood values, can artificially be made binary by defining a cutoff value, with test results being designated as positive or negative depending on whether the resultant value is higher or lower than the cutoff.","However, such conversion causes a loss of information, as the resultant binary classification does not tell how much above or below the cutoff a value is. As a result, when converting a continuous value that is close to the cutoff to a binary one, the resultant positive or negative predictive value is generally higher than the predictive value given directly from the continuous value. In such cases, the designation of the test of being either positive or negative gives the appearance of an inappropriately high certainty, while the value is in fact in an interval of uncertainty. For example, with the urine concentration of hCG as a continuous value, a urine pregnancy test that measured 52 mIU/ml of hCG may show as ""positive"" with 50 mIU/ml as cutoff, but is in fact in an interval of uncertainty, which may be apparent only by knowing the original continuous value. On the other hand, a test result very far from the cutoff generally has a resultant positive or negative predictive value that is lower than the predictive value given from the continuous value. For example, a urine hCG value of 200,000 mIU/ml confers a very high probability of pregnancy, but conversion to binary values results in that it shows just as ""positive"" as the one of 52 mIU/ml."
"Bing Predicts is a prediction engine developed by Microsoft that uses machine learning from data on trending social media topics (and sentiment towards those topics), along with trending searches on Bing. It predicts the outcomes of political elections, popular reality shows, and major sporting events. Predictions can be accessed through the Bing search engine.[1]","The idea for a prediction engine was first suggested by Walter Sun, Development Manager for the Core Ranking team at Bing, when he noticed that school districts were more frequently searched before a major weather event in the area was forecasted, because searchers wanted to find out if a closing or delay was caused. He concluded that the time and location of major weather events could accurately be predicted without referring to a weather forecast by observing major increases in search frequency of school districts in the area. This inspired Bing to use its search data to infer outcomes of certain events, such as winners of reality shows.[1] Bing Predicts launched on April 21, 2014. The first reality shows to be featured on Bing Predicts were The Voice, American Idol, and Dancing with the Stars.[2]","The prediction accuracy for Bing Predicts is 80% for American Idol, and 85% for The Voice. Bing Predicts also predicts the outcomes of major political elections in the United States. Bing Predicts had 97% accuracy for the 2014 United States Senate elections, 96% accuracy for the 2014 United States House of Representatives elections, and an 89% accuracy for the 2014 United States gubernatorial elections. Bing Predicts is also making predictions for the results of the 2016 United States presidential primaries.[3]"
"A Bongard problem is a kind of puzzle invented by the Russian computer scientist Mikhail Moiseevich Bongard (Михаил Моисеевич Бонгард, 1924–1971), probably in the mid-1960s. They were published in his eponymous 1967 book on pattern recognition. Bongard, in the introduction of the book (which deals with a number of topics including perceptrons) credits the ideas in it to a group including M. N. Vaintsvaig, V. V. Maksimov, and M. S. Smirnov.",,,"The idea of a Bongard problem is to present two sets of relatively simple diagrams, say A and B. All the diagrams from set A have a common factor or attribute, which is lacking in all the diagrams of set B. The problem is to find, or to formulate, convincingly, the common factor. The problems were popularised by their occurrence in the 1979 book Gödel, Escher, Bach by Douglas Hofstadter, himself a composer of Bongard problems. Bongard problems are also at the heart of the game Zendo.","Many computational architectures have been devised to solve Bongard problems, the most extensive of which being Phaeaco, by Harry Foundalis,[1] who left the field in 2008 due to ethical concerns regarding machines that can pass as human.[2]"
"The Bradley–Terry model is a probability model that can predict the outcome of a comparison. Given a pair of individuals i and j drawn from some population, it estimates the probability that the pairwise comparison i > j turns out true, as","where pi is a positive real-valued score assigned to individual i. The comparison i > j can be read as ""i is preferred to j"", ""i ranks higher than j"", or ""i beats j"", depending on the application.","For example, pi may represent the skill of a team in a sports tournament, estimated from the number of times i has won a match. 



P
(
i
>
j
)


{\displaystyle P(i>j)}

 then represents the probability that i will win a match against j.[1][2] Another example used to explain the model's purpose is that of scoring products in a certain category by quality. While it's hard for a person to draft a direct ranking of (many) brands of wine, it may be feasible to compare a sample of pairs of wines and say, for each pair, which one is better. The Bradley–Terry model can then be used to derive a full ranking.[2]",,,"The model is named after R. A. Bradley and M. E. Terry,[3] who presented it in 1952,[4] although it had already been studied by Zermelo in the 1920s.[1][5][6]","Real-world applications of the model include estimation of the influence of statistical journals, or ranking documents by relevance in machine-learned search engines.[7] In the latter application, 



P
(
i
>
j
)


{\displaystyle P(i>j)}

 may reflect that document i is more relevant to the user's query than document j, so it should be displayed earlier in the results list. The individual pi then express the relevance of the document, and can be estimated from the frequency with which users click particular ""hits"" when presented with a result list.[8]","The Bradley–Terry model can be parametrized in various ways. One way to do so is to pick a single parameter per observation, leading to a model of n parameters p1, ..., pn.[9] Another variant, in fact the version considered by Bradley and Terry,[2] uses exponential score functions 




p

i


=

e


β

i






{\displaystyle p_{i}=e^{\beta _{i}}}

 so that","or, using the logit (and disallowing ties),[1]",reducing the model to logistic regression on pairs of individuals.,"The following algorithm computes the parameters pi of the basic version of the model from a sample of observations. Formally, it computes a maximum likelihood estimate, i.e., it maximizes the likelihood of the observed data. The algorithm dates back to the work of Zermelo.[1]","The observations required are the outcomes of previous comparisons, for example, pairs (i, j) where i beats j. Summarizing these outcomes as wij, the number of times i has beaten j, we obtain the log-likelihood of the parameter vector p = p1, ..., pn as[1]","Denote the number of comparisons ""won"" by i as Wi, and the number of comparisons made between i and j as Nij. Starting from an arbitrary vector p, the algorithm iteratively performs the update","for all i. After computing all of the new parameters, they should be renormalized,","This estimation procedure improves the log-likelihood in every iteration, and eventually converges to a unique minimum."
"Catastrophic interference, also known as catastrophic forgetting, is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information.[1][2] Neural networks are an important part of the network approach and connectionist approach to cognitive science. These networks use computer simulations to try and model human behaviours, such as memory and learning. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989),[1] and Ractcliff (1990).[2] It is a radical manifestation of the ‘sensitivity-stability’ dilemma [3] or the ‘stability-plasticity’ dilemma.[4] Specifically, these problems refer to the issue of being able to make an artificial neural network that is sensitive to, but not disrupted by, new information. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum.[5] The former remains completely stable in the presence of new information but lacks the ability to generalize, i.e. infer general principles, from new inputs. On the other hand, connectionst networks like the standard backpropagation network are very sensitive to new information and can generalize on new inputs. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is considered an issue when attempting to model human memory because, unlike these networks, humans typically do not show catastrophic forgetting. Thus, the issue of catastrophic interference must be eradicated from these backpropagation models in order to enhance the plausibility as models of human memory.",,,"In order to understand the topic of catastrophic interference it is important to understand the components of an artificial neural network and, more specifically, the behaviour of a backpropagation network. The following account of neural networks is summarized from Rethinking Innateness: A Connectionist Perspective on Development by Elman et al. (1996).[6]","Artificial neural networks are inspired by biological neural networks. They use mathematical models, namely algorithms, to do things such as classifying data and learning patterns in data. Information is represented in these networks through patterns of activation, known as a distributed representations.","The basic components of artificial neural networks are nodes/units and weights. Nodes or units are simple processing elements, which can be considered artificial neurons. These units can act in a variety of ways. They can act like sensory neurons and collect inputs from the environment, they can act like motor neurons and sent and output, they can act like interneurons and relay information, or they may do all three functions. A backpropagation network is often a three-layer neural network that includes input nodes, hidden nodes, and output nodes (see Figure 1). The hidden nodes allow the network to be transformed into an internal representation, akin to a mental representation. These internal representations give the backpropagation network its ability to capture abstract relationships between different input patterns.","The nodes are also connected to each other, thus they can send activation to one another like neurons. These connections can be unidirectional, creating a feedforward network, or they can be bidirectional, creating a recurrent network. Each of the connections between the nodes has a ``weight``, or strength, and it is in these weights are where the knowledge is ‘stored’. The weights act to multiply the output of a node. They can be excitatory (a positive value) or inhibitory (a negative value). For example, if a node has an output of 1.0 and it is connected to another node with a weight of -0.5 then the second node will receive an input signal of (1.0 x -0.5) = -0.5. Since any one node can receive multiple inputs, the sum of all of these inputs must be taken to calculate the net input.",The net input (neti) to a node j would be defined as:,"Once the input has been sent to the hidden layer from the input layer, the hidden node may then send an output to the output layer. The output of any given node depends on the activation of that node and the response function of that node. In the case of a three-layer backpropagation network, the response function is a non-linear, logistic function. This function allows a node to behave in an all or none fashion towards high or low input values and in a more graded and sensitive fashion towards mid-ranged input values. It allows the nodes the result in more substantial changes in the network when the node activation is at the more extreme values. Transforming the net input into a net output that can be sent onto the output layer is calculated by:","An important feature of neural networks is that they can learn. Simply put, this means that they can change their outputs when they are given new inputs. Backpropagation, specifically refers to how this the network is trained, i.e. how the network is told to learn. The way in which a backpropagation network learns, is through comparing the actual output to the desired output of the unit. The desired output is known as a 'teacher' and it can be the same as the input, as in the case of auto-associative/auto-encoder networks, or it can be completely different from the input. Either way, learning which requires a teacher is called supervised learning. The difference between these actual and desired output constitutes an error signal. This error signal is then fedback, or backpropagated, to the nodes in order to modify the weights in the neural network. Backpropagation first modifies the weights between output layer to the hidden layer, then next modifies the weights between the hidden units and the input units. The change in weights help to decrease the discrepancy between the actual and desired output. However, learning is typically incremental in these networks. This means that these networks will require a series of presentations of the same input before it can come up with the weight changes that will result in the desired output. The weights are usually set to random values for first learning trial and after many trials the weights become more able represent the desired output. The process of converging on an output is called settling. This kind of training is based on the error signal and backpropagation learning algorithm / delta rule:","The issue of catastrophic interference, comes about when learning is sequential. Sequential training involves the network learning an input-output patter until the error is reduced below a specific criterion, then training the network on another set of input-output patterns. Specifically, a backpropagation network will forget information if it first learns input A and then next learns input B. It is not seen when learning is concurrent or interleaved. Interleaved training means the network learns the both inputs-output patterns at the same time, i.e. as AB. Weights are only changed when the network is being trained and not when the network is being tested on its response.","To summarize, backpropagation networks:","Humans often learn information in a sequential manner. For example, a child will often to learn their 1s addition facts first, later followed by the 2s addition facts, etc. It would be impossible for a child to learn all of the addition facts at the same time. Catastrophic interference can be considered an issue when modelling human memory because, unlike backpropagation networks, humans typically do not show catastrophic forgetting during sequential learning. Rather humans tend to show gradual forgetting or interference when they learn information sequentially. For example, the classic retroactive interference study by Barnes and Underwood (1959)[7] used paired associate learning to determine how much new learning interfered with old learning in humans. Paired associates, means that a pair of stimuli is and responses are learned. Their experiment used eight lists of paired associates, A-B and A-C. The pairs had the stimuli as consonant-vowel-consonant trigrams (e.g., dax) and responses as adjectives. Subjects were initially trained on the A-B list, until they could correctly recall all A-B pairings. Next subjects were given 1, 5, 10 or 20 trials on the A-C list. After learning the A-C pairs the subjects were given a final test in which the stimulus A was presented and the subject was asked to recall the response B and C. They found that as the number of learning trials on A-C list increased, the recall of C increased. But the training on A-C interfered with the recall of B. Specifically recall of B dropped to around 80% after one learning trials of A-C and to 50% after 20 learning trials of A-C. Subsequent research on the topic of retroactive interference has found similar results, with human forgetting being gradual and typically levelling off near 50% recall.[8] Thus when compared with why typical human retroactive interference, catastrophic interference could be likened to retrograde amnesia.[1]","Some researchers have argued that catastrophic interference is not an issue with the backpropagation model of human memory. For example, Mirman and Spivey (2001) found that humans show more interference when learning pattern-based information.[9] Pattern-based learning is analogous to how a standard backpropagation network learns. Thus, they concluded that catastrophic interference is not limited to connectionist memory models but rather that it is a ""general product of patterm-based learning that occurs in humans as well"" (p. 272).[9] However, Musca, Rousset and Ans (2004) found contrasting results where retroactive interference was more pronounced in subjects who sequentially learned unstructured lists when controlling for methodological failure that occurred in the Mirman, D., & Spivey, M. (2001) study.[10]",The term catastrophic interference was originally coined by McCloskey and Cohen (1989) but was also brought to the attention of the scientific community by research from Ratcliff (1990).[2],McCloskey and Cohen(1989) noted the problem of catastrophic interference during two different experiments with backpropagation neural network modelling.,"In their first experiment they trained a standard backpropagation neural network on a single training set consisting of 17 single-digit ones problems (i.e., 1 +1 through 9 + 1, and 1 + 2 through 1 + 9) until the network could represent and respond properly to all of them. The error between the actual output and the desired output steadily declined across training sessions, which reflected that the network learned to represent the target outputs better across trials. Next they trained the network on a single training set consisting of 17 single-digit twos problems (i.e., 2+1 through 2 + 9, and 1 + 2 through 9 + 2) until the network could represent, respond properly to all of them. They noted that their procedure was similar to how a child would learn their addition facts. Following each learning trial on the twos facts, the network was tested for its knowledge on both the ones and twos addition facts. Like the ones facts, the twos facts were readily learned by the network. However, McCloskey and Cohen noted the network was no longer able to properly answer the ones addition problems even after one learning trial of the twos addition problems. The output pattern produced in response to the ones facts often resembled an output pattern for an incorrect number more closely than the output pattern for an incorrect number.[clarification needed] This is considered to be a drastic amount of error. Furthermore, the problems 2+1 and 2+1, which were included in both training sets, even showed dramatic disruption during the first learning trials of the twos facts.","In their second connectionist model, McCloskey and Cohen attempted to replicate the study on retroactive interference in humans by Barnes and Underwood (1959). They trained the model on A-B and A-C lists and used a context pattern in the input vector (input pattern), to differential between the lists. Specifically the network was trained to responds with the right B response when shown the A stimulus and A-B context patter and to respond with the correct C response when shown the A stimulus and the A-C context pattern. When the model was trained concurrently on the A-B and A-C items then network readily learned all of the associations correctly. In sequential training the A-B list was trained first, followed by the A-C list. After each presentation of the A-C list, performance was measured for both the A-B and A-C lists. They found that, the amount of training on the A-C list in Barnes and Underwood study that lead to 50% correct responses lead to nearly 0% correct responses by the backpropagation network. Furthermore, they found that the network tended to show responses that looked like the C response pattern when the network was prompted to give the B response pattern. This indicated that the A-C list apparently had overwritten the A-B list. This could be likened to learning the word dog, followed learning the word stool and then finding that you cannot recognize the word cat well but instead think of the word stool when presented with the word dog.","McCloskey and Cohen tried to reduce interference through a number of manipulations including changing the number of hidden units, changing the value of the learning rate parameter, overtraining on the A-B list, freezing certain connection weights, changing target values 0 and 1 instead 0.1 and 0.9. However none of these manipulations satisfactorily reduced the catastrophic interference exhibited by the networks.","Overall, McCloskey and Cohen (1989) concluded that:","Ratcliff (1990) used multiple sets of backpropagation models applied to standard recognition memory procedures, in which the items were sequentially learned.[2] After inspecting the recognition performance models he found two major problems:","Even one learning trial with new information resulted in significant of the old information, paralleling the findings of McCloskey and Cohen (1989).[1] Ratcliff also found that the resulting outputs were often a blend of the previous input and the new input. In larger networks, items learned in groups (e.g. AB then CD) were more resistant to forgetting than were items learned singly (e.g. A then B then C…). However, the forgetting for items learned in groups was still large. Adding new hidden units to the network did not reduce interference.","This finding contradicts with studies on human memory, which indicated that discrimination increases with learning. Ratcliff attempted to alleviate this problem by adding ‘response nodes’ that would selectively respond to old and new inputs. However, this method did not work as these response nodes would become active for all inputs. A model which used a context pattern also failed to increase discrimination between new and old items.","Many researchers have suggested that the main cause of catastrophic interference is overlap in the representations at the hidden layer of distributed neural networks.[11][12][13] In a distributed representation any given input will tend to create changes in the weights to many of the nodes. Catastrophic forgetting occurs because when many of the weights, where ‘knowledge is stored, are changed it is impossible for prior knowledge to be kept intact. During sequential learning, the inputs become mixed with the new input being superimposed over top of the old input.[12] Another way to conceptualize this is through visualizing learning as movement through a weight space.[14] This weight space can be likened to a spatial representation of all of the possible combinations of weights that the network can possess. When a network first learns to represent a set of patterns, it has found a point in weight space which allows it to recognize all of the patterns that is has seen.[13] However, when the network learns a new set of patterns sequentially it will move to a place in the weight space that allows it to only recognize the new pattern.[13] To recognize both sets of patterns, the network must find a place in weight space that can represent both the new and the old output. One way to do this is by connecting a hidden unit to only a subset of the input units. This reduces the likelihood that two different inputs will be encoded by the same hidden units and weights and so will decrease the chance of interference.[12] Indeed, a number of the proposed solutions to catastrophic interference involve reducing the amount of overlap that occurs when storing information in these weights.","Many of the early techniques in reducing representational overlap involved making either the input vectors or the hidden unit activation patterns orthogonal to one another. Lewandowsky and Li (1995) [15] noted that the interference between sequentially learned patterns is minimized if the input vectors are orthogonal to each other. Input vectors are said to be orthogonal to each other if the pairwise product of their elements across the two vectors sum to zero. For example, the patterns [0,0,1,0] and [0,1,0,0] are said to be orthogonal because (0x0 + 0x1 + 1x0 + 0x0) = 0. One of the techniques which can create orthogonal representations at the hidden layers involves bipolar feature coding (i.e., coding using -1 and 1 rather than 0 and 1).[13] Orthogonal patterns tend to produce less interference with each other. However, not all learning problems can be represented using these types of vectors and some studies report that the degree of interference is still problematic with orthogonal vectors.[2] Simple techniques such varying the learning rate parameters in the backpropagation equation were not successful in reducing interferece. Varying the number of hidden nodes has also been used to try and reduce interference. However, the findings have been mixed with some studies finding that more hidden units decrease interference [16] and other studies finding it does not.[1][2]",Below are a number of techniques which have empirical support in successfully reducing catastrophic interference in backpropagation neural networks:,"French(1991) [11] proposed that catastrophic interference arises in feedforward backpropagation networks due to the interaction of node activations, or activation overlap, that occur in distributed representations at the hidden layer. Specifically, he defined this activation overlap as the average shared activation over all units in the hidden layer, calculated by summing the lowest activation of the nodes at the hidden layer and averaging this sum. For example, if the activations at the hidden layer from one input are (0.3, 0.1, 0.9, 1.0) and the activations from the next input are (0.0, 0.9, 0.1, 0.9) the activation overlap would be (0.0 + 0.1 + 0.1 + 0.9 ) / 4 = 0.275. When using [binary number|binary] representation of input [row vector|vectors], activation values will be 0 through 1, where 0 indicates no activation overlap and 1 indicates full activation overlap. French noted that neural networks which employ very localized representations do not show catastrophic interference because the lack of overlap at the hidden layer. That is to say, each input pattern will create a hidden layer representation that involves the activation of only one node, so differed inputs will have an activation overlap of 0. Thus, he suggested that reducing the value of activation overlap at the hidden layer would reduce catastrophic interference in distributed networks. Specifically he proposed that this could be done through changing the distributed representations at the hidden layer to ‘semi-distributed’ representations. A ‘semi-distributed’ representation have fewer hidden nodes that are active, and/or a lower activation value for these nodes, for each representation, which will make the representations of the different inputs overlap less at the hidden layer. French recommended that this could be done through ‘activation sharpening’, a technique which slightly increases the activation of a certain number of the most active nodes in the hidden layer, slightly reduces the activation of all the other units and then changes the input-to-hidden layer weights to reflect these activation changes (similar to error backpropgation). Overall the guidelines for the process of ‘activation sharpening’ are as follows:","In his tests of an 8-8-8 (input-hidden-output) node backpropagation network where one node was sharpened, French found that this sharpening paradigm did result in one node being much more active than the other seven. Moreover, when sharpened, this network took one fourth the time to relearn the initial inputs than a standard backpropagation without node sharpening. Relearning is a measure of memory savings and thus extent of forgetting, where more time to relearn suggests more forgetting (Ebbinghaus savings method). A two-node sharpened network performed even slightly better, however if more than two nodes were sharpened forgetting increased again.","According to French, the sharpened activations interfere less with weights in the network than unsharpened weights and this is due specifically to the way that backpropagation algorithm calculates weight changes. Activations near 0 will change the weights of links less than activations near 1. Consequently, when there are many nodes with low activations (due to sharpening), the weights to and from these nodes will be modified much less than the weights on very active,nodes. As a result, when a new input is fed into the network, sharpening will reduce activation overlap by limiting the number of highly active hidden units and will reduce the likelihood of representational overlap by reducing the number of weights that are to be changed. Thus, node sharpening will decrease the amount of disruption in the old weights, which store prior input patterns, thereby reducing the likelihood of catastrophic forgetting.","Kortge (1990) [17] proposed a learning rule for training neural networks, called the ‘novelty rule’, to help alleviate catastrophic interference. As its name suggests, this rule helps the neural network to learn only the components of a new input that differ from an old input. Consequently, the novelty rule changes only the weights that were not previously dedicated to storing information, thereby reducing the overlap in representations at the hidden units. Thus, even when inputs are somewhat similar to another, dissimilar representations can be made at the hidden layer. In order to apply the novelty rule, during learning the input pattern is replaced by a novelty vector that represents the components that differ. The novelty vector for the first layer (input units to hidden units) is determined by taking the target pattern away from the current output of the network (the delta rule). For the second layer (hidden units to output units) the novelty vector is simply the activation of the hidden units that resulted from using the novelty vector as an input through the first layer. Weight changes in the network are computed by using a modified delta rule with the novelty vector replacing the activation value (sum of the inputs):","When the novelty rule is used in a standard backpropagation network there is no, or lessened, forgetting of old items when new items are presented sequentially.[17] However, this rule can only apply to auto-encoder or auto-associative networks, in which the target response for the output layer is identical to the input pattern. This is because the novelty vector would be meaningless if the desired output was not identical to the input as it would be impossible to calculate how much a new input differed from the old input.","McRae and Hetherington (1993)[12] argued that humans, unlike most neural networks, do not take on new learning tasks with a random set of weights. Rather, people tend to bring a wealth of prior knowledge to a task and this helps to avoid the problem of interference. They proposed that when a network is pre-trained on a random sample of data prior to starting a sequential learning task that this prior knowledge will naturally constrain how the new information can be incorporated. This would occur because a random sample of data from a domain which has a high degree of internal structure, such as the English language, training would capture the regularities, or recurring patterns, found within that domain. Since the domain is based on regularities, a newly learned item will tend to be similar to the previously learned information, which will allow the network to incorporate new data with little interference with existing data. Specifically, an input vector which follows the same pattern of regularities as the previously trained data should not cause a drastically different pattern of activation at the hidden layer or a drastically alter weights.","To test their hypothesis, McRae and Hetherington (1993) compared the performance of a naïve and pretrained auto-encoder backpropagation network on three simulations of verbal learning tasks. The pre-trained network was trained using letter based representations of English monosyllabic words or English word pairs. All three tasks involved the learning of some consonant-vowel-consonant (CVC) strings or CVC pairs (list A), followed by training on a second list of these items (list B). Afterwards, the distributions of the hidden node activations were compared between the naïve and pre-trained network. In all three tasks, the representations of a CVC in the naïve network tended to be spread fairly evenly across all hidden nodes, whereas most hidden nodes were inactive in the pre-trained network. Furthermore, in the pre-trained network the representational overlap between CVCs was reduced compared to the naïve network. The pre-trained network also retained some similarity information as the representational overlap between similar CVCs, like JEP and ZEP, was greater than for dissimilar CVCs, such as JEP and YUG. This suggests that the pre-trained network had a better ability to generalize, i.e. notice the patterns, than the naïve network. Most importantly, this reduction in hidden unit activation and representational overlap resulted in significantly less forgetting in the pre-trained network than the naïve network, essentially eliminating catastrophic interference. Essentially, the pre-training acted to create internal orthogonalization of the activations at the hidden layer, which reduced interference.[13] Thus, pre-training is a simple way to reduce catastrophic forgetting in standard backpropagation networks.","French (1997) proposed the idea of a pseudo-recurrent backpropagation network in order to help reduce catastrophic interference (see Figure 2).[5] In this model the network is separated into two functionally distinct but interacting sub-networks. This model is biologically inspired and is based on research from McClelland, McNaughton, and O’Reilly (1995).[18] In this research McClelland et al. (1995), suggested that the hippocampus and neocortex act as separable but complementary memory systems. Specifically, the hippocampus short term memory storage and acts gradually over time to transfer memories into the neocortex for long term memory storage. They suggest that the information that is stored can be ""brought back"" to the hippocampus during active rehearsal, reminiscence, and sleep and renewed activation is what acts to transfer the information to the neocortex over time. In the pseudo-recurrent network, one of the sub-networks acts as an early processing area, akin to the hippocampus, and functions to learn new input patters. The other sub-network acts as a final-storage area, akin to the neocortex. However, unlike in McClelland et al. (1995) model, the final-storage area sends internally generated representation back to the early processing area. This creates a recurrent network. French proposed that this interleaving of old representations with new representations is the only way to reduce radical forgetting. Since the brain would most likely not have access to the original input patterns, the patterns that would be fed back to the neocortex would be internally generated representations called pseudopatterns. These pseudopatterns are approximations of previous inputs [19] and they can be interleaved with the learning of new inputs.","The use of these pseudopatterns could be biologically plausible as parallels between the consolidation of learning that occurs during sleep and the use of interleaved pseudopatterns. Specifically, they both serve to integrate new information with old information without disruption of the old information.[20] When given an input (and a teacher value) is fed into the pseudo-recurrent network would act as follows:","When tested on sequential learning of real world patterns, categorization of edible and poisonous mushrooms, the pseudo-recurrent network was shown less interference than a standard backpropagation network. This improvement was with both memory savings and exact recognition of old patterns. When the activation patterns of the pseudo-recurrent network were investigated, it was shown that this network automatically formed semi-distributed representations. Since these types of representations involve fewer nodes being activated for each pattern, it is likely what helped to reduce interference.","Not only did the pseudo-recurrent model show reduced interference but also it models list-length and list-strength effects seen in humans. The list-length effect means that adding new items to a list harms the memory of earlier items. Like humans, the pseudo recurrent network showed a more gradual forgetting when to be trained list is lengthened. The list-strength effect means that when the strength of recognition for one item is increased, there is no effect on the recognition of the other list items. This is an important finding as other models often exhibit a decrease in the recognition of other list items when one list item is strengthened. Since the direct copying of weights from the early processing area to the final storage area does not seem highly biologically plausible, the transfer of information to the final storage area can be done through training the final storage area with pseudopatterns created by the early processing area. However, a disadvantage of the pseudo-recurrent model is that the number of hidden units in the early processing and final storage sub-networks must be identical.","Following the same basic idea contributed by Robins,[19][20] Ans and Rousset (1997)[21] have also proposed a two-network artificial neural architecture with memory self-refreshing that overcomes catastrophic interference when sequential learning tasks are carried out in distributed networks trained by backpropagation. The principle is to interleave, at the time when new external patterns are learned, those to-be-learned new external patterns with internally generated pseudopatterns, or ‘pseudo-memories’, that reflect the previously learned information. What mainly distinguishes this model from those that use classical pseudorehearsal in feedforward multilayer networks is a reverberating process that is used for generating pseudopatterns. This process which, after a number of activity re-injections from a single random seed, tends to go up to nonlinear network attractors, is more suitable for optimally capturing the deep structure of previously learned knowledge than a single feedforward pass of random activation. Ans and Rousset (2000)[22] have shown that the learning mechanism avoiding catastrophic forgetting they proposed provides a more appropriate way to deal with knowledge transfer as measured by learning speed, ability to generalize and vulnerability to network damages. Musca, Rousset and Ans (2009)[23] have also shown that pseudopatterns originating from an artificial reverberating neural network could induce familiarity in humans with never seen items in the way predicted by simulations conducted with a two-network artificial neural architecture. Furthermore, Ans (2004)[24] has implemented a version of the self-refreshing mechanism using only one network trained by the Contrastive Hebbian Learning rule, training rule considered as more realistic than the largely used backpropagation algorithm, but fortunately equivalent to the latter.[25]
So far, the different solutions to catastrophic interference that have been presented concern tasks of sequential learning involving only non-temporally ordered lists of items. But, to be credible, the self-refreshing mechanism for ‘static’ learning has to encompass our human ability to learn serially many temporal sequences of patterns without catastrophic interference (e.g. learning one song followed by learning a second song without forgetting the first one). This was done by Ans, Rousset, French and Musca (2004)[26] who have presented, in addition to simulation work, an experiment that evidences a close similarity between the behaviour of humans and the behaviour of the proposed neuromimetic architecture.","Latent Learning is a technique used by Gutstein & Stump (2015) [27] both to mitigate catastrophic interference and to take advantage of transfer learning. Rather than manipulating the representations for new classes used by the hidden nodes, this approach tries to train optimal representations for new classes into the output nodes. It chooses output encodings that are least likely to catastrophically interfere with existing responses.","Given a net that has learned to discriminate among one set of classes using Error Correcting Output Codes (ECOC) [28] (as opposed to 1 hot codes), optimal encodings for new classes are chosen by observing the net's average responses to them. Since these average responses arose while learning the original set of classes without any exposure to the new classes, they are referred to as 'Latently Learned Encodings'. This terminology borrows from the concept of Latent Learning, as introduced by Tolman in 1930.[29] In effect, this technique uses transfer learning to avoid catastrophic interference, by making a net's responses to new classes as consistent as possible with existing responses to classes already learned.","Practopoietic theory[30] proposes that biological systems solve the problem of catastrophic interference by storing long-term memories only in a general form, not applicable to a given situation but instead loosely applicable to a class of different situations. In order to adjust the loosely applicable knowledge to the given current situation, the process of anapoiesis is applied. Anapoiesis stands for ""reconstruction of knowledge""—transforming knowledge from a general form to a specific one. Practopoietic theory is founded in the theorems of cybernetics and is concerned with the question of how cybernetic systems obtain their capabilities to control and act."
"Category utility is a measure of ""category goodness"" defined in Gluck & Corter (1985) and Corter & Gluck (1992). It attempts to maximize both the probability that two objects in the same category have attribute values in common, and the probability that objects from different categories have different attribute values. It was intended to supersede more limited measures of category goodness such as ""cue validity"" (Reed 1972; Rosch & Mervis 1975) and ""collocation index"" (Jones 1983). It provides a normative information-theoretic measure of the predictive advantage gained by the observer who possesses knowledge of the given category structure (i.e., the class labels of instances) over the observer who does not possess knowledge of the category structure. In this sense the motivation for the category utility measure is similar to the information gain metric used in decision tree learning. In certain presentations, it is also formally equivalent to the mutual information, as discussed below. A review of category utility in its probabilistic incarnation, with applications to machine learning, is provided in Witten & Frank (2005, pp. 260–262).",,,The probability-theoretic definition of category utility given in Fisher (1987) and Witten & Frank (2005) is as follows:,"where 



F
=
{

f

i


}
,
 
i
=
1
…
n


{\displaystyle F=\{f_{i}\},\ i=1\ldots n}

 is a size-



n
 


{\displaystyle n\ }

 set of 



m
 


{\displaystyle m\ }

-ary features, and 



C
=
{

c

j


}
 
j
=
1
…
p


{\displaystyle C=\{c_{j}\}\ j=1\ldots p}

 is a set of 



p
 


{\displaystyle p\ }

 categories. The term 



p
(

f

i
k


)
 


{\displaystyle p(f_{ik})\ }

 designates the marginal probability that feature 




f

i


 


{\displaystyle f_{i}\ }

 takes on value 



k
 


{\displaystyle k\ }

, and the term 



p
(

f

i
k



|


c

j


)
 


{\displaystyle p(f_{ik}|c_{j})\ }

 designates the category-conditional probability that feature 




f

i


 


{\displaystyle f_{i}\ }

 takes on value 



k
 


{\displaystyle k\ }

 given that the object in question belongs to category 




c

j


 


{\displaystyle c_{j}\ }

.","The motivation and development of this expression for category utility, and the role of the multiplicand 







1
p






{\displaystyle \textstyle {\tfrac {1}{p}}}

 as a crude overfitting control, is given in the above sources. Loosely (Fisher 1987), the term 




p
(

c

j


)

∑


f

i


∈
F



∑

k
=
1


m


p
(

f

i
k



|


c

j



)

2





{\displaystyle \textstyle p(c_{j})\sum _{f_{i}\in F}\sum _{k=1}^{m}p(f_{ik}|c_{j})^{2}}

 is the expected number of attribute values that can be correctly guessed by an observer using a probability-matching strategy together with knowledge of the category labels, while 




p
(

c

j


)

∑


f

i


∈
F



∑

k
=
1


m


p
(

f

i
k



)

2





{\displaystyle \textstyle p(c_{j})\sum _{f_{i}\in F}\sum _{k=1}^{m}p(f_{ik})^{2}}

 is the expected number of attribute values that can be correctly guessed by an observer the same strategy but without any knowledge of the category labels. Their difference therefore reflects the relative advantage accruing to the observer by having knowledge of the category structure.","The information-theoretic definition of category utility for a set of entities with size-



n
 


{\displaystyle n\ }

 binary feature set 



F
=
{

f

i


}
,
 
i
=
1
…
n


{\displaystyle F=\{f_{i}\},\ i=1\ldots n}

, and a binary category 



C
=
{
c
,



c
¯



}


{\displaystyle C=\{c,{\bar {c}}\}}

 is given in Gluck & Corter (1985) as follows:","where 



p
(
c
)
 


{\displaystyle p(c)\ }

 is the prior probability of an entity belonging to the positive category 



c
 


{\displaystyle c\ }

 (in the absence of any feature information), 



p
(

f

i



|

c
)
 


{\displaystyle p(f_{i}|c)\ }

 is the conditional probability of an entity having feature 




f

i


 


{\displaystyle f_{i}\ }

 given that the entity belongs to category 



c
 


{\displaystyle c\ }

, 



p
(

f

i



|




c
¯



)


{\displaystyle p(f_{i}|{\bar {c}})}

 is likewise the conditional probability of an entity having feature 




f

i


 


{\displaystyle f_{i}\ }

 given that the entity belongs to category 






c
¯





{\displaystyle {\bar {c}}}

, and 



p
(

f

i


)
 


{\displaystyle p(f_{i})\ }

 is the prior probability of an entity possessing feature 




f

i


 


{\displaystyle f_{i}\ }

 (in the absence of any category information).","The intuition behind the above expression is as follows: The term 



p
(
c
)


∑

i
=
1


n


p
(

f

i



|

c
)
log
⁡
p
(

f

i



|

c
)



{\displaystyle p(c)\textstyle \sum _{i=1}^{n}p(f_{i}|c)\log p(f_{i}|c)}

 represents the cost (in bits) of optimally encoding (or transmitting) feature information when it known that the objects to be described belong to category 



c
 


{\displaystyle c\ }

. Similarly, the term 



p
(



c
¯



)


∑

i
=
1


n


p
(

f

i



|




c
¯



)
log
⁡
p
(

f

i



|




c
¯



)



{\displaystyle p({\bar {c}})\textstyle \sum _{i=1}^{n}p(f_{i}|{\bar {c}})\log p(f_{i}|{\bar {c}})}

 represents the cost (in bits) of optimally encoding (or transmitting) feature information when it known that the objects to be described belong to category 






c
¯





{\displaystyle {\bar {c}}}

. The sum of these two terms in the brackets is therefore the weighted average of these two costs. The final term, 





∑

i
=
1


n


p
(

f

i


)
log
⁡
p
(

f

i


)



{\displaystyle \textstyle \sum _{i=1}^{n}p(f_{i})\log p(f_{i})}

, represents the cost (in bits) of optimally encoding (or transmitting) feature information when no category information is available. The value of the category utility will, in the above formulation, be negative (???).","It is mentioned in Gluck & Corter (1985) and Corter & Gluck (1992) that the category utility is equivalent to the mutual information. Here we provide a simple demonstration of the nature of this equivalence. Let us assume a set of entities each having the same 



n


{\displaystyle n}

 features, i.e., feature set 



F
=
{

f

i


}
,
 
i
=
1
…
n


{\displaystyle F=\{f_{i}\},\ i=1\ldots n}

, with each feature variable having cardinality 



m


{\displaystyle m}

. That is, each feature has the capacity to adopt any of 



m


{\displaystyle m}

 distinct values (which need not be ordered; all variables can be nominal); for the special case 



m
=
2


{\displaystyle m=2}

 these features would be considered binary, but more generally, for any 



m


{\displaystyle m}

, the features are simply m-ary. For our purposes, without loss of generality, we can replace feature set 



F


{\displaystyle F}

 with a single aggregate variable 




F

a




{\displaystyle F_{a}}

 that has cardinality 




m

n




{\displaystyle m^{n}}

, and adopts a unique value 




v

i


,
 
i
=
1
…

m

n




{\displaystyle v_{i},\ i=1\ldots m^{n}}

 corresponding to each feature combination in the Cartesian product 



⊗
F


{\displaystyle \otimes F}

. (Ordinality does not matter, because the mutual information is not sensitive to ordinality.) In what follows, a term such as 



p
(

F

a


=

v

i


)


{\displaystyle p(F_{a}=v_{i})}

 or simply 



p
(

v

i


)


{\displaystyle p(v_{i})}

 refers to the probability with which 




F

a




{\displaystyle F_{a}}

 adopts the particular value 




v

i




{\displaystyle v_{i}}

. (Using the aggregate feature variable 




F

a




{\displaystyle F_{a}}

 replaces multiple summations, and simplifies the presentation to follow.)","We assume also a single category variable 



C


{\displaystyle C}

, which has cardinality 



p


{\displaystyle p}

. This is equivalent to a classification system in which there are 



p


{\displaystyle p}

 non-intersecting categories. In the special case of 



p
=
2


{\displaystyle p=2}

 we have the two-category case discussed above. From the definition of mutual information for discrete variables, the mutual information 



I
(

F

a


;
C
)


{\displaystyle I(F_{a};C)}

 between the aggregate feature variable 




F

a




{\displaystyle F_{a}}

 and the category variable 



C


{\displaystyle C}

 is given by:","where 



p
(

v

i


)


{\displaystyle p(v_{i})}

 is the prior probability of feature variable 




F

a




{\displaystyle F_{a}}

 adopting value 




v

i




{\displaystyle v_{i}}

, 



p
(

c

j


)


{\displaystyle p(c_{j})}

 is the marginal probability of category variable 



C


{\displaystyle C}

 adopting value 




c

j




{\displaystyle c_{j}}

, and 



p
(

v

i


,

c

j


)


{\displaystyle p(v_{i},c_{j})}

 is the joint probability of variables 




F

a




{\displaystyle F_{a}}

 and 



C


{\displaystyle C}

 simultaneously adopting those respective values. In terms of the conditional probabilities this can be re-written (or defined) as","If we will rewrite the original definition of the category utility from above, with 



C
=
{
c
,



c
¯



}


{\displaystyle C=\{c,{\bar {c}}\}}

, we have","This equation clearly has the same form as the (blue) equation expressing the mutual information between the feature set and the category variable; the difference is that the sum 





∑


f

i


∈
F





{\displaystyle \textstyle \sum _{f_{i}\in F}}

 in the category utility equation runs over independent binary variables 



F
=
{

f

i


}
,
 
i
=
1
…
n


{\displaystyle F=\{f_{i}\},\ i=1\ldots n}

, whereas the sum 





∑


v

i


∈

F

a







{\displaystyle \textstyle \sum _{v_{i}\in F_{a}}}

 in the mutual information runs over values of the single 




m

n




{\displaystyle m^{n}}

-ary variable 




F

a




{\displaystyle F_{a}}

. The two measures are actually equivalent then only when the features 



{

f

i


}


{\displaystyle \{f_{i}\}}

, are independent (and assuming that terms in the sum corresponding to 



p
(




f

i


¯



)


{\displaystyle p({\bar {f_{i}}})}

 are also added).","Like the mutual information, the category utility is not sensitive to any ordering in the feature or category variable values. That is, as far as the category utility is concerned, the category set {small,medium,large,jumbo} is not qualitatively different from the category set {desk,fish,tree,mop} since the formulation of the category utility does not account for any ordering of the class variable. Similarly, a feature variable adopting values {1,2,3,4,5} is not qualitatively different from a feature variable adopting values {fred,joe,bob,sue,elaine}. As far as the category utility or mutual information are concerned, all category and feature variables are nominal variables. For this reason, category utility does not reflect any gestalt aspects of ""category goodness"" that might be based on such ordering effects. One possible adjustment for this insensitivity to ordinality is given by the weighting scheme described in the article for mutual information.","This section provides some background on the origins of, and need for, formal measures of ""category goodness"" such as the category utility, and some of the history that lead to the development of this particular metric.","At least since the time of Aristotle there has been a tremendous fascination in philosophy with the nature of concepts and universals. What kind of entity is a concept such as ""horse""? Such abstractions do not designate any particular individual in the world, and yet we can scarcely imagine being able to comprehend the world without their use. Does the concept ""horse"" therefore have an independent existence outside of the mind? If it does, then what is the locus of this independent existence? The question of locus was an important issue on which the classical schools of Plato and Aristotle famously differed. However, they remained in agreement that universals did indeed have a mind-independent existence. There was, therefore, always a fact to the matter about which concepts and universals exist in the world.","In the late Middle Ages (perhaps beginning with Occam, although Porphyry also makes a much earlier remark indicating a certain discomfort with the status quo), however, the certainty that existed on this issue began to erode, and it became acceptable among the so-called nominalists and empiricists to consider concepts and universals as strictly mental entities or conventions of language. On this view of concepts—that they are purely representational constructs—a new question then comes to the fore: Why do we possess one set of concepts rather than another? What makes one set of concepts ""good"" and another set of concepts ""bad""? This is a question that modern philosophers, and subsequently machine learning theorists and cognitive scientists, have struggled with for many decades.","One approach to answering such questions is to investigate the ""role"" or ""purpose"" of concepts in cognition. Thus, we ask: What are concepts good for in the first place? The answer provided by Mill (1843/1936, p. 425) and many others is that classification (conception) is a precursor to induction: By imposing a particular categorization on the universe, an organism gains the ability to deal with physically non-identical objects or situations in an identical fashion, thereby gaining substantial predictive leverage (Smith & Medin 1981;Harnad 2005). As J.S. Mill puts it (Mill 1843/1936, pp. 466–468),","The general problem of classification... [is] to provide that things shall be thought of in such groups, and those groups in such an order, as will best conduce to the remembrance and to the ascertainment of their laws... [and] one of the uses of such a classification that by drawing attention to the properties on which it is founded, and which, if the classification be good, are marks of many others, it facilitates the discovery of those others.","From this base, Mill reaches the following conclusion, which foreshadows much subsequent thinking about category goodness, including the notion of category utility:","The ends of scientific classification are best answered when the objects are formed into groups respecting which a greater number of general propositions can be made, and those propositions more important, than could be made respecting any other groups into which the same things could be distributed. The properties, therefore, according to which objects are classified should, if possible, be those which are causes of many other properties; or, at any rate, which are sure marks of them.","One may compare this to the ""category utility hypothesis"" proposed by Corter & Gluck (1992): ""A category is useful to the extent that it can be expected to improve the ability of a person to accurately predict the features of instances of that category."" Mill here seems to be suggesting that the best category structure is one in which object features (properties) are maximally informative about the object's class, and, simultaneously, the object class is maximally informative about the object's features. In other words, a useful classification scheme is one in which we can use category knowledge to accurately infer object properties, and we can use property knowledge to accurately infer object classes. One may also compare this idea to Aristotle's criterion of counter-predication for definitional predicates, as well as to the notion of concepts described in formal concept analysis.","A variety of different measures have been suggested with an aim of formally capturing this notion of ""category goodness,"" the best known of which is probably the ""cue validity"". Cue validity of a feature 




f

i


 


{\displaystyle f_{i}\ }

 with respect to category 




c

j


 


{\displaystyle c_{j}\ }

 is defined as the conditional probability of the category given the feature (Reed 1972;Rosch & Mervis 1975;Rosch 1978), 



p
(

c

j



|


f

i


)
 


{\displaystyle p(c_{j}|f_{i})\ }

, or as the deviation of the conditional probability from the category base rate (Edgell 1993;Kruschke & Johansen 1999), 



p
(

c

j



|


f

i


)
−
p
(

c

j


)
 


{\displaystyle p(c_{j}|f_{i})-p(c_{j})\ }

. Clearly, these measures quantify only inference from feature to category (i.e., cue validity), but not from category to feature, i.e., the category validity 



p
(

f

i



|


c

j


)
 


{\displaystyle p(f_{i}|c_{j})\ }

. Also, while the cue validity was originally intended to account for the demonstrable appearance of basic categories in human cognition—categories of a particular level of generality that are evidently preferred by human learners—a number of major flaws in the cue validity quickly emerged in this regard (Jones 1983;Murphy 1982;Corter & Gluck 1992, and others).","One attempt to address both problems by simultaneously maximizing both feature validity and category validity was made by Jones (1983) in defining the ""collocation index"" as the product 



p
(

c

j



|


f

i


)
p
(

f

i



|


c

j


)
 


{\displaystyle p(c_{j}|f_{i})p(f_{i}|c_{j})\ }

, but this construction was fairly ad hoc (see Corter & Gluck 1992). The category utility was introduced as a more sophisticated refinement of the cue validity, which attempts to more rigorously quantify the full inferential power of a class structure. As shown above, on a certain view the category utility is equivalent to the mutual information between the feature variable and the category variable. It has been suggested that categories having the greatest overall category utility are those that are not only those ""best"" in a normative sense, but also those human learners prefer to use, e.g., ""basic"" categories (Corter & Gluck 1992). Other related measures of category goodness are ""cohesion"" (Hanson & Bauer 1989;Gennari, Langley & Fisher 1989) and ""salience"" (Gennari 1989)."
The Center for Biological & Computational Learning is a research lab at the Massachusetts Institute of Technology.,"CBCL was established in 1992 with support from the National Science Foundation. It is based in the Department of Brain & Cognitive Sciences at MIT, and is associated with the McGovern Institute for Brain Research, and the MIT Computer Science and Artificial Intelligence Laboratory.","It was founded with the belief that learning is at the very core of the problem of intelligence, both biological and artificial. Learning is thus the gateway to understanding how the human brain works and for making intelligent machines. CBCL studies the problem of learning within a multidisciplinary approach. Its main goal is to nurture serious research on the mathematics, the engineering and the neuroscience of learning.","Research is focused on the problem of learning in theory, engineering applications, and neuroscience.","In computational neuroscience, the center has developed a model of the ventral stream in the visual cortex which accounts for much of the physiological data, and psychophysical experiments in difficult object recognition tasks. The model performs at the level of the best computer vision systems[citation needed].",Tomaso Poggio director of CBCL
"The computational intelligence and machine learning (CIML) community portal is an international multi-university initiative. Its primary purpose is to help facilitate a virtual scientific community infrastructure for all those involved with, or interested in, computational intelligence and machine learning. This includes CIML research-,education, and application-oriented resources residing at the portal and others that are linked from the CIML site.",,,"The CIML community portal was created to facilitate an online virtual scientific community wherein anyone interested in CIML can share research, obtain resources, or simply learn more. The effort is currently led by Jacek Zurada (principal investigator), with Rammohan Ragade and Janusz Wojtusiak, aided by a team of 25 volunteer researchers from 13 different countries.[1][2]","The ultimate goal of the CIML community portal is to accommodate and cater to a broad range of users, including experts, students, the public, and outside researchers interested in using CIML methods and software tools. Each community member and user will be guided through the portal resources and tools based on their respective CIML experience (e.g. expert, student, outside researcher) and goals (e.g. collaboration, education). A preliminary version of the community's portal, with limited capabilities, is now operational and available for users.[3] All electronic resources on the portal are peer-reviewed to ensure high quality and cite-ability for literature."
"Cleverbot is a web application that uses an artificial intelligence algorithm to have conversations with humans. It was created by the British AI scientist Rollo Carpenter, who also created Jabberwacky, a similar web application. In its first decade, Cleverbot held several thousand conversations with Carpenter and his associates. Since launching on the web in 1997, the number of conversations held has exceeded 200 million. Besides the web application, Cleverbot is also available as an iOS, Android, and Windows Phone app.[2]",,,"Unlike other chatterbots, Cleverbot's responses are not programmed. Instead, it ""learns"" from human input; Humans type into the box below the Cleverbot logo and the system finds all keywords or an exact phrase matching the input. After searching through its saved conversations, it responds to the input by finding how a human responded to that input when it was asked, in part or in full, by Cleverbot.[3][4]","Cleverbot participated in a formal Turing test at the 2011 Techniche festival at the Indian Institute of Technology Guwahati on September 3, 2011. Out of the 334 votes cast, Cleverbot was judged to be 59.3% human, compared to the rating of 63.3% human achieved by human participants. A score of 50.05% or higher is often considered to be a passing grade.[5] The software running for the event had to handle just 1 or 2 simultaneous requests, whereas online Cleverbot is usually talking to around 80,000 people at once.","Cleverbot is constantly ""learning"", growing in data size, and perhaps also in the degree of ""intelligence"" it appears to display. Updates to the software have been mostly behind the scenes. In 2014 Cleverbot was upgraded to use GPU serving techniques.[6] The program chooses how to respond to users fuzzily, and contextually, the whole of the conversation being compared to the millions that have taken place before. The Cleverbot database now has over 265 million rows, using several Big Data techniques and more recently with Machine Learning.[citation needed]","A significant part of the engine behind Cleverbot, and an API for access to serving, has been made available to developers in the form of Cleverscript. A service for directly accessing Cleverbot has been made available to developers in the form of Cleverbot.io.","An app that uses the Cleverscript engine to play a game of 20 Questions, has been launched under the name Clevernator. Unlike other such games, the player asks the questions and it is the role of the AI to understand, and answer factually. An app that allows owners to create and talk to their own small Cleverbot-like AI has been launched, called Cleverme! for Apple products.[7]"
Cognitive robotics is concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition.,,,"While traditional cognitive modeling approaches have assumed symbolic coding schemes as a means for depicting the world, translating the world into these kinds of symbolic representations has proven to be problematic if not untenable. Perception and action and the notion of symbolic representation are therefore core issues to be addressed in cognitive robotics.","Cognitive robotics views animal cognition as a starting point for the development of robotic information processing, as opposed to more traditional Artificial Intelligence techniques. Target robotic cognitive capabilities include perception processing, attention allocation, anticipation, planning, complex motor coordination, reasoning about other agents and perhaps even about their own mental states. Robotic cognition embodies the behavior of intelligent agents in the physical world (or a virtual world, in the case of simulated cognitive robotics). Ultimately the robot must be able to act in the real world.","A preliminary robot learning technique called motor babbling involves correlating pseudo-random complex motor movements by the robot with resulting visual and/or auditory feedback such that the robot may begin to expect a pattern of sensory feedback given a pattern of motor output. Desired sensory feedback may then be used to inform a motor control signal. This is thought to be analogous to how a baby learns to reach for objects or learns to produce speech sounds. For simpler robot systems, where for instance inverse kinematics may feasibly be used to transform anticipated feedback (desired motor result) into motor output, this step may be skipped.","Once a robot can coordinate its motors to produce a desired result, the technique of learning by imitation may be used. The robot monitors the performance of another agent and then the robot tries to imitate that agent. It is often a challenge to transform imitation information from a complex scene into a desired motor result for the robot. Note that imitation is a high-level form of cognitive behavior and imitation is not necessarily required in a basic model of embodied animal cognition.","A more complex learning approach is ""autonomous knowledge acquisition"": the robot is left to explore the environment on its own. A system of goals and beliefs is typically assumed.","A somewhat more directed mode of exploration can be achieved by ""curiosity"" algorithms, such as Intelligent Adaptive Curiosity[1][2] or Category-Based Intrinsic Motivation.[3] These algorithms generally involve breaking sensory input into a finite number of categories and assigning some sort of prediction system (such as an Artificial Neural Network) to each. The prediction system keeps track of the error in its predictions over time. Reduction in prediction error is considered learning. The robot then preferentially explores categories in which it is learning (or reducing prediction error) the fastest.",Some researchers in cognitive robotics have tried using architectures such as (ACT-R and Soar (cognitive architecture)) as a basis of their cognitive robotics programs. These highly modular symbol-processing architectures have been used to simulate operator performance and human performance when modeling simplistic and symbolized laboratory data. The idea is to extend these architectures to handle real-world sensory input as that input continuously unfolds through time. What is needed is a way to somehow translate the world into symbols.,Some of the fundamental questions to still be answered in cognitive robotics are:,"Cognitive Robotics book [4] by Hooman Samani,[5] takes a multidiciplinary approach to cover various aspects of cognitive robotics such as artificial intelligence, physical, chemical, philosophical, psychological, social, cultural, and ethical aspects."
A committee machine is a type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response.[1] The combined response of the committee machine is supposed to be superior to those of its constituent experts. Compare with ensembles of classifiers.,,,"In this class of committee machines, the responses of several predictors (experts) are combined by means of a mechanism that does not involve the input signal, hence the designation static. This category includes the following methods:","In ensemble averaging, outputs of different predictors are linearly combined to produce an overall output.","In boosting, a weak algorithm is converted into one that achieves arbitrarily high accuracy.","In this second class of committee machines, the input signal is directly involved in actuating the mechanism that integrates the outputs of the individual experts into an overall output, hence the designation dynamic. There are two kinds of dynamic structures:","In mixture of experts, the individual responses of the experts are non-linearly combined by means of a single gating network.","In hierarchical mixture of experts, the individual responses of the individual experts are non-linearly combined by means of several gating networks arranged in a hierarchical fashion."
"En théorie de l'apprentissage (apprentissage automatique par exemple), La complexité de Rademacher, nommée d'après Hans Rademacher, mesure la richesse d'une classe de fonctions à valeur réelle, selon une distribution de probabilité.","Etant donné des observations 



S
=
(

z

1


,

z

2


,
…
,

z

m


)
∈

Z

m




{\displaystyle S=(z_{1},z_{2},\dots ,z_{m})\in Z^{m}}

, et une classe 





H




{\displaystyle {\mathcal {H}}}

 de fonctions à valeurs réelles définies sur un espace 



Z


{\displaystyle Z}

, la Complexité empirique de Rademacher de 





H




{\displaystyle {\mathcal {H}}}

 est définie comme :","








R

^




S


(


H


)
=


2
m



E


[

sup

h
∈


H





|

∑

i
=
1


m



σ

i


h
(

z

i


)
|

 


|


 
S
]



{\displaystyle {\widehat {\mathcal {R}}}_{S}({\mathcal {H}})={\frac {2}{m}}\mathbb {E} \left[\sup _{h\in {\mathcal {H}}}\left|\sum _{i=1}^{m}\sigma _{i}h(z_{i})\right|\ {\bigg |}\ S\right]}

","où 




σ

1


,

σ

2


,
…
,

σ

m




{\displaystyle \sigma _{1},\sigma _{2},\dots ,\sigma _{m}}

 sont des variables aléatoires indépendentes, tirées selon la distribution de Rademacher i.e. 



Pr
(

σ

i


=
+
1
)
=
Pr
(

σ

i


=
−
1
)
=
1

/

2


{\displaystyle \Pr(\sigma _{i}=+1)=\Pr(\sigma _{i}=-1)=1/2}

 for 



i
=
1
,
2
,
…
,
m


{\displaystyle i=1,2,\dots ,m}

.","Soit 



P


{\displaystyle P}

, la distribution de probabilité sur 



Z


{\displaystyle Z}

. La Complexité de Rademacher de la classe de fonction 





H




{\displaystyle {\mathcal {H}}}

 selon 



P


{\displaystyle P}

 pour des données de taille 



m


{\displaystyle m}

 est :","






R



m


(


H


)
=

E


[





R

^




S


(


H


)
]



{\displaystyle {\mathcal {R}}_{m}({\mathcal {H}})=\mathbb {E} \left[{\widehat {\mathcal {R}}}_{S}({\mathcal {H}})\right]}

","où les espérances, ci-dessus, sont calculées selon des observations indépendentes et identiquement distribuées (i.i.d.) 



S
=
(

z

1


,

z

2


,
…
,

z

m


)


{\displaystyle S=(z_{1},z_{2},\dots ,z_{m})}

 générées selon 



P


{\displaystyle P}

.","On peut montrer, par exemple, qu'il existe une constante 



C


{\displaystyle C}

, telle que n'importe quelle classe de fonctions indicatrices sur 



{
0
,
1
}


{\displaystyle \{0,1\}}

 avec la dimension de Vapnik-Chervonenkis 



d


{\displaystyle d}

 a la complexité de Rademacher majorée par 



C



d
m





{\displaystyle C{\sqrt {\frac {d}{m}}}}

.","La complexité gaussienne est une mesure de complexité similaire, avec des interprétations physique similaire. Elle peut être obtenue à partir de la complexité précédente en utilisant les variables aléatoires 




g

i




{\displaystyle g_{i}}

 au lieu de 




σ

i




{\displaystyle \sigma _{i}}

, où 




g

i




{\displaystyle g_{i}}

 sont des variables aléatoires gaussiennes i.i.d, de moyenne nulle et de variance 1, i.e. 




g

i


∼


N



(
0
,
1
)



{\displaystyle g_{i}\sim {\mathcal {N}}\left(0,1\right)}

."
"In computer science, computational learning theory (or just learning theory) is a subfield of Artificial Intelligence devoted to studying the design and analysis of machine learning algorithms.[1]",,,"Theoretical results in machine learning mainly deal with a type of inductive learning called supervised learning. In supervised learning, an algorithm is given samples that are labeled in some useful way. For example, the samples might be descriptions of mushrooms, and the labels could be whether or not the mushrooms are edible. The algorithm takes these previously labeled samples and uses them to induce a classifier. This classifier is a function that assigns labels to samples including the samples that have never been previously seen by the algorithm. The goal of the supervised learning algorithm is to optimize some measure of performance such as minimizing the number of mistakes made on new samples.","In addition to performance bounds, computational learning theory studies the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results:","Negative results often rely on commonly believed, but yet unproven assumptions, such as:","There are several different approaches to computational learning theory. These differences are based on making assumptions about the inference principles used to generalize from limited data. This includes different definitions of probability (see frequency probability, Bayesian probability) and different assumptions on the generation of samples. The different approaches include:","Computational learning theory has led to several practical algorithms. For example, PAC theory inspired boosting, VC theory led to support vector machines, and Bayesian inference led to belief networks (by Judea Pearl).",A description of some of these publications is given at important publications in machine learning.
"In predictive analytics and machine learning, the concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.","The term concept refers to the quantity to be predicted. More generally, it can also refer to other phenomena of interest besides the target concept, such as an input, but, in the context of concept drift, the term commonly refers to the target variable.",,,"In a fraud detection application the target concept may be a binary attribute FRAUDULENT with values ""yes"" or ""no"" that indicates whether a given transaction is fraudulent. Or, in a weather prediction application, there may be several target concepts such as TEMPERATURE, PRESSURE, and HUMIDITY.","The behavior of the customers in an online shop may change over time. For example, if weekly merchandise sales are to be predicted, and a predictive model has been developed that works satisfactorily. The model may use inputs such as the amount of money spent on advertising, promotions being run, and other metrics that may affect sales. The model is likely to become less and less accurate over time - this is concept drift. In the merchandise sales application, one reason for concept drift may be seasonality, which means that shopping behavior changes seasonally. Perhaps there will be higher sales in the winter holiday season than during the summer, for example.","To prevent deterioration in prediction accuracy because of concept drift, both active and passive solutions can be adopted. Active solutions rely on triggering mechanisms, e.g., change-detection tests (Basseville and Nikiforov 1993; Alippi and Roveri, 2007) to explicitly detect concept drift as a change in the statistics of the data-generating process. In stationary conditions, any fresh information made available can be integrated to improve the model. Differently, when concept drift is detected, the current model is no more up-to-date and must be substituted with a new one to maintain the prediction accuracy (Gama et al., 2004; Alippi et al., 2011). On the contrary, in passive solutions the model is continuously updated, e.g., by retraining the model on the most recently observed samples (Widmer and Kubat, 1996), or enforcing an ensemble of classifiers (Elwell and Polikar 2011).","Contextual information, when available, can be used to better explain the causes of the concept drift: for instance, in the sales prediction application, concept drift might be compensated by adding information about the season to the model. By providing information about the time of the year, the rate of deterioration of your model is likely to decrease, concept drift is unlikely to be eliminated altogether. This is because actual shopping behavior does not follow any static, finite model. New factors may arise at any time that influence shopping behavior, the influence of the known factors or their interactions may change.","Concept drift cannot be avoided for complex phenomenon that are not governed by fixed laws of nature. All processes that arise from human activity, such as socioeconomic processes, and biological processes are likely to experience concept drift. Therefore periodic retraining, also known as refreshing, of any model is necessary.","Announcements, discussions, job postings related to the topic of concept drift in data mining / machine learning. Posts are moderated.",To subscribe go to the group home page: https://groups.google.com/group/conceptdrift,"Many papers have been published describing algorithms for concept drift detection. Only reviews, surveys and overviews are here:"
"Concept learning, also known as category learning, concept attainment, and concept formation, is largely based on the works of the cognitive psychologist Jerome Bruner. Bruner, Goodnow, & Austin (1967) defined concept attainment (or concept learning) as ""the search for and listing of attributes that can be used to distinguish exemplars from non exemplars of various categories."" More simply put, concepts are the mental categories that help us classify objects, events, or ideas, building on the understanding that each object, event, or idea has a set of common relevant features. Thus, concept learning is a strategy which requires a learner to compare and contrast groups or categories that contain concept-relevant features with groups or categories that do not contain concept-relevant features.","Concept learning also refers to a learning task in which a human or machine learner is trained to classify objects by being shown a set of example objects along with their class labels. The learner simplifies what has been observed by condensing it in the form of an example. This simplified version of what has been learned is then applied to future examples. Concept learning may be simple or complex because learning takes place over many areas. When a concept is difficult, it is less likely that the learner will be able to simplify, and therefore will be less likely to learn. Colloquially, the task is known as learning from examples. Most theories of concept learning are based on the storage of exemplars and avoid summarization or overt abstraction of any kind.",,,Concrete or Perceptual Concepts vs Abstract Concepts,Defined (or Relational) and Associated Concepts,Complex Concepts. Constructs such as a schema and a script are examples of complex concepts. A schema is an organization of smaller concepts (or features) and is revised by situational information to assist in comprehension. A script on the other hand is a list of actions that a person follows in order to complete a desired goal. An example of a script would be the process of buying a CD. There are several actions that must occur before the actual act of purchasing the CD and a script provides a sequence of the necessary actions and proper order of these actions in order to be successful in purchasing the CD.,"Discovery - Every baby discovers concepts for itself, such as discovering that each of its fingers can be individually controlled or that care givers are individuals. Although this is perception driven, formation of the concept is more than memorizing perceptions.","Examples - Supervised or unsupervised generalizing from examples may lead to learning a new concept, but concept formation is more than generalizing from examples.","Words - Hearing or reading new words leads to learning new concepts, but forming a new concept is more than learning a dictionary definition. A person may have previously formed a new concept before encountering the word or phrase for it.","Exemplars comparison and contrast - An efficient way to learn new categories and to induce new categorization rules is by comparing a few example objects while being informed about their categorical relation. Comparing two exemplars while being informed that the two are from the same category allows identifying the attributes shared by the category members , as it exemplifies variability within this category. On the other hand, contrasting two exemplars while being informed that the two are from different categories may allow identifying attributes with diagnostic value. Within category comparison and between categories contrast are not similarly useful for category learning (Hammer et al., 2008), and the capacity to use these two forms of comparison-based learning changes at childhood (Hammer et al., 2009).","Invention - When prehistoric people who lacked tools used their fingernails to scrape food from killed animals or smashed melons, they noticed that a broken stone sometimes had a sharp edge like a fingernail and was therefore suitable for scraping food. Inventing a stone tool to avoid broken fingernails was a new concept.","In general, the theoretical issues underlying concept learning are those underlying induction. These issues are addressed in many diverse publications, including literature on subjects like Version Spaces, Statistical Learning Theory, PAC Learning, Information Theory, and Algorithmic Information Theory. Some of the broad theoretical ideas are also discussed by Watanabe (1969,1985), Solomonoff (1964a,1964b), and Rendell (1986); see the reference list below.","It is difficult to make any general statements about human (or animal) concept learning without already assuming a particular psychological theory of concept learning. Although the classical views of concepts and concept learning in philosophy speak of a process of abstraction, data compression, simplification, and summarization, currently popular psychological theories of concept learning diverge on all these basic points. The history of psychology has seen the rise and fall of many theories about concept learning. Classical conditioning (as defined by Pavlov) created the earliest experimental technique. Reinforcement learning as described by Watson and elaborated by Clark Hull created a lasting paradigm in behavioral psychology. Cognitive psychology emphasized a computer and information flow metaphor for concept formation. Neural network models of concept formation and the structure of knowledge have opened powerful hierarchical models of knowledge organization such as George Miller's Wordnet. Neural networks are based on computational models of learning using factor analysis or convolution. Neural networks also are open to neuroscience and psychophysiological models of learning following Karl Lashley and Donald Hebb.","Rule-based theories of concept learning began with cognitive psychology and early computer models of learning that might be implemented in a high level computer language with computational statements such as if:then production rules. They take classification data and a rule-based theory as input which are the result of a rule-based learner with the hopes of producing a more accurate model of the data (Hekenaho 1997). The majority of rule-based models that have been developed are heuristic, meaning that rational analyses have not been provided and the models are not related to statistical approaches to induction. A rational analysis for rule-based models could presume that concepts are represented as rules, and would then ask to what degree of belief a rational agent should be in agreement with each rule, with some observed examples provided (Goodman, Griffiths, Feldman, and Tenenbaum). Rule-based theories of concept learning are focused more so on perceptual learning and less on definition learning. Rules can be used in learning when the stimuli are confusable, as opposed to simple. When rules are used in learning, decisions are made based on properties alone and rely on simple criteria that do not require a lot of memory ( Rouder and Ratcliff, 2006).",Example of rule-based theory:,"""A radiologist using rule-based categorization would observe whether specific properties of an X-ray image meet certain criteria; for example, is there an extreme difference in brightness in a suspicious region relative to other regions? A decision is then based on this property alone."" (see Rouder and Ratcliff 2006)",The prototype view of concept learning holds that people abstract out the central tendency (or prototype) of the examples experienced and use this as a basis for their categorization decisions.,"The prototype view of concept learning holds that people categorize based on one or more central examples of a given category followed by a penumbra of decreasingly typical examples. This implies that people do not categorize based on a list of things that all correspond to a definition, but rather on a hierarchical inventory based on semantic similarity to the central example(s).","To illustrate, imagine the following mental representations of the category: Sports",The first illustration demonstrates a mental representation if we were to categorize by definition:,Definition of Sports: an athletic activity requiring skill or physical prowess and often of a competitive nature.,The second illustration demonstrates a mental representation that prototype theory would predict:,"1. Baseball
2. Football
3. Basketball
4. Soccer
5. Hockey
6. Tennis
7. Golf
...
15. Bike-racing
16. Weightlifting
17. Skateboarding
18. Snowboarding
19. Boxing
20. Wrestling
...
32. Fishing
33. Hunting
34. Hiking
35. Sky-diving
36. Bungee-jumping
...
62. Cooking
63. Walking
...
82. Gatorade
83. Water
84. Protein
85. Diet",It is evident that prototype theory hypothesizes a more continuous (less discrete) way of categorization in which the list of things that match the category’s definition is not limited.,"Exemplar theory is the storage of specific instances (exemplars), with new objects evaluated only with respect to how closely they resemble specific known members (and nonmembers) of the category. This theory hypothesizes that learners store examples verbatim. This theory views concept learning as highly simplistic. Only individual properties are represented. These individual properties are not abstract and they do not create rules. An example of what exemplar theory might look like is, ""water is wet"". It is simply known that some (or one, or all) stored examples of water have the property wet. Exemplar based theories have become more empirically popular over the years with some evidence suggesting that human learners use exemplar based strategies only in early learning, forming prototypes and generalizations later in life. An important result of exemplar models in psychology literature has been a de-emphasis of complexity in concept learning. One of the best known exemplar theories of concept learning is the Generalized Context Model (GCM).","A problem with exemplar theory is that exemplar models critically depend on two measures: similarity between exemplars, and having a rule to determine group membership. Sometimes it is difficult to attain or distinguish these measures.","More recently, cognitive psychologists have begun to explore the idea that the prototype and exemplar models form two extremes. It has been suggested that people are able to form a multiple prototype representation, besides the two extreme representations. For example, consider the category 'spoon'. There are two distinct subgroups or conceptual clusters: spoons tend to be either large and wooden, or small and made of metal. The prototypical spoon would then be a medium-size object made of a mixture of metal and wood, which is clearly an unrealistic proposal. A more natural representation of the category 'spoon' would instead consist of multiple (at least two) prototypes, one for each cluster. A number of different proposals have been made in this regard (Anderson, 1991; Griffiths, Canini, Sanborn & Navarro, 2007; Love, Medin & Gureckis, 2004; Vanpaemel & Storms, 2008). These models can be regarded as providing a compromise between exemplar and prototype models.","The basic idea of explanation-based learning suggests that a new concept is acquired by experiencing examples of it and forming a basic outline.1 Put simply, by observing or receiving the qualities of a thing the mind forms a concept which possesses and is identified by those qualities.","The original theory, proposed by Mitchell, Keller, and Kedar-Cabelli in 1986 and called explanation-based generalization, is that learning occurs through progressive generalizing.2 This theory was first developed to program machines to learn. When applied to human cognition, it translates as follows: the mind actively separates information that applies to more than one thing and enters it into a broader description of a category of things. This is done by identifying sufficient conditions for something to fit in a category, similar to schematizing.","The revised model revolves around the integration of four mental processes – generalization, chunking, operationalization, and analogy3.",This particular theory of concept learning is relatively new and more research is being conducted to test it.,"Bayes' theorem is important because it provides a powerful tool for understanding, manipulating and controlling data5 that takes a larger view that is not limited to data analysis alone6. The approach is subjective, and this requires the assessment of prior probabilities6, making it also very complex. However, if Bayesians show that the accumulated evidence and the application of Bayes' law are sufficient, the work will overcome the subjectivity of the inputs involved7. Bayesian inference can be used for any honestly collected data and has a major advantage because of its scientific focus6.","One model that incorporates the Bayesian theory of concept learning is the ACT-R model, developed by John R. Anderson.[citation needed] The ACT-R model is a programming language that defines the basic cognitive and perceptual operations that enable the human mind by producing a step-by-step simulation of human behavior. This theory exploits the idea that each task humans perform consists of a series of discrete operations. The model has been applied to learning and memory, higher level cognition, natural language, perception and attention, human-computer interaction, education, and computer generated forces.[citation needed]","In addition to John R. Anderson, Joshua Tenenbaum has been a contributor to the field of concept learning; he studied the computational basis of human learning and inference using behavioral testing of adults, children, and machines from Bayesian statistics and probability theory, but also from geometry, graph theory, and linear algebra. Tenenbaum is working to achieve a better understanding of human learning in computational terms and trying to build computational systems that come closer to the capacities of human learners.","M. D. Merrill's Component Display Theory (CDT) is a cognitive matrix that focuses on the interaction between two dimensions: the level of performance expected from the learner and the types of content of the material to be learned. Merrill classifies a learner's level of performance as: find, use, remember, and material content as: facts, concepts, procedures, and principles. The theory also calls upon four primary presentation forms and several other secondary presentation forms. The primary presentation forms include: rules, examples, recall, and practice. Secondary presentation forms include: prerequisites, objectives, helps, mnemonics, and feedback. A complete lesson includes a combination of primary and secondary presentation forms, but the most effective combination varies from learner to learner and also from concept to concept. Another significant aspect of the CDT model is that it allows for the learner to control the instructional strategies used and adapt them to meet his or her own learning style and preference. A major goal of this model was to reduce three common errors in concept formation: over-generalization, under-generalization and misconception."
"Conditional random fields (CRFs) are a class of statistical modelling method often applied in pattern recognition and machine learning, where they are used for structured prediction. Whereas an ordinary classifier predicts a label for a single sample without regard to ""neighboring"" samples, a CRF can take context into account; e.g., the linear chain CRF popular in natural language processing predicts sequences of labels for sequences of input samples.","CRFs are a type of discriminative undirected probabilistic graphical model. It is used to encode known relationships between observations and construct consistent interpretations. It is often used for labeling or parsing of sequential data, such as natural language text or biological sequences[1] and in computer vision.[2] Specifically, CRFs find applications in shallow parsing,[3] named entity recognition,[4] gene finding and peptide critical functional region finding,[5] among other tasks, being an alternative to the related hidden Markov models (HMMs). In computer vision, CRFs are often used for object recognition and image segmentation.",,,"Lafferty, McCallum and Pereira[1] define a CRF on observations 




X



{\displaystyle {\boldsymbol {X}}}

 and random variables 




Y



{\displaystyle {\boldsymbol {Y}}}

 as follows:","Let 



G
=
(
V
,
E
)


{\displaystyle G=(V,E)}

 be a graph such that","




Y

=
(


Y


v



)

v
∈
V




{\displaystyle {\boldsymbol {Y}}=({\boldsymbol {Y}}_{v})_{v\in V}}

, so that 




Y



{\displaystyle {\boldsymbol {Y}}}

 is indexed by the vertices of 



G


{\displaystyle G}

. Then 



(

X

,

Y

)


{\displaystyle ({\boldsymbol {X}},{\boldsymbol {Y}})}

 is a conditional random field when the random variables 





Y


v




{\displaystyle {\boldsymbol {Y}}_{v}}

, conditioned on 




X



{\displaystyle {\boldsymbol {X}}}

, obey the Markov property with respect to the graph: 



p
(


Y


v



|


X

,


Y


w


,
w
≠
v
)
=
p
(


Y


v



|


X

,


Y


w


,
w
∼
v
)


{\displaystyle p({\boldsymbol {Y}}_{v}|{\boldsymbol {X}},{\boldsymbol {Y}}_{w},w\neq v)=p({\boldsymbol {Y}}_{v}|{\boldsymbol {X}},{\boldsymbol {Y}}_{w},w\sim v)}

, where 





w


∼
v


{\displaystyle {\mathit {w}}\sim v}

 means that 



w


{\displaystyle w}

 and 



v


{\displaystyle v}

 are neighbors in 



G


{\displaystyle G}

.","What this means is that a CRF is an undirected graphical model whose nodes can be divided into exactly two disjoint sets 




X



{\displaystyle {\boldsymbol {X}}}

 and 




Y



{\displaystyle {\boldsymbol {Y}}}

, the observed and output variables, respectively; the conditional distribution 



p
(

Y


|


X

)


{\displaystyle p({\boldsymbol {Y}}|{\boldsymbol {X}})}

 is then modeled.","For general graphs, the problem of exact inference in CRFs is intractable. The inference problem for a CRF is basically the same as for an MRF and the same arguments hold.[6] However, there exist special cases for which exact inference is feasible:","If exact inference is impossible, several algorithms can be used to obtain approximate solutions. These include:","Learning the parameters 



θ


{\displaystyle \theta }

 is usually done by maximum likelihood learning for 



p
(

Y

i



|


X

i


;
θ
)


{\displaystyle p(Y_{i}|X_{i};\theta )}

. If all nodes have exponential family distributions and all nodes are observed during training, this optimization is convex.[6] It can be solved for example using gradient descent algorithms, or Quasi-Newton methods such as the L-BFGS algorithm. On the other hand, if some variables are unobserved, the inference problem has to be solved for these variables. Exact inference is intractable in general graphs, so approximations have to be used.","In sequence modeling, the graph of interest is usually a chain graph. An input sequence of observed variables 



X


{\displaystyle X}

 represents a sequence of observations and 



Y


{\displaystyle Y}

 represents a hidden (or unknown) state variable that needs to be inferred given the observations. The 




Y

i




{\displaystyle Y_{i}}

 are structured to form a chain, with an edge between each 




Y

i
−
1




{\displaystyle Y_{i-1}}

 and 




Y

i




{\displaystyle Y_{i}}

. As well as having a simple interpretation of the 




Y

i




{\displaystyle Y_{i}}

 as ""labels"" for each element in the input sequence, this layout admits efficient algorithms for:","The conditional dependency of each 




Y

i




{\displaystyle Y_{i}}

 on 



X


{\displaystyle X}

 is defined through a fixed set of feature functions of the form 



f
(
i
,

Y

i
−
1


,

Y

i


,
X
)


{\displaystyle f(i,Y_{i-1},Y_{i},X)}

, which can informally be thought of as measurements on the input sequence that partially determine the likelihood of each possible value for 




Y

i




{\displaystyle Y_{i}}

. The model assigns each feature a numerical weight and combines them to determine the probability of a certain value for 




Y

i




{\displaystyle Y_{i}}

.","Linear-chain CRFs have many of the same applications as conceptually simpler hidden Markov models (HMMs), but relax certain assumptions about the input and output sequence distributions. An HMM can loosely be understood as a CRF with very specific feature functions that use constant probabilities to model state transitions and emissions. Conversely, a CRF can loosely be understood as a generalization of an HMM that makes the constant transition probabilities into arbitrary functions that vary across the positions in the sequence of hidden states, depending on the input sequence.","Notably in contrast to HMMs, CRFs can contain any number of feature functions, the feature functions can inspect the entire input sequence 



X


{\displaystyle X}

 at any point during inference, and the range of the feature functions need not have a probabilistic interpretation.","CRFs can be extended into higher order models by making each 




Y

i




{\displaystyle Y_{i}}

 dependent on a fixed number 



o


{\displaystyle o}

 of previous variables 




Y

i
−
o


,
.
.
.
,

Y

i
−
1




{\displaystyle Y_{i-o},...,Y_{i-1}}

. Training and inference are only practical for small values of 



o


{\displaystyle o}

 (such as o ≤ 5),[citation needed] since their computational cost increases exponentially with 



o


{\displaystyle o}

. Large-margin models for structured prediction, such as the structured Support Vector Machine can be seen as an alternative training procedure to CRFs.","There exists another generalization of CRFs, the semi-Markov conditional random field (semi-CRF), which models variable-length segmentations of the label sequence 



Y


{\displaystyle Y}

.[7] This provides much of the power of higher-order CRFs to model long-range dependencies of the 




Y

i




{\displaystyle Y_{i}}

, at a reasonable computational cost.",Latent-dynamic conditional random fields (LDCRF) or discriminative probabilistic latent variable models (DPLVM) are a type of CRFs for sequence tagging tasks. They are latent variable models that are trained discriminatively.,"In an LDCRF, like in any sequence tagging task, given a sequence of observations x = x₁, … xₙ, the main problem the model must solve is how to assign a sequence of labels y = y₁, … yₙ from one finite set of labels Y. Instead of directly modeling P(y|x) as an ordinary linear-chain CRF would do, a set of latent variables h is ""inserted"" between x and y using the chain rule of probability:[8]","This allows capturing latent structure between the observations and labels.[9] While LDCRFs can be trained using quasi-Newton methods, a specialized version of the perceptron algorithm called the latent-variable perceptron has been developed for them as well, based on Collins' structured perceptron algorithm.[8] These models find applications in computer vision, specifically gesture recognition from video streams[9] and shallow parsing.[8]",This is a partial list of software that implement generic CRF tools.,This is a partial list of software that implement CRF related tools.
"Informedness = Sensitivity + Specificity - 1
Markedness = Precision + NPV - 1",Sources: Fawcett (2006) and Powers (2011).[1][2],"In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as an error matrix,[3] is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each column of the matrix represents the instances in a predicted class while each row represents the instances in an actual class (or vice-versa).[2] The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another).","It is a special kind of contingency table, with two dimensions (""actual"" and ""predicted""), and identical sets of ""classes"" in both dimensions (each combination of dimension and class is a variable in the contingency table).",,,"If a classification system has been trained to distinguish between cats, dogs and rabbits, a confusion matrix will summarize the results of testing the algorithm for further inspection. Assuming a sample of 27 animals — 8 cats, 6 dogs, and 13 rabbits, the resulting confusion matrix could look like the table below:","In predictive analytics, a table of confusion (sometimes also called a confusion matrix), is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. This allows more detailed analysis than mere proportion of correct guesses (accuracy). Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the number of samples in different classes vary greatly). For example, if there were 95 cats and only 5 dogs in the data set, the classifier could easily be biased into classifying all the samples as cats. The overall accuracy would be 95%, but in practice the classifier would have a 100% recognition rate for the cat class but a 0% recognition rate for the dog class.","Assuming the confusion matrix above, its corresponding table of confusion, for the cat class, would be:",The final table of confusion would contain the average values for all classes combined.,"Let us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 confusion matrix, as follows:",
A constrained conditional model (CCM) is a machine learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints. The constraint can be used as a way to incorporate expressive[clarification needed] prior knowledge into the model and bias the assignments made by the learned model to satisfy these constraints. The framework can be used to support decisions in an expressive output space while maintaining modularity and tractability of training and inference.,"Models of this kind have recently[when?] attracted much attention[citation needed] within the natural language processing (NLP) community. Formulating problems as constrained optimization problems over the output of learned models has several advantages. It allows one to focus on the modeling of problems by providing the opportunity to incorporate domain-specific knowledge as global constraints using a first order language. Using this declarative framework frees the developer from low level feature engineering while capturing the problem's domain-specific properties and guarantying exact inference. From a machine learning perspective it allows decoupling the stage of model generation (learning) from that of the constrained inference stage, thus helping to simplify the learning stage while improving the quality of the solutions. For example, in the case of generating compressed sentences, rather than simply relying on a language model to retain the most commonly used n-grams in the sentence, constraints can be used to ensure that if a modifier is kept in the compressed sentence, its subject will also be kept.",,,"Making decisions in many domains (such as natural language processing and computer vision problems) often involves assigning values to sets of interdependent variables where the expressive dependency structure can influence, or even dictate, what assignments are possible. These settings are applicable not only to Structured Learning problems such as semantic role labeling, but also for cases that require making use of multiple pre-learned components, such as summarization, textual entailment and question answering. In all these cases, it is natural to formulate the decision problem as a constrained optimization problem, with an objective function that is composed of learned models, subject to domain- or problem-specific constraints.","Constrained conditional models form a learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints (written, for example, using a first-order representation) as a way to support decisions in an expressive output space while maintaining modularity and tractability of training and inference. These constraints can express either hard restrictions, completely prohibiting some assignments, or soft restrictions, penalizing unlikely assignments. In most applications of this framework in NLP, following,[1] Integer Linear Programming (ILP) was used as the inference framework, although other algorithms can be used for that purpose.","Given a set of feature functions 



{

ϕ

i


(
x
,
y
)
}


{\displaystyle \{\phi _{i}(x,y)\}}

 and a set of constraints 



{

C

i


(
x
,
y
)
}


{\displaystyle \{C_{i}(x,y)\}}

, defined over an input structure 



x
∈
X


{\displaystyle x\in X}

 and an output structure 



y
∈
Y


{\displaystyle y\in Y}

, a constraint conditional model is characterized by two weight vectors, w and 



ρ


{\displaystyle \rho }

, and is defined as the solution to the following optimization problem:","Each constraint 




C

i


∈
C


{\displaystyle C_{i}\in C}

 is a boolean mapping indicating if the joint assignment 



(
x
,
y
)


{\displaystyle (x,y)}

 violates a constraint, and 



ρ


{\displaystyle \rho }

 is the penalty incurred for violating the constraints. Constraints assigned an infinite penalty are known as hard constraints, and represent unfeasible assignments to the optimization problem.","The objective function used by CCMs can be decomposed and learned in several ways, ranging from a complete joint training of the model along with the constraints to completely decoupling the learning and the inference stage. In the latter case, several local models are learned independently and the dependency between these models is considered only at decision time via a global decision process. The advantages of each approach are discussed in [2] which studies the two training paradigms: (1) local models: L+I (learning + inference) and (2) global model: IBT (Inference based training), and shows both theoretically and experimentally that while IBT (joint training) is best in the limit, under some conditions (basically, ”good” components) L+I can generalize better.","The ability of CCM to combine local models is especially beneficial in cases where joint learning is computationally intractable or when training data are not available for joint learning. This flexibility distinguishes CCM from the other learning frameworks that also combine statistical information with declarative constraints, such as Markov logic network, that emphasize joint training.",CCM can help reduce supervision by using domain knowledge (expressed as constraints) to drive learning. These settings were studied in [3] and.[4] These works introduce semi-supervised Constraints Driven Learning (CODL) and show that by incorporating domain knowledge the performance of the learned model improves significantly.,"CCMs have also been applied to latent learning frameworks, where the learning problem is defined over a latent representation layer. Since the notion of a correct representation is inherently ill-defined, no gold-standard labeled data regarding the representation decision is available to the learner. Identifying the correct (or optimal) learning representation is viewed as a structured prediction process and therefore modeled as a CCM. This problem was covered in several papers, in both supervised[5] and unsupervised [6] settings. In all cases research showed that explicitly modeling the interdependencies between representation decisions via constraints results in an improved performance.","The advantages of the CCM declarative formulation and the availability of off-the-shelf solvers have led to a large variety of natural language processing tasks being formulated within the framework, including semantic role labeling,[7] syntactic parsing,[8] coreference resolution,[9] summarization,[10][11][12] transliteration,[13] natural language generation [14] and joint information extraction.[15][16]","Most of these works use an integer linear programming (ILP) solver to solve the decision problem. Although theoretically solving an Integer Linear Program is exponential in the size of the decision problem, in practice using state-of-the-art solvers and approximate inference techniques [17] large scale problems can be solved efficiently.","The key advantage of using an ILP solver for solving the optimization problem defined by a constrained conditional model is the declarative formulation used as input for the ILP solver, consisting of a linear objective function and a set of linear constraints."
Coupled Pattern Learner (CPL) is a machine learning algorithm which couples the semi-supervised learning of categories and relations to forestall the problem of semantic drift associated with boot-strap learning methods.,,,"Semi-supervised learning approaches using a small number of labeled examples with many unlabeled examples are usually unreliable as they produce an internally consistent, but incorrect set of extractions. CPL solves this problem by simultaneously learning classiﬁers for many different categories and relations in the presence of an ontology deﬁning constraints that couple the training of these classiﬁers. It was introduced by Andrew Carlson, Justin Betteridge, Estevam R. Hruschka Jr. and Tom M. Mitchell in 2009.[1][2]","CPL is an approach to semi-supervised learning that yields more accurate results by coupling the training of many information extractors. Basic idea behind CPL is that semi-supervised training of a single type of extractor such as ‘coach’ is much more difﬁcult than simultaneously training many extractors that cover a variety of inter-related entity and relation types. Using prior knowledge about the relationships between these different entities and relations CPL makes unlabeled data as a useful constraint during training. For e.g., ‘coach(x)’ implies ‘person(x)’ and ‘not sport(x)’.",CPL primarily relies on the notion of coupling the learning of multiple functions so as to constrain the semi-supervised learning problem. CPL constrains the learned function in two ways.,"Each predicate P in the ontology has a list of other same-arity predicates with which P is mutually exclusive. If A is mutually exclusive with predicate B, A’s positive instances and patterns become negative instances and negative patterns for B. For example, if ‘city’, having an instance ‘Boston’ and a pattern ‘mayor of arg1’, is mutually exclusive with ‘scientist’, then ‘Boston’ and ‘mayor of arg1’ will become a negative instance and a negative pattern respectively for ‘scientist.’ Further, Some categories are declared to be a subset of another category. For e.g., ‘athlete’ is a subset of ‘person’.","This is a type checking information used to couple the learning of relations and categories. For example, the arguments of the ‘ceoOf’ relation are declared to be of the categories ‘person’ and ‘company’. CPL does not promote a pair of noun phrases as an instance of a relation unless the two noun phrases are classiﬁed as belonging to the correct argument types.",Following is a quick summary of the CPL algorithm.[2],"A large corpus of Part-Of-Speech tagged sentences and an initial ontology with predeﬁned categories, relations, mutually exclusive relationships between same-arity predicates, subset relationships between some categories, seed instances for all predicates, and seed patterns for the categories.","CPL ﬁnds new candidate instances by using newly promoted patterns to extract the noun phrases that co-occur with those patterns in the text corpus. CPL extracts,","Candidate instances and patterns are ﬁltered to maintain high precision, and to avoid extremely speciﬁc patterns. An instance is only considered for assessment if it co-occurs with at least two promoted patterns in the text corpus, and if its co-occurrence count with all promoted patterns is at least three times greater than its co-occurrence count with negative patterns.",CPL ranks candidate instances using the number of promoted patterns that they co-occur with so that candidates that occur with more patterns are ranked higher. Patterns are ranked using an estimate of the precision of each pattern.,"CPL ranks the candidates according to their assessment scores and promotes at most 100 instances and 5 patterns for each predicate. Instances and patterns are only promoted if they co-occur with at least two promoted patterns or instances, respectively.","Meta-Bootstrap Learner (MBL) was also proposed by the authors of CPL in.[2] Meta-Bootstrap learner couples the training of multiple extraction techniques with a multi-view constraint, which requires the extractors to agree. It makes addition of coupling constraints on top of existing extraction algorithms, while treating them as black boxes, feasible. MBL assumes that the errors made by different extraction techniques are independent. Following is a quick summary of MBL.","Subordinate algorithms used with MBL do not promote any instance on their own, they report the evidence about each candidate to MBL and MBL is responsible for promoting instances.","In their paper [1] authors have presented results showing the potential of CPL to contribute new facts to existing repository of semantic knowledge, Freebase [3]"
"The cross-entropy (CE) method attributed to Reuven Rubinstein is a general Monte Carlo approach to combinatorial and continuous multi-extremal optimization and importance sampling. The method originated from the field of rare event simulation, where very small probabilities need to be accurately estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems. The CE method can be applied to static and noisy combinatorial optimization problems such as the traveling salesman problem, the quadratic assignment problem, DNA sequence alignment, the max-cut problem and the buffer allocation problem, as well as continuous global optimization problems with many local extrema.",In a nutshell the CE method consists of two phases:,,,"Consider the general problem of estimating the quantity 



ℓ
=


E



u



[
H
(

X

)
]
=
∫
H
(

x

)

f
(

x

;

u

)



d



x



{\displaystyle \ell =\mathbb {E} _{\mathbf {u} }[H(\mathbf {X} )]=\int H(\mathbf {x} )\,f(\mathbf {x} ;\mathbf {u} )\,{\textrm {d}}\mathbf {x} }

, where 



H


{\displaystyle H}

 is some performance function and 



f
(

x

;

u

)


{\displaystyle f(\mathbf {x} ;\mathbf {u} )}

 is a member of some parametric family of distributions. Using importance sampling this quantity can be estimated as 






ℓ
^



=


1
N



∑

i
=
1


N


H
(


X


i


)



f
(


X


i


;

u

)


g
(


X


i


)





{\displaystyle {\hat {\ell }}={\frac {1}{N}}\sum _{i=1}^{N}H(\mathbf {X} _{i}){\frac {f(\mathbf {X} _{i};\mathbf {u} )}{g(\mathbf {X} _{i})}}}

, where 





X


1


,
…
,


X


N




{\displaystyle \mathbf {X} _{1},\dots ,\mathbf {X} _{N}}

 is a random sample from 



g



{\displaystyle g\,}

. For positive 



H


{\displaystyle H}

, the theoretically optimal importance sampling density (pdf) is given by 




g

∗


(

x

)
=
H
(

x

)
f
(

x

;

u

)

/

ℓ


{\displaystyle g^{*}(\mathbf {x} )=H(\mathbf {x} )f(\mathbf {x} ;\mathbf {u} )/\ell }

. This, however, depends on the unknown 



ℓ


{\displaystyle \ell }

. The CE method aims to approximate the optimal PDF by adaptively selecting members of the parametric family that are closest (in the Kullback–Leibler sense) to the optimal PDF 




g

∗




{\displaystyle g^{*}}

.","In several cases, the solution to step 3 can be found analytically. Situations in which this occurs are","The same CE algorithm can be used for optimization, rather than estimation. Suppose the problem is to maximize some function 



S
(
x
)


{\displaystyle S(x)}

, for example, 



S
(
x
)
=



e



−
(
x
−
2

)

2




+
0.8




e



−
(
x
+
2

)

2






{\displaystyle S(x)={\textrm {e}}^{-(x-2)^{2}}+0.8\,{\textrm {e}}^{-(x+2)^{2}}}

. To apply CE, one considers first the associated stochastic problem of estimating 





P


θ


(
S
(
X
)
≥
γ
)


{\displaystyle \mathbb {P} _{\boldsymbol {\theta }}(S(X)\geq \gamma )}

 for a given level 



γ



{\displaystyle \gamma \,}

, and parametric family 




{
f
(
⋅
;

θ

)
}



{\displaystyle \left\{f(\cdot ;{\boldsymbol {\theta }})\right\}}

, for example the 1-dimensional Gaussian distribution, parameterized by its mean 




μ

t





{\displaystyle \mu _{t}\,}

 and variance 




σ

t


2




{\displaystyle \sigma _{t}^{2}}

 (so 




θ

=
(
μ
,

σ

2


)


{\displaystyle {\boldsymbol {\theta }}=(\mu ,\sigma ^{2})}

 here). Hence, for a given 



γ



{\displaystyle \gamma \,}

, the goal is to find 




θ



{\displaystyle {\boldsymbol {\theta }}}

 so that 




D


K
L



(



I



{
S
(
x
)
≥
γ
}


∥

f

θ


)


{\displaystyle D_{\mathrm {KL} }({\textrm {I}}_{\{S(x)\geq \gamma \}}\|f_{\boldsymbol {\theta }})}

 is minimized. This is done by solving the sample version (stochastic counterpart) of the KL divergence minimization problem, as in step 3 above. It turns out that parameters that minimize the stochastic counterpart for this choice of target distribution and parametric family are the sample mean and sample variance corresponding to the elite samples, which are those samples that have objective function value 



≥
γ


{\displaystyle \geq \gamma }

. The worst of the elite samples is then used as the level parameter for the next iteration. This yields the following randomized algorithm that happens to coincide with the so-called Estimation of Multivariate Normal Algorithm (EMNA), an estimation of distribution algorithm."
"Cross-validation, sometimes called rotation estimation,[1][2][3] is a model validation technique for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (testing dataset).[4] The goal of cross validation is to define a dataset to ""test"" the model in the training phase (i.e., the validation dataset), in order to limit problems like overfitting, give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem), etc.","One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, multiple rounds of cross-validation are performed using different partitions, and the validation results are averaged over the rounds.","One of the main reasons for using cross-validation instead of using the conventional validation (e.g. partitioning the data set into two sets of 70% for training and 30% for test) is that there is not enough data available to partition it into separate training and test sets without losing significant modelling or testing capability. In these cases, a fair way to properly estimate model prediction performance is to use cross-validation as a powerful general technique.[5]","In summary, cross-validation combines (averages) measures of fit (prediction error) to derive a more accurate estimate of model prediction performance.[5]",,,"Suppose we have a model with one or more unknown parameters, and a data set to which the model can be fit (the training data set). The fitting process optimizes the model parameters to make the model fit the training data as well as possible. If we then take an independent sample of validation data from the same population as the training data, it will generally turn out that the model does not fit the validation data as well as it fits the training data. This is called overfitting, and is particularly likely to happen when the size of the training data set is small, or when the number of parameters in the model is large. Cross-validation is a way to predict the fit of a model to a hypothetical validation set when an explicit validation set is not available.","Linear regression provides a simple illustration of overfitting. In linear regression we have real response values y1, ..., yn, and n p-dimensional vector covariates x1, ..., xn. The components of the vectors xi are denoted xi1, ..., xip. If we use least squares to fit a function in the form of a hyperplane y = a + βTx to the data (xi, yi)1≤i≤n, we could then assess the fit using the mean squared error (MSE). The MSE for a given value of the parameters a and β on the training set (xi, yi)1≤i≤n is","It can be shown under mild assumptions that the expected value of the MSE for the training set is (n − p − 1)/(n + p + 1) < 1 times the expected value of the MSE for the validation set[citation needed] (the expected value is taken over the distribution of training sets). Thus if we fit the model and compute the MSE on the training set, we will get an optimistically biased assessment of how well the model will fit an independent data set. This biased estimate is called the in-sample estimate of the fit, whereas the cross-validation estimate is an out-of-sample estimate.","Since in linear regression it is possible to directly compute the factor (n − p − 1)/(n + p + 1) by which the training MSE underestimates the validation MSE, cross-validation is not practically useful in that setting (however, cross-validation remains useful in the context of linear regression in that it can be used to select an optimally regularized cost function). In most other regression procedures (e.g. logistic regression), there is no simple formula to make such an adjustment. Cross-validation is, thus, a generally applicable way to predict the performance of a model on a validation set using computation in place of mathematical analysis.","Two types of cross-validation can be distinguished, exhaustive and non-exhaustive cross-validation.",Exhaustive cross-validation methods are cross-validation methods which learn and test on all possible ways to divide the original sample into a training and a validation set.,Leave-p-out cross-validation (LpO CV) involves using p observations as the validation set and the remaining observations as the training set. This is repeated on all ways to cut the original sample on a validation set of p observations and a training set.,"LpO cross-validation requires to learn and validate 




C

n


p




{\displaystyle C_{n}^{p}}

 times, where n is the number of observations in the original sample and 




C

n


p




{\displaystyle C_{n}^{p}}

 is the binomial coefficient. For p > 1 and n even moderately large, LpO can become impossible to calculate. For example, with n = 100 and p = 30 = 30 percent of 100 (as suggested above), 




C

100


30




{\displaystyle C_{100}^{30}}

 = 3e25 = 3 followed by 25 zeros.","Leave-one-out cross-validation (LOOCV) is a particular case of leave-p-out cross-validation with p = 1. The process looks similar to jackknife, however with cross-validation you compute a statistic on the left-out sample(s), while with jackknifing you compute a statistic from the kept samples only.","LOO cross-validation does not have the same problem of excessive compute time as general LpO cross-validation because 




C

n


1


=
n


{\displaystyle C_{n}^{1}=n}

.",Non-exhaustive cross validation methods do not compute all ways of splitting the original sample. Those methods are approximations of leave-p-out cross-validation.,"In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data. The k results from the folds can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used,[6] but in general k remains an unfixed parameter.","When k=n (the number of observations), the k-fold cross-validation is exactly the leave-one-out cross-validation.","In stratified k-fold cross-validation, the folds are selected so that the mean response value is approximately equal in all the folds. In the case of a dichotomous classification, this means that each fold contains roughly the same proportions of the two types of class labels.","This is the simplest variation of k-fold cross-validation. Also called holdout method.[7] For each fold, we randomly assign data points to two sets d0 and d1, so that both sets are equal size (this is usually implemented by shuffling the data array and then splitting it in two). We then train on d0 and test on d1, followed by training on d1 and testing on d0.","This has the advantage that our training and test sets are both large, and each data point is used for both training and validation on each fold.","This method, also known as Monte Carlo cross-validation,[8] randomly splits the dataset into training and validation data. For each such split, the model is fit to the training data, and predictive accuracy is assessed using the validation data. The results are then averaged over the splits. The advantage of this method (over k-fold cross validation) is that the proportion of the training/validation split is not dependent on the number of iterations (folds). The disadvantage of this method is that some observations may never be selected in the validation subsample, whereas others may be selected more than once. In other words, validation subsets may overlap. This method also exhibits Monte Carlo variation, meaning that the results will vary if the analysis is repeated with different random splits.","When the number of random splits goes to infinity, the repeated random sub-sampling validation become arbitrary close to the leave-p-out cross-validation.","In a stratified variant of this approach, the random samples are generated in such a way that the mean response value (i.e. the dependent variable in the regression) is equal in the training and testing sets. This is particularly useful if the responses are dichotomous with an unbalanced representation of the two response values in the data.","The goal of cross-validation is to estimate the expected level of fit of a model to a data set that is independent of the data that were used to train the model. It can be used to estimate any quantitative measure of fit that is appropriate for the data and model. For example, for binary classification problems, each case in the validation set is either predicted correctly or incorrectly. In this situation the misclassification error rate can be used to summarize the fit, although other measures like positive predictive value could also be used. When the value being predicted is continuously distributed, the mean squared error, root mean squared error or median absolute deviation could be used to summarize the errors.","Cross-validation can be used to compare the performances of different predictive modeling procedures. For example, suppose we are interested in optical character recognition, and we are considering using either support vector machines (SVM) or k nearest neighbors (KNN) to predict the true character from an image of a handwritten character. Using cross-validation, we could objectively compare these two methods in terms of their respective fractions of misclassified characters. If we simply compared the methods based on their in-sample error rates, the KNN method would likely appear to perform better, since it is more flexible and hence more prone to overfitting compared to the SVM method.","Cross-validation can also be used in variable selection.[9] Suppose we are using the expression levels of 20 proteins to predict whether a cancer patient will respond to a drug. A practical goal would be to determine which subset of the 20 features should be used to produce the best predictive model. For most modeling procedures, if we compare feature subsets using the in-sample error rates, the best performance will occur when all 20 features are used. However under cross-validation, the model with the best fit will generally include only a subset of the features that are deemed truly informative.","Suppose we choose a measure of fit F, and use cross-validation to produce an estimate F* of the expected fit EF of a model to an independent data set drawn from the same population as the training data. If we imagine sampling multiple independent training sets following the same distribution, the resulting values for F* will vary. The statistical properties of F* result from this variation.","The cross-validation estimator F* is very nearly unbiased for EF. The reason that it is slightly biased is that the training set in cross-validation is slightly smaller than the actual data set (e.g. for LOOCV the training set size is n − 1 when there are n observed cases). In nearly all situations, the effect of this bias will be conservative in that the estimated fit will be slightly biased in the direction suggesting a poorer fit. In practice, this bias is rarely a concern.","The variance of F* can be large.[10][11] For this reason, if two statistical procedures are compared based on the results of cross-validation, it is important to note that the procedure with the better estimated performance may not actually be the better of the two procedures (i.e. it may not have the better value of EF). Some progress has been made on constructing confidence intervals around cross-validation estimates,[10] but this is considered a difficult problem.","Most forms of cross-validation are straightforward to implement as long as an implementation of the prediction method being studied is available. In particular, the prediction method can be a ""black box"" – there is no need to have access to the internals of its implementation. If the prediction method is expensive to train, cross-validation can be very slow since the training must be carried out repeatedly. In some cases such as least squares and kernel regression, cross-validation can be sped up significantly by pre-computing certain values that are needed repeatedly in the training, or by using fast ""updating rules"" such as the Sherman–Morrison formula. However one must be careful to preserve the ""total blinding"" of the validation set from the training procedure, otherwise bias may result. An extreme example of accelerating cross-validation occurs in linear regression, where the results of cross-validation have a closed-form expression known as the prediction residual error sum of squares (PRESS).",Cross-validation only yields meaningful results if the validation set and training set are drawn from the same population and only if human biases are controlled.,"In many applications of predictive modeling, the structure of the system being studied evolves over time. Both of these can introduce systematic differences between the training and validation sets. For example, if a model for predicting stock values is trained on data for a certain five-year period, it is unrealistic to treat the subsequent five-year period as a draw from the same population. As another example, suppose a model is developed to predict an individual's risk for being diagnosed with a particular disease within the next year. If the model is trained using data from a study involving only a specific population group (e.g. young people or males), but is then applied to the general population, the cross-validation results from the training set could differ greatly from the actual predictive performance.","In many applications, models also may be incorrectly specified and vary as a function of modeler biases and/or arbitrary choices. When this occurs, there may be an illusion that the system changes in external samples, whereas the reason is that the model has missed a critical predictor and/or included a confounded predictor. New evidence is that cross-validation by itself is not very predictive of external validity, whereas a form of experimental validation known as swap sampling that does control for human bias can be much more predictive of external validity.[12] As defined by this large MAQC-II study across 30,000 models, swap sampling incorporates cross-validation in the sense that predictions are tested across independent training and validation samples. Yet, models are also developed across these independent samples and by modelers who are blinded to one another. When there is a mismatch in these models developed across these swapped training and validation samples as happens quite frequently, MAQC-II shows that this will be much more predictive of poor external predictive validity than traditional cross-validation.","The reason for the success of the swapped sampling is a built-in control for human biases in model building. In addition to placing too much faith in predictions that may vary across modelers and lead to poor external validity due to these confounding modeler effects, these are some other ways that cross-validation can be misused:",
The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces (often with hundreds or thousands of dimensions) that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. The expression was coined by Richard E. Bellman when considering problems in dynamic optimization.[1][2],"There are multiple phenomena referred to by this name in domains such as numerical analysis, sampling, combinatorics, machine learning, data mining, and databases. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties; in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient.",,,"In some problems, each variable can take one of several discrete values, or the range of possible values is divided to give a finite number of possibilities. Taking the variables together, a huge number of combinations of values must be considered. This effect is also known as the combinatorial explosion. Even in the simplest case of 



d


{\displaystyle d}

 binary variables, the number of possible combinations already is 



O
(

2

d


)


{\displaystyle O(2^{d})}

, exponential in the dimensionality. Naively, each additional dimension doubles the effort needed to try all combinations.","There is an exponential increase in volume associated with adding extra dimensions to a mathematical space. For example, 102=100 evenly spaced sample points suffice to sample a unit interval (a ""1-dimensional cube"") with no more than 10−2=0.01 distance between points; an equivalent sampling of a 10-dimensional unit hypercube with a lattice that has a spacing of 10−2=0.01 between adjacent points would require 1020[=(102)10] sample points. In general, with a spacing distance of 10-n the 10-dimensional hypercube appears to be a factor of 10n(10-1)[=(10n)10/(10n)] ""larger"" than the 1-dimensional hypercube, which is the unit interval. In the above example n=2: when using a sampling distance of 0.01 the 10-dimensional hypercube appears to be 1018 ""larger"" than the unit interval. This effect is a combination of the combinatorics problems above and the distance function problems explained below.","When solving dynamic optimization problems by numerical backward induction, the objective function must be computed for each combination of values. This is a significant obstacle when the dimension of the ""state variable"" is large.","In machine learning problems that involve learning a ""state-of-nature"" (maybe an infinite distribution) from a finite number of data samples in a high-dimensional feature space with each feature having a number of possible values, an enormous amount of training data are required to ensure that there are several samples with each combination of values. With a fixed number of training samples, the predictive power reduces as the dimensionality increases, and this is known as Hughes phenomenon (named after Gordon F. Hughes).[3][4]","When a measure such as a Euclidean distance is defined using many coordinates, there is little difference in the distances between different pairs of samples.","One way to illustrate the ""vastness"" of high-dimensional Euclidean space is to compare the proportion of an inscribed hypersphere with radius 



r


{\displaystyle r}

 and dimension 



d


{\displaystyle d}

, to that of a hypercube with edges of length 



2
r


{\displaystyle 2r}

. The volume of such a sphere is: 






2

r

d



π

d

/

2




d

Γ
(
d

/

2
)





{\displaystyle {\frac {2r^{d}\pi ^{d/2}}{d\;\Gamma (d/2)}}}

. The volume of the cube would be: 



(
2
r

)

d




{\displaystyle (2r)^{d}}

. As the dimension 



d


{\displaystyle d}

 of the space increases, the hypersphere becomes an insignificant volume relative to that of the hypercube. This can clearly be seen by comparing the proportions as the dimension 



d


{\displaystyle d}

 goes to infinity:","In this sense, nearly all of the high-dimensional space is ""far away"" from the centre. To put it another way, the high-dimensional unit hypercube can be said to consist almost entirely of the ""corners"" of the hypercube, with almost no ""middle"".","This also helps to understand the chi-squared distribution. Indeed, the (non-central) chi-squared distribution associated to a random point in the interval [-1, 1] is the same as the distribution of the length-squared of a random point in the d-cube. By the law of large numbers, this distribution concentrates itself in a narrow band around d times the standard deviation squared (σ2) of the original derivation. This illuminates the chi-squared distribution and also illustrates that most of the volume of the d-cube concentrates near the surface of a sphere of radius √dσ.","A further development of this phenomenon is as follows. Any fixed distribution on R induces a product distribution on points in Rd. For any fixed n, it turns out that the minimum and the maximum distance between a random reference point Q and a list of n random data points P1,...,Pn become indiscernible compared to the minimum distance:[5]","This is often cited as distance functions losing their usefulness (for the nearest-neighbor criterion in feature-comparison algorithms, for example) in high dimensions. However, recent research has shown this to only hold in the artificial scenario when the one-dimensional distributions R are independent and identically distributed.[6] When attributes are correlated, data can become easier and provide higher distance contrast and the signal-to-noise ratio was found to play an important role, thus feature selection should be used.[6]",The effect complicates nearest neighbor search in high dimensional space. It is not possible to quickly reject candidates by using the difference in one coordinate as a lower bound for a distance based on all the dimensions.[7][8],"However, it has recently been observed that the mere number of dimensions does not necessarily result in difficulties,[9] since relevant additional dimensions can also increase the contrast. In addition, for the resulting ranking it remains useful to discern close and far neighbors. Irrelevant (""noise"") dimensions, however, reduce the contrast in the manner described above. In time series analysis, where the data are inherently high-dimensional, distance functions also work reliably as long as the signal-to-noise ratio is high enough.[10]","Another effect of high dimensionality on distance functions concerns k-nearest neighbor (k-NN) graphs constructed from a data set using a distance function. As the dimension increases, the indegree distribution of the k-NN digraph becomes skewed with a peak on the right because of the emergence of a disproportionate number of hubs, that is, data-points that appear in many more k-NN lists of other data-points than the average. This phenomenon can have a considerable impact on various techniques for classification (including the k-NN classifier), semi-supervised learning, and clustering,[11] and it also affects information retrieval.[12]","In a recent survey, Zimek et al. identified the following problems when searching for anomalies in high-dimensional data:[6]","Many of the analyzed specialized methods tackle one or another of these problems, but there remain many open research questions."
"Darkforest is a computer go program developed by Facebook, based on deep learning techniques using a convolutional neural network. Its updated version Darkfores2 combines the techniques of its predecessor with Monte Carlo tree search.[1][2] The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them.[3] With the update, the system is known as Darkfmcts3.[4]","Darkforest is of similar strength to programs like CrazyStone and Zen.[5] It has not been tested an against professional human player, however, Google's AlphaGo program won against a professional player in October 2015 using a similar combination of techniques .[6]",Darkforest is named after Liu Cixin's science fiction novel The Dark Forest.[7],,,"Competing with top human players in the ancient game of Go has been a long-term goal of artificial intelligence. Go’s high branching factor makes traditional search techniques ineffective, even on cutting-edge hardware, and Go’s evaluation function could change drastically with one stone change. However, by using a Deep Convolutional Neural Network designed for long-term predictions, Darkforest has been able to substantially improve the win rate for bots over more traditional Monte Carlo Tree Search based approaches.","Against human players, Darkfores2 achieves a stable 3d ranking on KGS Go Server, which roughly corresponds to an advanced amateur human player. However, after adding Monte Carlo Tree Search to Darkfores2 to create a much stronger player named darkfmcts3, it can achieve a 5d ranking on the KGS Go Server.","darkfmcts3 is on par with state-of-the-art Go AIs such as Zen, DolBaram and Crazy Stone but lags behind AlphaGo.[8] It won 3rd place in January 2016 KGS Bot Tournament against other Go AIs.","After Google's AlphaGo won against Fan Hui in 2015, Facebook made its AI's hardware designs public, alongside releasing the code behind DarkForest as open-source, along with heavy recruiting to strengthen its team of AI engineers.[3]","Darkforest uses a neural network to sort through the 10100 board positions, and find the most powerful next move.[9] However, neural networks alone cannot match the level of good amateur players or the best search-based Go engines, and so Darkfores2 combines the neural network approach with a search-based machine. A database of 250,000 real Go games were used in the development of Darkforest, with 220,000 used as a training set and the rest used to test the neural network's ability to predict the next moves played in the real games. This allows Darkforest to accurately evaluate the global state of the board, but local tactics were still poor. Search-based engines have poor global evaluation, but are good at local tactics. Combining these two approaches is difficult because search-based engines work much faster than neural networks, a problem which was solved in Darkfores2 by running the processes in parallel with frequent communication between the two.[9]","Go is generally played by analyzing the position of the stones on the board. Some advanced players have described it as playing in some part subconsciously. Unlike chess and checkers, where AI players can simply look farther forward at moves than human players, but with each round of Go having on average 250 possible moves, that approach is ineffective. Instead, neural networks copy human play by training the AI systems on images of successful moves, the AI can effectively learn how to interpret how the board looks, as many grandmasters do.[10] In November 2015, Facebook demonstrated the combination of MCTS with neural networks, which played with a style that ""felt human"".[10]","It has been noted that Darkforest still has flaws in its play style. Sometimes the bot plays tenuki (""move elsewhere"") pointlessly local powerful moves are required. When the bot is losing, it shows the typical behavior of MCTS, that plays bad moves and loses more. The Facebook AI team has acknowledged these as areas of future improvement.[11]","The family of Darkforest computer go programs is based on convolution neural networks.[3] The most recent advances in Darkfmcts3 combined convolutional neural networks with more traditional Monte Carlo tree search.[3] Darkfmcts3 is the most advanced version of Darkforest, which combines Facebook's most advanced convolutional neural network architecture from Darkfores2 with a Monte Carlo tree search.","Darkfmcts3 relies on a convolution neural networks that predicts the next k moves based on the current state of play. It treats the board as a 19x19 image with multiple channels. Each channel represents a different aspect of board information based upon the specific style of play. For standard and extended play, there are 21 and 25 different channels, respectively. In standard play, each players liberties are represented as six binary channels or planes. The respective plane is true if the player one, two, or three or more liberties available. Ko (i.e. illegal moves) is represented as one binary plane. Stone placement for each opponent and empty board positions are represented as three binary planes, and the duration since a stone has been placed is represented as real numbers on two planes, one for each player. Lastly, the opponents rank is represented by nine binary planes, where if all are true, the player is a 9d level, if 8 are true, a 8d level, and so forth. Extended play additionally considers the boarder (binary plane that is true at the border), position mask (represented as distance from the board center, i.e. 




x

(
−
0.5
∗
d
i
s
t
a
n
c

e

2


)




{\displaystyle x^{(-0.5*distance^{2})}}

, where 



x


{\displaystyle x}

 is a real number at a position), and each player's territory (binary, based on which player a location is closer to).","Darkfmct3 uses a 12-layer full convolutional network with a width of 384 nodes without weight sharing or pooling. Each convolutional layer is followed by a rectified linear unit, a popular activation function for deep neural networks.[12] A key innovation of Darkfmct3 compared to previous approaches is that it uses only one softmax function to predict the next move, which enables the approach to reduce the overall number of parameters.[3] Darkfmct3 was trained against 300 random selected games from an empirical dataset representing different game stages. The learning rate was determined by vanilla stochastic gradient descent.","Darkfmct3 synchronously couples a convolutional neural network with a Monte Carlo tree search. Because the convolutional neural network is computationally taxing, the Monte Carlo tree search focuses computation on the more likely game play trajectories. By running the neural network synchronously with the Monte Carlo tree search, it is possible to guarantee that each node is expanded by the moves predicted by the neural network.","Darkfores2 beats Darkforest, its neural network-only predecessor around 90% of the time, and Pachi, one of the best search-based engines around 95% of the time.[9] On the Kyu rating system, Darkforest holds a 1-2d level. Darkfores2 achieves a stable 3d level on KGS Go Server as a ranked bot.[1] With the added Monte Carlo tree search, Darkfmcts3 with 5,000 rollouts beats Pachi with 10k rollouts in all 250 games; with 75k rollouts it achieves a stable 5d level in KGS server, on par with state-of-the-art Go AIs (e.g., Zen, DolBaram, CrazyStone); with 110k rollouts, it won the 3rd place in January KGS Go Tournament.[4]"
"Data pre-processing is an important step in the data mining process. The phrase ""garbage in, garbage out"" is particularly applicable to data mining and machine learning projects. Data-gathering methods are often loosely controlled, resulting in out-of-range values (e.g., Income: −100), impossible data combinations (e.g., Sex: Male, Pregnant: Yes), missing values, etc. Analyzing data that has not been carefully screened for such problems can produce misleading results. Thus, the representation and quality of data is first and foremost before running an analysis.[1]","If there is much irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase is more difficult. Data preparation and filtering steps can take considerable amount of processing time. Data pre-processing includes cleaning, normalization, transformation, feature extraction and selection, etc. The product of data pre-processing is the final training set. Kotsiantis et al. (2006) present a well-known algorithm for each step of data pre-processing.[2]"
Dataiku is a French computer software startup company headquartered in Paris. The company develops a collaborative data science software platform aimed towards helping data teams deliver Big data solutions and services efficiently.,,,"The company was founded in 2013 by 4 co-founders. Two of them met while working at French search engine company Exalead, including current CEO Florian Douetteau, former VP of Research & Development at Exalead, and CTO Clément Sténac.[1] For its first two years, the company relied on its own capital equity. Dataiku raised 3.6 million in 2015 from Serena Capital et Alven Capital, two French high-tech venture capital funds.[2]","After launching in France, Dataiku opened an office in New-York City in 2015 to boost its growth and participate in the boom of the big data analytics industry.[3]","The Dataiku team is dedicated to developing a single product: data analytics software Dataiku Data Science Studio. The first version was launched in 2014 with the goal of helping data science teams use predictive models to build business applications.[2] Since then, Dataiku has released two new versions of DSS respectively facilitating data cleaning and deployment on big data infrastructures.[4]","Dataiku offers a free edition and enterprise versions with additional features, such as multi-user collaboration or real-time scoring."
"Decision lists are a representation for Boolean functions.[1] Single term decision lists are more expressive than disjunctions and conjunctions; however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form.",The language specified by a k-length decision list includes as a subset the language specified by a k-depth decision tree.,Learning decision lists can be used for attribute efficient learning.[2],A decision list (DL) of length r is of the form:,"where fi is the ith formula and bi is the ith boolean for 



i
∈
{
1...
r
}


{\displaystyle i\in \{1...r\}}

. The last if-then-else is the default case, which means formula fr is always equal to true. A k-DL is a decision list where all of formulas have at most k terms. Sometimes ""decision list"" is used to refer to a 1-DL, where all of the formulas are either a variable or its negation.",
"Deep Feature Synthesis is an algorithm developed by James Max Kanter and Kalyan Veeramachaneni in their paper ""Deep Feature Synthesis: Towards Automating Data Science Endeavors"" [1]",,,"Quoting the above paper: ""Deep Feature Synthesis is an algorithm that automatically generates features for relational datasets. In essence, the algorithm follows relationships in the data to a base field, and then sequentially applies mathematical functions along that path to create the final feature.""",Kanter and Veeramachaneni implemented the Deep Feature Synthesis algorithm in their Data Science Machine and proceeded to enter the automated results in several competitions:,"Their results competed against human teams to find predictive patterns in unfamiliar data sets. Of the 906 teams participating in the three competitions, the researchers' ""Data Science Machine"" finished ahead of 615. In two of the three competitions, the predictions made by the Data Science Machine were 94 percent and 96 percent as accurate as the winning submissions. In the third, the figure was a more modest 87 percent. But where the teams of humans typically labored over their prediction algorithms for months, the Data Science Machine took somewhere between two and 12 hours to produce each of its entries.[2]",Little to no human intervention.,Results in hours not weeks.,Relies on SQL schema and normalized table relationships.,Quickly create feature sets of predictive value.,"The process of feature synthesis from relational data is known as propositionalization, which is known at least from 1991.[3] The employed algorithm in Deep feature synthesis was for the first time described by Knobbe in 2001 [4] and is known as RollUp. RollUp was later on enhanced in PRORED.[5] A commercial version of RollUp is sold under the name Safarii.",Relational data mining
"Deeplearning4j is an open source deep learning library written for Java and the Java Virtual Machine[1][2] and a computing framework with wide support for deep learning algorithms.[3] Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, as well as word2vec, doc2vec and GloVe. These algorithms all include distributed parallel versions that integrate with Hadoop and Spark.[4] It is commercially supported by the startup Skymind.",,,"Deeplearning4j relies on the widely used programming language, Java - though it is compatible with Clojure and includes a Scala API. It is powered by its own open-source numerical computing library, ND4J, and works with both CPUs and GPUs.[5][6] Deeplearning4j is an open source project[7] primarily developed by a machine learning group in San Francisco led by Adam Gibson.[8][9] Deeplearning4j is the only open-source project listed on Google's Word2vec page for its Java implementation.[10]",Deeplearning4j has been used in a number of commercial and academic applications. The code is hosted on GitHub[11] and a support forum is maintained on Google Groups.[12],"The framework is composable, meaning shallow neural nets such as restricted Boltzmann machines, convolutional nets, autoencoders and recurrent nets can be added to one another to create deep nets of varying types.","Training with Deeplearning4j takes place in a cluster. Neural nets are trained in parallel via iterative reduce, which works on Hadoop/YARN and on Spark.[8][13] Deeplearning4j also integrates with Cuda kernels to conduct pure GPU operations, and works with distributed GPUs.","Deeplearning4j includes an n-dimensional array class using ND4J that allows for scientific computing in Java and Scala, similar to the functionality that Numpy provides to Python. It's effectively based on a library for linear algebra and matrix manipulation in a production environment.","Canova vectorizes[clarification needed] various file formats and data types using an input/output format system similar to Hadoop's use of MapReduce. A work in progress, Canova is designed to vectorize CSVs, images, sound, text and video. Canova can be used from the command line.","Deeplearning4j includes a vector space modeling and topic modeling toolkit, implemented in Java and integrating with parallel GPUs for performance. It is specifically intended for handling large text collections.","Deeplearning4j includes implementations of tf–idf, deep learning, and Mikolov's word2vec algorithm, doc2vec and GloVe – reimplemented and optimized in Java. It relies on t-SNE for word-cloud visualizations.","Real-world use cases for Deeplearning4j include fraud detection for the financial sector,[14] anomaly detection in industries such as manufacturing, recommender systems in e-commerce and advertising,[15] and image recognition. Deeplearning4j has integrated with other machine-learning platforms such as RapidMiner and Prediction.io.[16]"
"Developmental robotics (DevRob), sometimes called epigenetic robotics, is a scientific field which aims at studying the developmental mechanisms, architectures and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines. As in human children, learning is expected to be cumulative and of progressively increasing complexity, and to result from self-exploration of the world in combination with social interaction. The typical methodological approach consists in starting from theories of human and animal development elaborated in fields such as developmental psychology, neuroscience, developmental and evolutionary biology, and linguistics, then to formalize and implement them in robots, sometimes exploring extensions or variants of them. The experimentation of those models in robots allows researchers to confront them with reality, and as a consequence developmental robotics also provides feedback and novel hypotheses on theories of human and animal development.","Developmental robotics is related to, but differs from, evolutionary robotics (ER). ER uses populations of robots that evolve over time, whereas DevRob is interested in how the organization of a single robot's control system develops through experience, over time.",DevRob is also related to work done in the domains of robotics and artificial life.,,,"Can a robot learn like a child? Can it learn a variety of new skills and new knowledge unspecified at design time and in a partially unknown and changing environment? How can it discover its body and its relationships with the physical and social environment? How can its cognitive capacities continuously develop without the intervention of an engineer once it is ""out of the factory""? What can it learn through natural social interactions with humans? These are the questions at the center of developmental robotics. Alan Turing, as well as a number of other pioneers of cybernetics, already formulated those questions and the general approach in 1950,[1] but it is only since the end of the 20th century that they began to be investigated systematically.[2][3][4][5]","Because the concept of adaptive intelligent machine is central to developmental robotics, it has relationships with fields such as artificial intelligence, machine learning, cognitive robotics or computational neuroscience. Yet, while it may reuse some of the techniques elaborated in these fields, it differs from them from many perspectives. It differs from classical artificial intelligence because it does not assume the capability of advanced symbolic reasoning and focuses on embodied and situated sensorimotor and social skills rather than on abstract symbolic problems. It differs from traditional machine learning because it targets task- independent self-determined learning rather than task-specific inference over ""spoon fed human-edited sensori data"" (Weng et al., 2001). It differs from cognitive robotics because it focuses on the processes that allow the formation of cognitive capabilities rather than these capabilities themselves. It differs from computational neuroscience because it focuses on functional modeling of integrated architectures of development and learning. More generally, developmental robotics is uniquely characterized by the following three features:","Developmental robotics emerged at the crossroads of several research communities including embodied artificial intelligence, enactive and dynamical systems cognitive science, connectionism. Starting from the essential idea that learning and development happen as the self-organized result of the dynamical interactions among brains, bodies and their physical and social environment, and trying to understand how this self- organization can be harnessed to provide task-independent lifelong learning of skills of increasing complexity, developmental robotics strongly interacts with fields such as developmental psychology, developmental and cognitive neuroscience, developmental biology (embryology), evolutionary biology, and cognitive linguistics. As many of the theories coming from these sciences are verbal and/or descriptive, this implies a crucial formalization and computational modeling activity in developmental robotics. These computational models are then not only used as ways to explore how to build more versatile and adaptive machines, but also as a way to evaluate their coherence and possibly explore alternative explanations for understanding biological development.[5]","Due to the general approach and methodology, developmental robotics projects typically focus on having robots develop the same types of skills as human infants. A first category that is importantly being investigated is the acquisition of sensorimotor skills. These include the discovery of one's own body, including its structure and dynamics such as hand–eye coordination, locomotion, and interaction with objects as well as tool use, with a particular focus on the discovery and learning of affordances. A second category of skills targeted by developmental robots are social and linguistic skills: the acquisition of simple social behavioural games such as turn-taking, coordinated interaction, lexicons, syntax and grammar, and the grounding of these linguistic skills into sensorimotor skills (sometimes referred as symbol grounding). In parallel, the acquisition of associated cognitive skills are being investigated such as the emergence of the self/non-self distinction, the development of attentional capabilities, of categorization systems and higher-level representations of affordances or social constructs, of the emergence of values, empathy, or theories of mind.","The sensorimotor and social spaces in which humans and robot live are so large and complex that only a small part of potentially learnable skills can actually be explored and learnt within a life-time. Thus, mechanisms and constraints are necessary to guide developmental organisms in their development and control of the growth of complexity. There are several important families of these guiding mechanisms and constraints which are studied in developmental robotics, all inspired by human development:","While most developmental robotics projects strongly interact with theories of animal and human development, the degrees of similarities and inspiration between identified biological mechanisms and their counterpart in robots, as well as the abstraction levels of modeling, may vary a lot. While some projects aim at modeling precisely both the function and biological implementation (neural or morphological models), such as in neurorobotics, some other projects only focus on functional modeling of the mechanisms and constraints described above, and might for example reuse in their architectures techniques coming from applied mathematics or engineering fields.","As developmental robotics is a relatively novel research field and at the same time very ambitious, many fundamental open challenges remain to be solved.","First of all, existing techniques are far from allowing real-world high-dimensional robots to learn an open- ended repertoire of increasingly complex skills over a life-time period. High-dimensional continuous sensorimotor spaces are a major obstacle to be solved. Lifelong cumulative learning is another one. Actually, no experiments lasting more than a few days have been set up so far, which contrasts severely with the time period needed by human infants to learn basic sensorimotor skills while equipped with brains and morphologies which are tremendously more powerful than existing computational mechanisms.","Among the strategies to explore in order to progress towards this target, the interaction between the mechanisms and constraints described in the previous section shall be investigated more systematically. Indeed, they have so far mainly been studied in isolation. For example, the interaction of intrinsically motivated learning and socially guided learning, possibly constrained by maturation, is an essential issue to be investigated.","Another important challenge is to allow robots to perceive, interpret and leverage the diversity of multimodal social cues provided by non-engineer humans during human-robot interaction. These capacities are so far mostly too limited to allow efficient general purpose teaching from humans.","A fundamental scientific issue to be understood and resolved, which applied equally to human development, is how compositionality, functional hierarchies, primitives, and modularity, at all levels of sensorimotor and social structures, can be formed and leveraged during development. This is deeply linked with the problem of the emergence of symbols, sometimes referred as the ""symbol grounding problem"" when it comes to language acquisition. Actually, the very existence and need for symbols in the brain is actively questioned, and alternative concepts, still allowing for compositionality and functional hierarchies are being investigated.","During biological epigenesis, morphology is not fixed but rather develops in constant interaction with the development of sensorimotor and social skills. The development of morphology poses obvious practical problems with robots, but it may be a crucial mechanism that should be further explored, at least in simulation, such as in morphogenetic robotics.","Another open problem is the understanding of the relation between the key phenomena investigated by developmental robotics (e.g., hierarchical and modular sensorimotor systems, intrinsic/extrinsic/social motivations, and open-ended learning) and the underlying brain mechanisms.","Similarly, in biology, developmental mechanisms (operating at the ontogenetic time scale) strongly interact with evolutionary mechanisms (operating at the phylogenetic time scale) as shown in the flourishing ""evo-devo"" scientific literature.[6] However, the interaction of those mechanisms in artificial organisms, developmental robots in particular, is still vastly understudied. The interaction of evolutionary mechanisms, unfolding morphologies and developing sensorimotor and social skills will thus be a highly stimulating topic for the future of developmental robotics.","The NSF/DARPA funded Workshop on Development and Learning was held April 5–7, 2000 at Michigan State University. It was the first international meeting devoted to computational understanding of mental development by robots and animals. The term ""by"" was used since the agents are active during development.","The first undergraduate courses in DevRob were offered at Bryn Mawr College and Swarthmore College in the Spring of 2003 by Douglas Blank and Lisa Meeden, respectively. The first graduate course in DevRob was offered at Iowa State University by Alexander Stoytchev in the Fall of 2005."
"In machine learning and statistics, dimensionality reduction or dimension reduction is the process of reducing the number of random variables under consideration,[1] via obtaining a set of ""uncorrelated"" principal variables. It can be divided into feature selection and feature extraction.[2]",,,"Feature selection approaches try to find a subset of the original variables (also called features or attributes). There are three strategies; filter (e.g. information gain) and wrapper (e.g. search guided by accuracy) approaches, and embedded (features are selected to add or be removed while building the model based on the prediction errors). See also combinatorial optimization problems.","In some cases, data analysis such as regression or classification can be done in the reduced space more accurately than in the original space.","Feature extraction transforms the data in the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist.[3][4] For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning.[5]","The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors.",Principal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is entitled kernel PCA.,"Other prominent nonlinear techniques include manifold learning techniques such as Isomap, locally linear embedding (LLE), Hessian LLE, Laplacian eigenmaps, and local tangent space alignment (LTSA). These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA.","
More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space), while maximizing the distances between points that are not nearest neighbors.","An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical multidimensional scaling, which is identical to PCA; Isomap, which uses geodesic distances in the data space; diffusion maps, which use diffusion distances in the data space; t-distributed stochastic neighbor embedding (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis.","A different approach to nonlinear dimensionality reduction is through the use of autoencoders, a special kind of feed-forward neural networks with a bottle-neck hidden layer.[6] The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of restricted Boltzmann machines) that is followed by a finetuning stage based on backpropagation.","For high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a K-nearest neighbors algorithm (k-NN) in order to avoid the effects of the curse of dimensionality. [7]","Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step followed by clustering by K-NN on feature vectors in reduced-dimension space. In machine learning this process is also called low-dimensional embedding.[8]","For very-high-dimensional datasets (e.g. when performing similarity search on live video streams, DNA data or high-dimensional Time series) running a fast approximate K-NN search using locality sensitive hashing, random projection,[9] ""sketches"" [10] or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option.","A dimensionality reduction technique that is sometimes used in neuroscience is maximally informative dimensions, which finds a lower-dimensional representation of a dataset such that as much information as possible about the original data is preserved."
"Discriminative models, also called conditional models, are a class of models used in machine learning for modeling the dependence of an unobserved variable 



y


{\displaystyle y}

 on an observed variable 



x


{\displaystyle x}

. Within a probabilistic framework, this is done by modeling the conditional probability distribution 



P
(
y

|

x
)


{\displaystyle P(y|x)}

, which can be used for predicting 



y


{\displaystyle y}

 from 



x


{\displaystyle x}

.","Discriminative models, as opposed to generative models, do not allow one to generate samples from the joint distribution of 



x


{\displaystyle x}

 and 



y


{\displaystyle y}

. However, for tasks such as classification and regression that do not require the joint distribution, discriminative models can yield superior performance.[1][2][3] On the other hand, generative models are typically more flexible than discriminative models in expressing dependencies in complex learning tasks. In addition, most discriminative models are inherently supervised and cannot easily be extended to unsupervised learning. Application specific details ultimately dictate the suitability of selecting a discriminative versus generative model.",Examples of discriminative models used in machine learning include:
"Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done ""manually"" (or ""intellectually"") or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.","The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.","Documents may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach.",,,"Content-based classification is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20% of the content of a book should be about the class to which the book is assigned.[1] In automatic classification it could be the number of times given words appears in a document.","Request-oriented classification (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks himself: “Under which descriptors should this entity be found?” and “think of all the possible queries and decide for which ones the entity at hand is relevant” (Soergel, 1985, p. 230[2]).","Request-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library. It is probably better, however, to understand request-oriented classification as policy-based classification: The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach.","Sometimes a distinction is made between assigning documents to classes (""classification"") versus assigning subjects to documents (""subject indexing"") but as Frederick Wilfrid Lancaster has argued, this distinction is not fruitful. ""These terminological distinctions,” he writes, “are quite meaningless and only serve to cause confusion” (Lancaster, 2003, p. 21[3]). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a thesaurus and vice versa (cf., Aitchison, 1986,[4] 2004;[5] Broughton, 2008;[6] Riesthuis & Bliedung, 1991[7]). Therefore, is the act of labeling a document (say by assigning a term from a controlled vocabulary to a document) at the same time to assign that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents).","Automatic document classification tasks can be divided into three sorts: supervised document classification where some external mechanism (such as human feedback) provides information on the correct classification for documents, unsupervised document classification (also known as document clustering), where the classification must be done entirely without reference to external information, and semi-supervised document classification,[8] where parts of the documents are labeled by the external mechanism. There are several software products under various license models available.[9][10][11][12]",Automatic document classification techniques include:,Classification techniques have been applied to
"Domain Adaptation[1][2] is a field associated with machine learning and transfer learning. This scenario arises when we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution. For instance, one of the tasks of the common spam filtering problem consists in adapting a model from one user (the source distribution) to a new one who receives significantly different emails (the target distribution). Note that, when more than one source distribution is available the problem is referred to as multi-source domain adaptation.[3]",,,"Let 



X


{\displaystyle X}

 be the input space (or description space) and let 



Y


{\displaystyle Y}

 be the output space (or label space). The objective of a machine learning algorithm is to learn a mathematical model (a hypothesis) 



h
:
X
→
Y


{\displaystyle h:X\to Y}

 able to affect a label of 



Y


{\displaystyle Y}

 to an example from 



X


{\displaystyle X}

. This model is learned from a learning sample 



S
=
{
(

x

i


,

y

i


)

}

i
=
1


m


∈
(
X
×
Y

)

m




{\displaystyle S=\{(x_{i},y_{i})\}_{i=1}^{m}\in (X\times Y)^{m}}

.","Usually in supervised learning (without domain adaptation), we suppose that the examples 



(

x

i


,

y

i


)
∈
S


{\displaystyle (x_{i},y_{i})\in S}

 are drawn i.i.d. from a distribution 




D

S




{\displaystyle D_{S}}

 of support 



X
×
Y


{\displaystyle X\times Y}

 (unknown and fixed). The objective is then to learn 



h


{\displaystyle h}

 (from 



S


{\displaystyle S}

) such that it commits the least error as possible for labelling new examples coming from the distribution 




D

S




{\displaystyle D_{S}}

.","The main difference between supervised learning and domain adaptation is that in the latter situation we study two different (but related) distributions 




D

S




{\displaystyle D_{S}}

 and 




D

T




{\displaystyle D_{T}}

 on 



X
×
Y


{\displaystyle X\times Y}

. The domain adaptation task then consists of the transfer of knowledge from the source domain 




D

S




{\displaystyle D_{S}}

 to the target one 




D

T




{\displaystyle D_{T}}

. The goal is then to learn 



h


{\displaystyle h}

 (from labeled or unlabelled samples coming from the two domains) such that it commits as little error as possible on the target domain 




D

T




{\displaystyle D_{T}}

.","The major issue is the following: if a model is learned from a source domain, what is its capacity to correctly label data coming from the target domain?",There are several contexts of domain adaptation. They differ in the informations considered for the target task.,"The objective is to reweight the source labeled sample such that it ""looks like"" the target sample (in term of the error measure considered)[4][5]","A method for adapting consists in iteratively ""auto-labeling"" the target examples. The principle is simple:","Note that there exists other iterative approaches, but they usually need target labeled examples.",The goal is to find or construct a common representation space for the two domains. The objective is to obtain a space in which the domains are close to each other while keeping good performances on the source labeling task.
"Dropout is a technique of reducing overfitting in neural networks by preventing complex co-adaptations on training data. It is a very efficient way of performing model averaging with neural networks.[1] The term ""dropout"" refers to dropping out units (both hidden and visible) in a neural network.[2]"
"In artificial intelligence, eager learning is a learning method in which the system tries to construct a general, input independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system. [1] The main advantage gained in employing an eager learning method, such as an artificial neural network, is that the target function will be approximated globally during training, thus requiring much less space than a lazy learning system. Eager learning systems also deal much better with noise in the training data. Eager learning is an example of offline learning, in which post-training queries to the system have no effect on the system itself, and thus the same query to the system will always produce the same result.",The main disadvantage with eager learning is that it is generally unable to provide good local approximations in the target function.,
"In machine learning, early stopping is a form of regularization used to avoid overfitting when training a learner with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training data with each iteration. Up to a point, this improves the learner's performance on data outside of the training set. Past that point, however, improving the learner's fit to the training data comes at the expense of increased generalization error. Early stopping rules provide guidance as to how many iterations can be run before the learner begins to over-fit. Early stopping rules have been employed in many different machine learning methods, with varying amounts of theoretical foundation.",,,This section presents some of the basic machine-learning concepts required for a description of early stopping methods.,"Machine learning algorithms train a model based on a finite set of training data. During this training, the model is evaluated based on how well it predicts the observations contained in the training set. In general, however, the goal of a machine learning scheme is to produce a model that generalizes, that is, that predicts previously unseen observations. Overfitting occurs when a model fits the data in the training set well, while incurring larger generalization error.","Regularization, in the context of machine learning, refers to the process of modifying a learning algorithm so as to prevent overfitting. This generally involves imposing some sort of smoothness constraint on the learned model.[1] This smoothness may be enforced explicitly, by fixing the number of parameters in the model, or by augmenting the cost function as in Tikhonov regularization. Tikhonov regularization, along with principal component regression and many other regularization schemes, fall under the umbrella of spectral regularization, regularization characterized by the application of a filter. Early stopping also belongs to this class of methods.","Gradient descent methods are first-order, iterative, optimization methods. Each iteration updates an approximate solution to the optimization problem by taking a step in the direction of the negative of the gradient of the objective function. By choosing the step-size appropriately, such a method can be made to converge to a local minimum of the objective function. Gradient descent is used in machine-learning by defining a loss function that reflects the error of the learner on the training set and then minimizing that function.","Early-stopping can be used to regularize non-parametric regression problems encountered in machine learning. For a given input space, 



X


{\displaystyle X}

, output space, 



Y


{\displaystyle Y}

, and samples drawn from an unknown probability measure, 



ρ


{\displaystyle \rho }

, on 



Z
=
X
×
Y


{\displaystyle Z=X\times Y}

, the goal of such problems is to approximate a regression function, 




f

ρ




{\displaystyle f_{\rho }}

, given by","where 



ρ
(
y

|

x
)


{\displaystyle \rho (y|x)}

 is the conditional distribution at 



x


{\displaystyle x}

 induced by 



ρ


{\displaystyle \rho }

.[2] One common choice for approximating the regression function is to use functions from a reproducing kernel Hilbert space.[2] These spaces can be infinite dimensional, in which they can supply solutions that overfit training sets of arbitrary size. Regularization is, therefore, especially important for these methods. One way to regularize non-parametric regression problems is to apply an early stopping rule to an iterative procedure such as gradient descent.",The early stopping rules proposed for these problems are based on analysis of upper bounds on the generalization error as a function of the iteration number. They yield prescriptions for the number of iterations to run that can be computed prior to starting the solution process.[3] [4],"(Adapted from Yao, Rosasco and Caponnetto, 2007[3])","Let 



X
⊆


R


n




{\displaystyle X\subseteq \mathbb {R} ^{n}}

 and 



Y
=

R



{\displaystyle Y=\mathbb {R} }

. Given a set of samples","drawn independently from 



ρ


{\displaystyle \rho }

, minimize the functional","where, 



f


{\displaystyle f}

 is a member of the reproducing kernel Hilbert space 





H




{\displaystyle {\mathcal {H}}}

. That is, minimize the expected risk for a Least-squares loss function. Since 





E




{\displaystyle {\mathcal {E}}}

 depends on the unknown probability measure 



ρ


{\displaystyle \rho }

, it cannot be used for computation. Instead, consider the following empirical risk","Let 




f

t




{\displaystyle f_{t}}

 and 




f

t



z





{\displaystyle f_{t}^{\mathbf {z} }}

 be the t-th iterates of gradient descent applied to the expected and empirical risks, respectively, where both iterations are initialized at the origin, and both use the step size 




γ

t




{\displaystyle \gamma _{t}}

. The 




f

t




{\displaystyle f_{t}}

 form the population iteration, which converges to 




f

ρ




{\displaystyle f_{\rho }}

, but cannot be used in computation, while the 




f

t



z





{\displaystyle f_{t}^{\mathbf {z} }}

 form the sample iteration which usually converges to an overfitting solution.","We want to control the difference between the expected risk of the sample iteration and the minimum expected risk, that is, the expected risk of the regression function:",This difference can be rewritten as the sum of two terms: the difference in expected risk between the sample and population iterations and that between the population iteration and the regression function:,"This equation presents a bias-variance tradeoff, which is then solved to give an optimal stopping rule that may depend on the unknown probability distribution. That rule has associated probabilistic bounds on the generalization error. For the analysis leading to the early stopping rule and bounds, the reader is referred to the original article.[3] In practice, data-driven methods, e.g. cross-validation can be used to obtain an adaptive stopping rule.","Boosting refers to a family of algorithms in which a set of weak learners (learners that are only slightly correlated with the true process) are combined to produce a strong learner. It has been shown, for several boosting algorithms (including AdaBoost), that regularization via early stopping can provide guarantees of consistency, that is, that the result of the algorithm approaches the true solution as the number of samples goes to infinity.[5] [6] [7]","Boosting methods have close ties to the gradient descent methods described above can be regarded as a boosting method based on the 




L

2




{\displaystyle L_{2}}

 loss: L2Boost.[3]",These early stopping rules work by splitting the original training set into a new training set and a validation set. The error on the validation set is used as a proxy for the generalization error in determining when overfitting has begun. These methods are most commonly employed in the training of neural networks. Prechelt gives the following summary of a naive implementation of holdout-based early stopping as follows:[8],"More sophisticated forms use cross-validation – multiple partitions of the data into training set and validation set – instead of a single partition into a training set and validation set. Even this simple procedure is complicated in practice by the fact that the validation error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when overfitting has truly begun.[8]"
"Elastic matching is one of the pattern recognition techniques in computer science. Elastic matching (EM) is also known as deformable template, flexible matching, or nonlinear template matching.",Elastic matching can be defined as an optimization problem of two-dimensional warping specifying corresponding pixels between subjected images.
Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on the performance of learning algorithms.,,,"Consider the following situation, which is a general setting of many supervised learning problems. We have two spaces of objects 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

 and would like to learn a function 



 
h
:
X
→
Y


{\displaystyle \ h:X\to Y}

 (often called hypothesis) which outputs an object 



y
∈
Y


{\displaystyle y\in Y}

, given 



x
∈
X


{\displaystyle x\in X}

. To do so, we have at our disposal a training set of a few examples 



 
(

x

1


,

y

1


)
,
…
,
(

x

m


,

y

m


)


{\displaystyle \ (x_{1},y_{1}),\ldots ,(x_{m},y_{m})}

 where 




x

i


∈
X


{\displaystyle x_{i}\in X}

 is an input and 




y

i


∈
Y


{\displaystyle y_{i}\in Y}

 is the corresponding response that we wish to get from 



 
h
(

x

i


)


{\displaystyle \ h(x_{i})}

.","To put it more formally, we assume that there is a joint probability distribution 



P
(
x
,
y
)


{\displaystyle P(x,y)}

 over 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

, and that the training set consists of 



m


{\displaystyle m}

 instances 



 
(

x

1


,

y

1


)
,
…
,
(

x

m


,

y

m


)


{\displaystyle \ (x_{1},y_{1}),\ldots ,(x_{m},y_{m})}

 drawn i.i.d. from 



P
(
x
,
y
)


{\displaystyle P(x,y)}

. Note that the assumption of a joint probability distribution allows us to model uncertainty in predictions (e.g. from noise in data) because 



y


{\displaystyle y}

 is not a deterministic function of 



x


{\displaystyle x}

, but rather a random variable with conditional distribution 



P
(
y

|

x
)


{\displaystyle P(y|x)}

 for a fixed 



x


{\displaystyle x}

.","We also assume that we are given a non-negative real-valued loss function 



L
(



y
^



,
y
)


{\displaystyle L({\hat {y}},y)}

 which measures how different the prediction 






y
^





{\displaystyle {\hat {y}}}

 of a hypothesis is from the true outcome 



y


{\displaystyle y}

. The risk associated with hypothesis 



h
(
x
)


{\displaystyle h(x)}

 is then defined as the expectation of the loss function:","A loss function commonly used in theory is the 0-1 loss function: 



L
(



y
^



,
y
)
=
I
(



y
^



≠
y
)


{\displaystyle L({\hat {y}},y)=I({\hat {y}}\neq y)}

, where 



I
(
.
.
.
)


{\displaystyle I(...)}

 is the indicator notation.","The ultimate goal of a learning algorithm is to find a hypothesis 




h

∗




{\displaystyle h^{*}}

 among a fixed class of functions 





H




{\displaystyle {\mathcal {H}}}

 for which the risk 



R
(
h
)


{\displaystyle R(h)}

 is minimal:","In general, the risk 



R
(
h
)


{\displaystyle R(h)}

 cannot be computed because the distribution 



P
(
x
,
y
)


{\displaystyle P(x,y)}

 is unknown to the learning algorithm (this situation is referred to as agnostic learning). However, we can compute an approximation, called empirical risk, by averaging the loss function on the training set:","Empirical risk minimization principle states that the learning algorithm should choose a hypothesis 






h
^





{\displaystyle {\hat {h}}}

 which minimizes the empirical risk:",Thus the learning algorithm defined by the ERM principle consists in solving the above optimization problem.,"Empirical risk minimization for a classification problem with 0-1 loss function is known to be an NP-hard problem even for such relatively simple class of functions as linear classifiers.[1] Though, it can be solved efficiently when minimal empirical risk is zero, i.e. data is linearly separable.","In practice, machine learning algorithms cope with that either by employing a convex approximation to 0-1 loss function (like hinge loss for SVM), which is easier to optimize, or by posing assumptions on the distribution 



P
(
x
,
y
)


{\displaystyle P(x,y)}

 (and thus stop being agnostic learning algorithms to which the above result applies)."
"Recently in the area of machine learning the concept of combining classifiers is proposed as a new direction for the improvement of the performance of individual classifiers. These classifiers could be based on a variety of classification methodologies, and could achieve different rate of correctly classified individuals. The goal of classification result integration algorithms is to generate more certain, precise and accurate system results. Dietterich (2001) provides an accessible and informal reasoning, from statistical, computational and representational viewpoints, of why ensembles can improve results.",,,Numerous methods have been suggested for the creation of ensemble of classifiers.,"The first weakness, increased storage, is a direct consequence of the requirement that all component classifiers, instead of a single classifier, need to be stored after training. The total storage depends on the size of each component classifier itself and the size of the ensemble (number of classifiers in the ensemble). The second weakness is increased computation: to classify an input query, all component classifiers (instead of a single classifier) must be processed, and thus it requires more execution time. The last weakness is decreased comprehensibility. With involvement of multiple classifiers in decision-making, it is more difficult for users to perceive the underlying reasoning process leading to a decision.[dubious – discuss]","Bagging is a method of the first category (Breiman, 1996). If there is a training set of size t, then it is possible to draw t random instances from it with replacement (i.e. using a uniform distribution), these t instances can be learned, and this process can be repeated several times. Since the draw is with replacement, usually the instances drawn will contain some duplicates and some omissions as compared to the original training set. Each cycle through the process results in one classifier. After the construction of several classifiers, taking a vote of the predictions of each classifier performs the final prediction.","Another method of the first category is called boosting. AdaBoost is a practical version of the boosting approach (Freund and Schapire, 1996). Boosting is similar in overall structure to bagging, except that one keeps track of the performance of the learning algorithm and forces it to concentrate its efforts on instances that have not been correctly learned. Instead of choosing the t training instances randomly using a uniform distribution, one chooses the training instances in such a manner as to favour the instances that have not been accurately learned. After several cycles, the prediction is performed by taking a weighted vote of the predictions of each classifier, with the weights being proportional to each classifier’s accuracy on its training set.","Boosting algorithms are considered stronger than bagging on noise free data. However, there are strong empirical indications that bagging is much more robust than boosting in noisy settings. For this reason, Kotsiantis and Pintelas (2004) built an ensemble using a voting methodology of bagging and boosting ensembles that give better classification accuracy."
,,"In PAC learning, error tolerance refers to the ability of an algorithm to learn when the examples received have been corrupted in some way. In fact, this is a very common and important issue since in many applications it is not possible to access noise-free data. Noise can interfere with the learning process at different levels: the algorithm may receive data that have been occasionally mislabeled, or the inputs may have some false information, or the classification of the examples may have been maliciously adulterated.","In the following, let 



X


{\displaystyle X}

 be our 



n


{\displaystyle n}

-dimensional input space. Let 





H




{\displaystyle {\mathcal {H}}}

 be a class of functions that we wish to use in order to learn a 



{
0
,
1
}


{\displaystyle \{0,1\}}

-valued target function 



f


{\displaystyle f}

 defined over 



X


{\displaystyle X}

. Let 





D




{\displaystyle {\mathcal {D}}}

 be the distribution of the inputs over 



X


{\displaystyle X}

. The goal of a learning algorithm 





A




{\displaystyle {\mathcal {A}}}

 is to choose the best function 



h
∈


H




{\displaystyle h\in {\mathcal {H}}}

 such that it minimizes 



e
r
r
o
r
(
h
)
=

P

x
∼


D




(
h
(
x
)
≠
f
(
x
)
)


{\displaystyle error(h)=P_{x\sim {\mathcal {D}}}(h(x)\neq f(x))}

. Let us suppose we have a function 



s
i
z
e
(
f
)


{\displaystyle size(f)}

 that can measure the complexity of 



f


{\displaystyle f}

. Let 



O
r
a
c
l
e
(
x
)


{\displaystyle Oracle(x)}

 be an oracle that, whenever called, returns an example 



x


{\displaystyle x}

 and its correct label 



f
(
x
)


{\displaystyle f(x)}

.","When no noise corrupts the data, we can define learning in the Valiant setting:[1][2]","Definition: We say that 



f


{\displaystyle f}

 is efficiently learnable using 





H




{\displaystyle {\mathcal {H}}}

 in the Valiant setting if there exists a learning algorithm 





A




{\displaystyle {\mathcal {A}}}

 that has access to 



O
r
a
c
l
e
(
x
)


{\displaystyle Oracle(x)}

 and a polynomial 



p
(
⋅
,
⋅
,
⋅
,
⋅
)


{\displaystyle p(\cdot ,\cdot ,\cdot ,\cdot )}

 such that for any 



0
<
ε
≤
1


{\displaystyle 0<\varepsilon \leq 1}

 and 



0
<
δ
≤
1


{\displaystyle 0<\delta \leq 1}

 it outputs, in a number of calls to the oracle bounded by 



p

(


1
ε


,


1
δ


,
n
,
s
i
z
e
(
f
)
)



{\displaystyle p\left({\frac {1}{\varepsilon }},{\frac {1}{\delta }},n,size(f)\right)}

 , a function 



h
∈


H




{\displaystyle h\in {\mathcal {H}}}

 that satisfies with probability at least 



1
−
δ


{\displaystyle 1-\delta }

 the condition 



e
r
r
o
r
(
h
)
≤
ε


{\displaystyle error(h)\leq \varepsilon }

.","In the following we will define learnability of 



f


{\displaystyle f}

 when data have suffered some modification.[3][4][5]","In the Classification Noise Model[6] a noise rate 



0
≤
η
<


1
2




{\displaystyle 0\leq \eta <{\frac {1}{2}}}

 is introduced. Then, instead of 



O
r
a
c
l
e
(
x
)


{\displaystyle Oracle(x)}

 that returns always the correct label of example 



x


{\displaystyle x}

, algorithm 





A




{\displaystyle {\mathcal {A}}}

 can only call a faulty oracle 



O
r
a
c
l
e
(
x
,
η
)


{\displaystyle Oracle(x,\eta )}

 that will flip the label of 



x


{\displaystyle x}

 with probability 



η


{\displaystyle \eta }

. As in the Valiant case, the goal of a learning algorithm 





A




{\displaystyle {\mathcal {A}}}

 is to choose the best function 



h
∈


H




{\displaystyle h\in {\mathcal {H}}}

 such that it minimizes 



e
r
r
o
r
(
h
)
=

P

x
∼


D




(
h
(
x
)
≠
f
(
x
)
)


{\displaystyle error(h)=P_{x\sim {\mathcal {D}}}(h(x)\neq f(x))}

. In applications it is difficult to have access to the real value of 



η


{\displaystyle \eta }

, but we assume we have access to its upperbound 




η

B




{\displaystyle \eta _{B}}

.[7] Note that if we allow the noise rate to be 



1

/

2


{\displaystyle 1/2}

, then learning becomes impossible in any amount of computation time, because every label conveys no information about the target function.","Definition: We say that 



f


{\displaystyle f}

 is efficiently learnable using 





H




{\displaystyle {\mathcal {H}}}

 in the classification noise model if there exists a learning algorithm 





A




{\displaystyle {\mathcal {A}}}

 that has access to 



O
r
a
c
l
e
(
x
,
η
)


{\displaystyle Oracle(x,\eta )}

 and a polynomial 



p
(
⋅
,
⋅
,
⋅
,
⋅
)


{\displaystyle p(\cdot ,\cdot ,\cdot ,\cdot )}

 such that for any 



0
≤
η
≤


1
2




{\displaystyle 0\leq \eta \leq {\frac {1}{2}}}

, 



0
≤
ε
≤
1


{\displaystyle 0\leq \varepsilon \leq 1}

 and 



0
≤
δ
≤
1


{\displaystyle 0\leq \delta \leq 1}

 it outputs, in a number of calls to the oracle bounded by 



p

(


1

1
−
2

η

B





,


1
ε


,


1
δ


,
n
,
s
i
z
e
(
f
)
)



{\displaystyle p\left({\frac {1}{1-2\eta _{B}}},{\frac {1}{\varepsilon }},{\frac {1}{\delta }},n,size(f)\right)}

 , a function 



h
∈


H




{\displaystyle h\in {\mathcal {H}}}

 that satisfies with probability at least 



1
−
δ


{\displaystyle 1-\delta }

 the condition 



e
r
r
o
r
(
h
)
≤
ε


{\displaystyle error(h)\leq \varepsilon }

.","Statistical Query Learning[8] is a kind of active learning problem in which the learning algorithm 





A




{\displaystyle {\mathcal {A}}}

 can decide if to request information about the likelihood 




P

f
(
x
)




{\displaystyle P_{f(x)}}

 that a function 



f


{\displaystyle f}

 correctly labels example 



x


{\displaystyle x}

, and receives an answer accurate within a tolerance 



α


{\displaystyle \alpha }

. Formally, whenever the learning algorithm 





A




{\displaystyle {\mathcal {A}}}

 calls the oracle 



O
r
a
c
l
e
(
x
,
α
)


{\displaystyle Oracle(x,\alpha )}

, it receives as feedback probability 




Q

f
(
x
)




{\displaystyle Q_{f(x)}}

, such that 




Q

f
(
x
)


−
α
≤

P

f
(
x
)


≤

Q

f
(
x
)


+
α


{\displaystyle Q_{f(x)}-\alpha \leq P_{f(x)}\leq Q_{f(x)}+\alpha }

.","Definition: We say that 



f


{\displaystyle f}

 is efficiently learnable using 





H




{\displaystyle {\mathcal {H}}}

 in the Statistical Query Learning Model if there exists a learning algorithm 





A




{\displaystyle {\mathcal {A}}}

 that has access to 



O
r
a
c
l
e
(
x
,
α
)


{\displaystyle Oracle(x,\alpha )}

 and polynomials 



p
(
⋅
,
⋅
,
⋅
)


{\displaystyle p(\cdot ,\cdot ,\cdot )}

, 



q
(
⋅
,
⋅
,
⋅
)


{\displaystyle q(\cdot ,\cdot ,\cdot )}

, and 



r
(
⋅
,
⋅
,
⋅
)


{\displaystyle r(\cdot ,\cdot ,\cdot )}

 such that for any 



0
<
ε
≤
1


{\displaystyle 0<\varepsilon \leq 1}

 the following hold:","Note that the confidence parameter 



δ


{\displaystyle \delta }

 does not appear in the definition of learning. This is because the main purpose of 



δ


{\displaystyle \delta }

 is to allow the learning algorithm a small probability of failure due to an unrepresentative sample. Since now 



O
r
a
c
l
e
(
x
,
α
)


{\displaystyle Oracle(x,\alpha )}

 always guarantees to meet the approximation criterion 




Q

f
(
x
)


−
α
≤

P

f
(
x
)


≤

Q

f
(
x
)


+
α


{\displaystyle Q_{f(x)}-\alpha \leq P_{f(x)}\leq Q_{f(x)}+\alpha }

, the failure probability is no longer needed.","The statistical query model is strictly weaker than the PAC model: any efficiently SQ-learnable class is efficiently PAC learnable in the presence of classification noise, but there exist efficient PAC-learnable problems such as parity that are not efficiently SQ-learnable.[8]","In the Malicious Classification Model[9] an adversary generates errors to foil the learning algorithm. This setting describes situations of error burst, which may occur when for a limited time transmission equipment malfunctions repeteadly. Formally, algorithm 





A




{\displaystyle {\mathcal {A}}}

 calls an oracle 



O
r
a
c
l
e
(
x
,
β
)


{\displaystyle Oracle(x,\beta )}

 that returns a correctly labeled example 



x


{\displaystyle x}

 drawn, as usual, from distribution 





D




{\displaystyle {\mathcal {D}}}

 over the input space with probability 



1
−
β


{\displaystyle 1-\beta }

, but it returns with probability 



β


{\displaystyle \beta }

 an example drawn from a distribution that is not related to 





D




{\displaystyle {\mathcal {D}}}

. Moreover, this maliciously chosen example may strategically selected by an adversary who has knowledge of 



f


{\displaystyle f}

, 



β


{\displaystyle \beta }

, 





D




{\displaystyle {\mathcal {D}}}

, or the current progress of the learning algorithm.","Definition: Given a bound 




β

B


<


1
2




{\displaystyle \beta _{B}<{\frac {1}{2}}}

 for 



0
≤
β
<


1
2




{\displaystyle 0\leq \beta <{\frac {1}{2}}}

, we say that 



f


{\displaystyle f}

 is efficiently learnable using 





H




{\displaystyle {\mathcal {H}}}

 in the Malicious Classification Model if there exist a learning algorithm 





A




{\displaystyle {\mathcal {A}}}

 that has access to 



O
r
a
c
l
e
(
x
,
β
)


{\displaystyle Oracle(x,\beta )}

 and a polynomial 



p
(
⋅
,
⋅
,
⋅
,
⋅
,
⋅
)


{\displaystyle p(\cdot ,\cdot ,\cdot ,\cdot ,\cdot )}

 such that for any 



0
<
ε
≤
1


{\displaystyle 0<\varepsilon \leq 1}

, 



0
<
δ
≤
1


{\displaystyle 0<\delta \leq 1}

 it outputs, in a number of calls to the oracle bounded by 



p

(


1

1

/

2
−

β

B





,


1
ε


,


1
δ


,
n
,
s
i
z
e
(
f
)
)



{\displaystyle p\left({\frac {1}{1/2-\beta _{B}}},{\frac {1}{\varepsilon }},{\frac {1}{\delta }},n,size(f)\right)}

 , a function 



h
∈


H




{\displaystyle h\in {\mathcal {H}}}

 that satisfies with probability at least 



1
−
δ


{\displaystyle 1-\delta }

 the condition 



e
r
r
o
r
(
h
)
≤
ε


{\displaystyle error(h)\leq \varepsilon }

.","In the Nonuniform Random Attribute Noise[10][11] model the algorithm is learning a Boolean function, a malicious oracle 



O
r
a
c
l
e
(
x
,
ν
)


{\displaystyle Oracle(x,\nu )}

 may flip each 



i


{\displaystyle i}

-th bit of example 



x
=
(

x

1


,

x

2


,
…
,

x

n


)


{\displaystyle x=(x_{1},x_{2},\ldots ,x_{n})}

 independently with probability 




ν

i


≤
ν


{\displaystyle \nu _{i}\leq \nu }

.","This type of error can irreparably foil the algorithm, in fact the following theorem holds:","In the Nonuniform Random Attribute Noise setting, an algorithm 





A




{\displaystyle {\mathcal {A}}}

 can output a function 



h
∈


H




{\displaystyle h\in {\mathcal {H}}}

 such that 



e
r
r
o
r
(
h
)
<
ε


{\displaystyle error(h)<\varepsilon }

 only if 



ν
<
2
ε


{\displaystyle \nu <2\varepsilon }

."
Sources: Fawcett (2006) and Powers (2011).[1][2],"The evaluation of binary classifiers compares two methods of assigning a binary attribute, one of which is usually a standard method and the other is being investigated. There are many metrics that can be used to measure the performance of a classifier or predictor; different fields have different preferences for specific metrics due to different goals. For example, in medicine sensitivity and specificity are often used, while in computer science precision and recall are preferred. An important distinction is between metrics that are independent on the prevalence (how often each category occurs in the population), and metrics that depend on the prevalence – both types are useful, but they have very different properties.",,,"Given a data set, a classification (the output of a classifier on that set) gives two numbers: the number of positives and the number of negatives, which add up to the total size of the set. To evaluate a classifier, one compares its output to another reference classification – ideally a perfect classification, but in practice the output of another gold standard test – and cross tabulates the data into a 2×2 contingency table, comparing the two classifications. One then evaluates the classifier relative to the gold standard by computing summary statistics of these 4 numbers. Generally these statistics will be scale invariant (scaling all the numbers by the same factor does not change the output), to make them independent of population size, which is achieved by using ratios of homogeneous functions, most simply homogeneous linear or homogeneous quadratic functions.","Say we test some people for the presence of a disease. Some of these people have the disease, and our test correctly says they are positive. They are called true positives (TP). Some have the disease, but the test incorrectly claims they don't. They are called false negatives (FN). Some don't have the disease, and the test says they don't – true negatives (TN). Finally, there might be healthy people who have a positive test result – false positives (FP). These can be arranged into a 2×2 contingency table (confusion matrix), conventionally with the test result on the vertical axis and the actual condition on the horizontal axis.","These numbers can then be totaled, yielding both a grand total and marginal totals. Totaling the entire table, the number of true positives, false negatives, true negatives, and false positives add up to 100% of the set. Totaling the rows (adding horizontally) the number of true positives and false positives add up to 100% of the test positives, and likewise for negatives. Totaling the columns (adding vertically), the number of true positives and false negatives add up to 100% of the condition positives (conversely for negatives). The basic marginal ratio statistics are obtained by dividing the 2×2=4 values in the table by the marginal totals (either rows or columns), yielding 2 auxiliary 2×2 tables, for a total of 8 ratios. These ratios come in 4 complementary pairs, each pair summing to 1, and so each of these derived 2×2 tables can be summarized as a pair of 2 numbers, together with their complements. Further statistics can be obtained by taking ratios of these ratios, ratios of ratios, or more complicated functions.",The contingency table and the most common derived ratios are summarized below; see sequel for details.,,"Note that the columns correspond to the condition actually being positive or negative (or classified as such by the gold standard), as indicated by the color-coding, and the associated statistics are prevalence-independent, while the rows correspond to the test being positive or negative, and the associated statistics are prevalence-dependent. There are analogous likelihood ratios for prediction values, but these are less commonly used, and not depicted above.",The fundamental prevalence-independent statistics are sensitivity and specificity.,"Sensitivity or True Positive Rate (TPR), also known as recall, is the proportion of people that tested positive and are positive (True Positive, TP) of all the people that actually are positive (Condition Positive, CP = TP + FN). It can be seen as the probability that the test is positive given that the patient is sick. With higher sensitivity, fewer actual cases of disease go undetected (or, in the case of the factory quality control, fewer faulty products go to the market).","Specificity (SPC) or True Negative Rate (TNR) is the proportion of people that tested negative and are negative (True Negative, TN) of all the people that actually are negative (Condition Negative, CN = TN + FP). As with sensitivity, it can be looked at as the probability that the test result is negative given that the patient is not sick. With higher specificity, fewer healthy people are labeled as sick (or, in the factory case, fewer good products are discarding).","The relationship between sensitivity and specificity, as well as the performance of the classifier, can be visualized and studied using the Receiver Operating Characteristic (ROC) curve.","In theory, sensitivity and specificity are independent in the sense that it is possible to achieve 100% in both (such as in the red/blue ball example given above). In more practical, less contrived instances, however, there is usually a trade-off, such that they are inversely proportional to one another to some extent. This is because we rarely measure the actual thing we would like to classify; rather, we generally measure an indicator of the thing we would like to classify, referred to as a surrogate marker. The reason why 100% is achievable in the ball example is because redness and blueness is determined by directly detecting redness and blueness. However, indicators are sometimes compromised, such as when non-indicators mimic indicators or when indicators are time-dependent, only becoming evident after a certain lag time. The following example of a pregnancy test will make use of such an indicator.","Modern pregnancy tests do not use the pregnancy itself to determine pregnancy status; rather, human chorionic gonadotropin is used, or hCG, present in the urine of gravid females, as a surrogate marker to indicate that a woman is pregnant. Because hCG can also be produced by a tumor, the specificity of modern pregnancy tests cannot be 100% (in that false positives are possible). Also, because hCG is present in the urine in such small concentrations after fertilization and early embryogenesis, the sensitivity of modern pregnancy tests cannot be 100% (in that false negatives are possible).","In addition to sensitivity and specificity, the performance of a binary classification test can be measured with positive predictive value (PPV), also known as precision, and negative predictive value (NPV). The positive prediction value answers the question ""If the test result is positive, how well does that predict an actual presence of disease?"". It is calculated as TP/(TP + FP); that is, it is the proportion of true positives out of all positive results. The negative prediction value is the same, but for negatives, naturally.","Prevalence has a significant impact on prediction values. As an example, suppose there is a test for a disease with 99% sensitivity and 99% specificity. If 2000 people are tested and the prevalence (in the sample) is 50%, 1000 of them are sick and 1000 of them are healthy. Thus about 990 true positives and 990 true negatives are likely, with 10 false positives and 10 false negatives. The positive and negative prediction values would be 99%, so there can be high confidence in the result.","However, if the prevalence is only 5%, so of the 2000 people only 100 are really sick, then the prediction values change significantly. The likely result is 99 true positives, 1 false negative, 1881 true negatives and 19 false positives. Of the 19+99 people tested positive, only 99 really have the disease – that means, intuitively, that given that a patient's test result is positive, there is only 84% chance that they really have the disease. On the other hand, given that the patient's test result is negative, there is only 1 chance in 1882, or 0.05% probability, that the patient has the disease despite the test result.",There are various relationships between these ratios.,"If the prevalence, sensitivity, and specificity are known, the positive predictive value can be obtained from the following identity:","If the prevalence, sensitivity, and specificity are known, the negative predictive value can be obtained from the following identity:","In addition to the paired metrics, there are also single metrics that give a single number to evaluate the test.","Perhaps the simplest statistic is accuracy or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized; it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications: (TP + TN)/Total Population = (TP + TN)/(TP + TN + FP + FN). This is often not very useful, compared to the marginal ratios, as it does not yield useful marginal interpretations, due to mixing true positives (test positive, condition positive) and true negatives (test negative, condition negative) – in terms of the condition table, it sums the diagonal; further, it is prevalence-dependent. The complement is the Fraction Incorrect (FiC): FC + FiC = 1, or (FP + FN)/(TP + TN + FP + FN) – this is the sum of the antidiagonal, divided by the total population.","The diagnostic odds ratio (DOR) is a more useful overall metric, which can be defined directly as (TP×TN)/(FP×FN) = (TP/FN)/(FP/TN), or indirectly as a ratio of ratio of ratios (ratio of likelihood ratios, which are themselves ratios of True Rates or Prediction Values). This has a useful interpretation – as an odds ratio – and is prevalence-independent.","An F-score is a combination of the precision and the recall, providing a single score. There is a one-parameter family of statistics, with parameter β, which determines the relative weights of precision and recall. The traditional or balanced F-score (F1 score) is the harmonic mean of precision and recall:","Note, however, that the F-scores do not take the true negative rate into account, and that measures such as the Phi coefficient, Matthews correlation coefficient, Informedness or Cohen's kappa may be preferable to assess the performance of a binary classifier.[2] As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are markedness (deltap) and informedness (deltap').[3]",Other metrics include Youden's J statistic.
The term evolvability is used for a recent framework of computational learning introduced by Leslie Valiant in his paper of the same name and described below. The aim of this theory is to model biological evolution and categorize which types of mechanisms are evolvable. Evolution is an extension of PAC learning and learning from statistical queries.,"Let 




F

n





{\displaystyle F_{n}\,}

 and 




R

n





{\displaystyle R_{n}\,}

 be collections of functions on 



n



{\displaystyle n\,}

 variables. Given an ideal function 



f
∈

F

n




{\displaystyle f\in F_{n}}

, the goal is to find by local search a representation 



r
∈

R

n




{\displaystyle r\in R_{n}}

 that closely approximates 



f



{\displaystyle f\,}

. This closeness is measured by the performance 



Perf
⁡
(
f
,
r
)


{\displaystyle \operatorname {Perf} (f,r)}

 of 



r



{\displaystyle r\,}

 with respect to 



f



{\displaystyle f\,}

.","As is the case in the biological world, there is a difference between genotype and phenotype. In general, there can be multiple representations (genotypes) that correspond to the same function (phenotype). That is, for some 



r
,

r
′

∈

R

n




{\displaystyle r,r'\in R_{n}}

, with 



r
≠

r
′




{\displaystyle r\neq r'\,}

, still 



r
(
x
)
=

r
′

(
x
)



{\displaystyle r(x)=r'(x)\,}

 for all 



x
∈

X

n




{\displaystyle x\in X_{n}}

. However, this need not be the case. The goal then, is to find a representation that closely matches the phenotype of the ideal function, and the spirit of the local search is to allow only small changes in the genotype. Let the neighborhood 



N
(
r
)



{\displaystyle N(r)\,}

 of a representation 



r



{\displaystyle r\,}

 be the set of possible mutations of 



r



{\displaystyle r\,}

.","For simplicity, consider Boolean functions on 




X

n


=
{
−
1
,
1

}

n





{\displaystyle X_{n}=\{-1,1\}^{n}\,}

, and let 




D

n





{\displaystyle D_{n}\,}

 be a probability distribution on 




X

n





{\displaystyle X_{n}\,}

. Define the performance in terms of this. Specifically,","Note that 



Perf
⁡
(
f
,
r
)
=
Prob
⁡
(
f
(
x
)
=
r
(
x
)
)
−
Prob
⁡
(
f
(
x
)
≠
r
(
x
)
)
.


{\displaystyle \operatorname {Perf} (f,r)=\operatorname {Prob} (f(x)=r(x))-\operatorname {Prob} (f(x)\neq r(x)).}

 In general, for non-Boolean functions, the performance will not correspond directly to the probability that the functions agree, although it will have some relationship.","Throughout an organism's life, it will only experience a limited number of environments, so its performance cannot be determined exactly. The empirical performance is defined by 




Perf

s


⁡
(
f
,
r
)
=


1
s



∑

x
∈
S


f
(
x
)
r
(
x
)
,


{\displaystyle \operatorname {Perf} _{s}(f,r)={\frac {1}{s}}\sum _{x\in S}f(x)r(x),}

 where 



S



{\displaystyle S\,}

 is a multiset of 



s



{\displaystyle s\,}

 independent selections from 




X

n





{\displaystyle X_{n}\,}

 according to 




D

n





{\displaystyle D_{n}\,}

. If 



s



{\displaystyle s\,}

 is large enough, evidently 




Perf

s


⁡
(
f
,
r
)


{\displaystyle \operatorname {Perf} _{s}(f,r)}

 will be close to the actual performance 



Perf
⁡
(
f
,
r
)


{\displaystyle \operatorname {Perf} (f,r)}

.","Given an ideal function 



f
∈

F

n




{\displaystyle f\in F_{n}}

, initial representation 



r
∈

R

n




{\displaystyle r\in R_{n}}

, sample size 



s



{\displaystyle s\,}

, and tolerance 



t



{\displaystyle t\,}

, the mutator 



Mut
⁡
(
f
,
r
,
s
,
t
)


{\displaystyle \operatorname {Mut} (f,r,s,t)}

 is a random variable defined as follows. Each 




r
′

∈
N
(
r
)


{\displaystyle r'\in N(r)}

 is classified as beneficial, neutral, or deleterious, depending on its empirical performance. Specifically,","If there are any beneficial mutations, then 



Mut
⁡
(
f
,
r
,
s
,
t
)


{\displaystyle \operatorname {Mut} (f,r,s,t)}

 is equal to one of these at random. If there are no beneficial mutations, then 



Mut
⁡
(
f
,
r
,
s
,
t
)


{\displaystyle \operatorname {Mut} (f,r,s,t)}

 is equal to a random neutral mutation. In light of the similarity to biology, 



r



{\displaystyle r\,}

 itself is required to be available as a mutation, so there will always be at least one neutral mutation.","The intention of this definition is that at each stage of evolution, all possible mutations of the current genome are tested in the environment. Out of the ones who thrive, or at least survive, one is chosen to be the candidate for the next stage. Given 




r

0


∈

R

n




{\displaystyle r_{0}\in R_{n}}

, we define the sequence 




r

0


,

r

1


,

r

2


,
…


{\displaystyle r_{0},r_{1},r_{2},\ldots }

 by 




r

i
+
1


=
Mut
⁡
(
f
,

r

i


,
s
,
t
)


{\displaystyle r_{i+1}=\operatorname {Mut} (f,r_{i},s,t)}

. Thus 




r

g





{\displaystyle r_{g}\,}

 is a random variable representing what 




r

0





{\displaystyle r_{0}\,}

 has evolved to after 



g



{\displaystyle g\,}

 generations.","Let 



F



{\displaystyle F\,}

 be a class of functions, 



R



{\displaystyle R\,}

 be a class of representations, and 



D



{\displaystyle D\,}

 a class of distributions on 



X



{\displaystyle X\,}

. We say that 



F



{\displaystyle F\,}

 is evolvable by 



R



{\displaystyle R\,}

 over 



D



{\displaystyle D\,}

 if there exists polynomials 



p
(
⋅
,
⋅
)


{\displaystyle p(\cdot ,\cdot )}

, 



s
(
⋅
,
⋅
)


{\displaystyle s(\cdot ,\cdot )}

, 



t
(
⋅
,
⋅
)


{\displaystyle t(\cdot ,\cdot )}

, and 



g
(
⋅
,
⋅
)


{\displaystyle g(\cdot ,\cdot )}

 such that for all 



n



{\displaystyle n\,}

 and all 



ϵ
>
0



{\displaystyle \epsilon >0\,}

, for all ideal functions 



f
∈

F

n




{\displaystyle f\in F_{n}}

 and representations 




r

0


∈

R

n




{\displaystyle r_{0}\in R_{n}}

, with probability at least 



1
−
ϵ



{\displaystyle 1-\epsilon \,}

,","where the sizes of neighborhoods 



N
(
r
)



{\displaystyle N(r)\,}

 for 



r
∈

R

n





{\displaystyle r\in R_{n}\,}

 are at most 



p
(
n
,
1

/

ϵ
)



{\displaystyle p(n,1/\epsilon )\,}

, the sample size is 



s
(
n
,
1

/

ϵ
)



{\displaystyle s(n,1/\epsilon )\,}

, the tolerance is 



t
(
1

/

n
,
ϵ
)



{\displaystyle t(1/n,\epsilon )\,}

, and the generation size is 



g
(
n
,
1

/

ϵ
)



{\displaystyle g(n,1/\epsilon )\,}

.","



F



{\displaystyle F\,}

 is evolvable over 



D



{\displaystyle D\,}

 if it is evolvable by some 



R



{\displaystyle R\,}

 over 



D



{\displaystyle D\,}

.","



F



{\displaystyle F\,}

 is evolvable if it is evolvable over all distributions 



D



{\displaystyle D\,}

.","The class of conjunctions and the class of disjunctions are evolvable over the uniform distribution for short conjunctions and disjunctions, respectively.","The class of parity functions (which evaluate to the parity of the number of true literals in a given subset of literals) are not evolvable, even for the uniform distribution.",Evolvability implies PAC learnability.
Expectation propagation (EP) is a technique in Bayesian machine learning.,EP finds approximations to a probability distribution. It uses an iterative approach that leverages the factorization structure of the target distribution. It differs from other Bayesian approximation approaches such as Variational Bayesian methods.,
"'Explanation-based learning (EBL)is a form of machine learning that exploits a very strong, or even perfect, domain theory to make generalizations or form concepts from training examples.[1]",,,"An example of EBL using a perfect domain theory is a program that learns to play chess by being shown examples. A specific chess position that contains an important feature, say, ""Forced loss of black queen in two moves,"" includes many irrelevant features, such as the specific scattering of pawns on the board. EBL can take a single training example and determine what are the relevant features in order to form a generalization.[2]","A domain theory is perfect or complete if it contains, in principle, all information needed to decide any question about the domain. For example, the domain theory for chess is simply the rules of chess. Knowing the rules, in principle it is possible to deduce the best move in any situation. However, actually making such a deduction is impossible in practice due to combinatoric explosion. EBL uses training examples to make searching for deductive consequences of a domain theory efficient in practice.","In essence, an EBL system works by finding a way to deduce each training example from the system's existing database of domain theory. Having a short proof of the training example extends the domain-theory database, enabling the EBL system to find and classify future examples that are similar to the training example very quickly.[3] The main drawback of the method---the cost of applying the learned proof macros, as these become numerous---was analyzed by Minton.[4]",EBL software takes four inputs:,"An especially good application domain for an EBL is natural language processing (NLP). Here a rich domain theory, i.e., a natural language grammar---although neither perfect nor complete, is tuned to a particular application or particular language usage, using a treebank (training examples). Rayner pioneered this work.[6] The first successful industrial application was to a commercial NL interface to relational databases.[7] The method has been successfully applied to several large-scale natural language parsing system,[8] where the utility problem was solved by omitting the original grammar (domain theory) and using specialized LR-parsing techniques, resulting in huge speed-ups, at a cost in coverage, but with a gain in disambiguation. EBL-like techniques have also been applied to surface generation, the converse of parsing.[9]","When applying EBL to NLP, the operationality criteria can be hand-crafted,[10] or can be inferred from the treebank using either the entropy of its or-nodes[11] or a target coverage/disambiguation trade-off (= recall/precision trade-off = f-score).[12] EBL can also be used to compile grammar-based language models for speech recognition, from general unification grammars.[13] Note how the utility problem, first exposed by Minton, was solved by discarding the original grammar/domain theory, and that the quoted articles tend to contain the phrase grammar specialization---quite the opposite of the original term explanation-based generalization. Perhaps the best name for this technique would be data-driven search space reduction. Other people who worked on EBL for NLP include Guenther Neumann, Aravind Joshi, Srinivas Bangalore, and Khalil Sima'an."
"In machine learning and pattern recognition, a feature is an individual measurable property of a phenomenon being observed.[1] Choosing informative, discriminating and independent features is a crucial step for effective algorithms in pattern recognition, classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of ""feature"" is related to that of explanatory variable used in statistical techniques such as linear regression.","The initial set of raw features can be redundant and too large to be managed. Therefore, a preliminary step in many applications of machine learning and pattern recognition consists of selecting a subset of features, or constructing a new and reduced set of features to facilitate learning, and to improve generalization and interpretability.","Extracting or selecting features is a combination of art and science; developing systems to do so is known as feature engineering. It requires the experimentation of multiple possibilities and the combination of automated techniques with the intuition and knowledge of the domain expert. Automating this process is feature learning, where a machine not only uses features for learning, but learns the features itself.",,,"A set of numeric features can be conveniently described by a feature vector. An example of reaching a two way classification from a feature vector (related to the perceptron) consists of calculating the scalar product between the feature vector and a vector of weights, comparing the result with a threshold, and deciding the class based on the comparison.","Algorithms for classification from a feature vector include nearest neighbor classification, neural networks, and statistical techniques such as Bayesian approaches.","In character recognition, features may include histograms counting the number of black pixels along horizontal and vertical directions, number of internal holes, stroke detection and many others.","In speech recognition, features for recognizing phonemes can include noise ratios, length of sounds, relative power, filter matches and many others.","In spam detection algorithms, features may include the presence or absence of certain email headers, the email structure, the language, the frequency of specific terms, the grammatical correctness of the text.","In computer vision, there are a large number of possible features, such as edges and objects."
"Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The need for manual feature engineering can be obviated by automated feature learning.","Feature engineering is an informal topic, but it is considered essential in applied machine learning.","Coming up with features is difficult, time-consuming, requires expert knowledge. ""Applied machine learning"" is basically feature engineering.","When working on a machine learning problem, feature engineering is manually designing what the input x's should be.",,,"A feature is a piece of information that might be useful for prediction. Any attribute could be a feature, as long as it is useful to the model.","The purpose of a feature, other than being an attribute, would be much easier to understand in the context of a problem. A feature is a characteristic that might help when solving the problem.[3]",The features in your data are important to the predictive models you use and will influence the results you are going to achieve. The quality and quantity of the features will have great influence on whether the model is good or not.[4],"You could say the better the features are, the better the result is. This isn't entirely true, because the results achieved also depend on the model and the data, not just the chosen features. That said, choosing the right features is still very important. Better features can produce simpler and more flexible models, and they often yield better results.[3]",The algorithms we used are very standard for Kagglers. […] We spent most of our efforts in feature engineering. [...] We were also very careful to discard features likely to expose us to the risk of over-fitting our model.,…some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.,"Depending on a feature it could be strongly relevant (has information that doesn't exist in any other feature), relevant, weakly relevant (some information that other features include) or irrelevant. It is important to create a lot of features. Even if some of them are irrelevant, you can't afford missing the rest. Afterwards, feature selection can be used in order to prevent overfitting.[9]","Feature explosion can be caused by feature combination or feature templates, both leading to a quick growth in the total number of features.","There are a few solutions to help stop feature explosion such as: regularisation, kernel method, feature selection.[10]"
"In machine learning, feature hashing, also known as the hashing trick[1] (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly, rather than looking the indices up in an associative array.",,,"In a typical document classification task, the input to the machine learning algorithm (both during learning and classification) is free text. From this, a bag of words (BOW) representation is constructed: the individual tokens are extracted and counted, and each distinct token in the training set defines a feature (independent variable) of each of the documents in both the training and test sets.","Machine learning algorithms, however, are typically defined in terms of numerical vectors. Therefore, the bags of words for a set of documents is regarded as a term-document matrix where each row is a single document, and each column is a single feature/word; the entry i, j in such a matrix captures the frequency (or weight) of the j'th term of the vocabulary in document i. (An alternative convention swaps the rows and columns of the matrix, but this difference is immaterial.) Typically, these vectors are extremely sparse—according to Zipf's law.","The common approach is to construct, at learning time or prior to that, a dictionary representation of the vocabulary of the training set, and use that to map words to indices. Hash tables and tries are common candidates for dictionary implementation. E.g., the three documents","can be converted, using the dictionary",to the term-document matrix,"(Punctuation was removed, as is usual in document classification and clustering.)","The problem with this process is that such dictionaries take up a large amount of storage space and grow in size as the training set grows.[2] On the contrary, if the vocabulary is kept fixed and not increased with a growing training set, an adversary may try to invent new words or misspellings that are not in the stored vocabulary so as to circumvent a machine learned filter. This difficulty is why feature hashing has been tried for spam filtering at Yahoo! Research.[3]","Note that the hashing trick isn't limited to text classification and similar tasks at the document level, but can be applied to any problem that involves large (perhaps unbounded) numbers of features.","Instead of maintaining a dictionary, a feature vectorizer that uses the hashing trick can build a vector of a pre-defined length by applying a hash function h to the features (e.g., words) in the items under consideration, then using the hash values directly as feature indices and updating the resulting vector at those indices:","It has been suggested that a second, single-bit output hash function ξ be used to determine the sign of the update value, to counter the effect of hash collisions.[1] If such a hash function is used, the algorithm becomes","The above pseudocode actually converts each sample into a vector. An optimized version would instead only generate a stream of (h,ξ) pairs and let the learning and prediction algorithms consume such streams; a linear model can then be implemented as a single hash table representing the coefficient vector.","When a second hash function ξ is used to determine the sign of a feature's value, the expected mean of each column in the output array becomes zero because ξ causes some collisions to cancel out.[1] E.g., suppose an input contains two symbolic features f₁ and f₂ that collide with each other, but not with any other features in the same input; then there are four possibilities which, if we make no assumptions about ξ, have equal probability, as listed in the table on the right.","In this example, there is a 50% probability that the hash collision cancels out. Multiple hash functions can be used to further reduce the risk of collisions.[4]","Furthermore, if φ is the transformation implemented by a hashing trick with a sign hash ξ (i.e. φ(x) is the feature vector produced for a sample x), then inner products in the hashed space are unbiased:","where the expectation is taken over the hashing function φ.[1] It can be verified that



⟨
φ
(
x
)
,
φ
(

x
′

)
⟩


{\displaystyle \langle \varphi (x),\varphi (x')\rangle }

 is a positive semi-definite kernel.[1][5]","Recent work extends the hashing trick to supervised mappings from words to indices,[6] which are explicitly learned to avoid collisions of important terms.","Ganchev and Dredze showed that in text classification applications with random hash functions and several tens of thousands of columns in the output vectors, feature hashing need not have an adverse effect on classification performance, even without the signed hash function.[2] Weinberger et al. applied their variant of hashing to the problem of spam filtering, formulating this as a multi-task learning problem where the input features are pairs (user, feature) so that a single parameter vector captured per-user spam filters as well as a global filter for several hundred thousand users, and found that the accuracy of the filter went up.[1]",Implementations of the hashing trick are present in:
"In machine learning, feature learning or representation learning[1] is a set of techniques that learn a feature: a transformation of raw data input to a representation that can be effectively exploited in machine learning tasks. This obviates manual feature engineering, which is otherwise necessary, and allows a machine to both learn at a specific task (using the features) and learn the features themselves: to learn how to learn.","Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor measurement is usually complex, redundant, and highly variable. Thus, it is necessary to discover useful features or representations from raw data. Traditional hand-crafted features often require expensive human labor and often rely on expert knowledge. Also, they normally do not generalize well. This motivates the design of efficient feature learning techniques, to automate and generalize this.","Feature learning can be divided into two categories: supervised and unsupervised feature learning, analogous to these categories in machine learning generally.",,,Supervised feature learning is to learn features from labeled data. Several approaches are introduced in the following.,"Dictionary learning is to learn a set (dictionary) of representative elements from the input data such that each data point can be represented as a weighted sum of the representative elements. The dictionary elements and the weights may be found by minimizing the average representation error (over the input data), together with L1 regularization on the weights to enable sparsity (i.e., the representation of each data point has only a few nonzero weights).","Supervised dictionary learning exploits both the structure underlying the input data and the labels for optimizing the dictionary elements. For example, a supervised dictionary learning technique was proposed by Mairal et al. in 2009.[6] The authors apply dictionary learning on classification problems by jointly optimizing the dictionary elements, weights for representing data points, and parameters of the classifier based on the input data. In particular, a minimization problem is formulated, where the objective function consists of the classification error, the representation error, an L1 regularization on the representing weights for each data point (to enable sparse representation of data), and an L2 regularization on the parameters of the classifier.","Neural networks are used to illustrate a family of learning algorithms via a ""network"" consisting of multiple layers of inter-connected nodes. It is inspired by the nervous system, where the nodes are viewed as neurons and edges are viewed as synapse. Each edge has an associated weight, and the network defines computational rules that passes input data from the input layer to the output layer. A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights. With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights).","Multilayer neural networks can be used to perform feature learning, since they learn a representation of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer.","Unsupervised feature learning is to learn features from unlabeled data. The goal of unsupervised feature learning is often to discover low-dimensional features that captures some structure underlying the high-dimensional input data. When the feature learning is performed in an unsupervised way, it enables a form of semisupervised learning where first, features are learned from an unlabeled dataset, which are then employed to improve performance in a supervised setting with labeled data.[7][8] Several approaches are introduced in the following.","K-means clustering is an approach for vector quantization. In particular, given a set of n vectors, k-means clustering groups them into k clusters (i.e., subsets) in such a way that each vector belongs to the cluster with the closest mean. The problem is computationally NP-hard, and suboptimal greedy algorithms have been developed for k-means clustering.","In feature learning, k-means clustering can be used to group an unlabeled set of inputs into k clusters, and then use the centroids of these clusters to produce features. These features can be produced in several ways. The simplest way is to add k binary features to each sample, where each feature j has value one iff the jth centroid learned by k-means is the closest to the sample under consideration.[3] It is also possible to use the distances to the clusters as features, perhaps after transforming them through a radial basis function (a technique that has used to train RBF networks[9]). Coates and Ng note that certain variants of k-means behave similarly to sparse coding algorithms.[10]","In a comparative evaluation of unsupervised feature learning methods, Coates, Lee and Ng found that k-means clustering with an appropriate transformation outperforms the more recently invented auto-encoders and RBMs on an image classification task.[3] K-means has also been shown to improve performance in the domain of NLP, specifically for named-entity recognition;[11] there, it competes with Brown clustering, as well as with distributed word representations (also known as neural word embeddings).[8]","Principal component analysis (PCA) is often used for dimension reduction. Given an unlabeled set of n input data vectors, PCA generates p (which is much smaller than the dimension of the input data) right singular vectors corresponding to the p largest singular values of the data matrix, where the kth row of the data matrix is the kth input data vector shifted by the sample mean of the input (i.e., subtracting the sample mean from the data vector). Equivalently, these singular vectors are the eigenvectors corresponding to the p largest eigenvalues of the sample covariance matrix of the input vectors. These p singular vectors are the feature vectors learned from the input data, and they represent directions along which the data has the largest variations.","PCA is a linear feature learning approach since the p singular vectors are linear functions of the data matrix. The singular vectors can be generated via a simple algorithm with p iterations. In the ith iteration, the projection of the data matrix on the (i-1)th eigenvector is subtracted, and the ith singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix.","PCA has several limitations. First, it assumes that the directions with large variance are of most interest, which may not be the case in many applications. PCA only relies on orthogonal transformations of the original data, and it only exploits the first- and second-order moments of the data, which may not well characterize the distribution of the data. Furthermore, PCA can effectively reduce dimension only when the input data vectors are correlated (which results in a few dominant eigenvalues).",Local linear embedding (LLE) is a nonlinear unsupervised learning approach for generating low-dimensional neighbor-preserving representations from (unlabeled) high-dimension input. The approach was proposed by Sam T. Roweis and Lawrence K. Saul in 2000.[12][13],"The general idea of LLE is to reconstruct the original high-dimensional data using lower-dimensional points while maintaining some geometric properties of the neighborhoods in the original data set. LLE consists of two major steps. The first step is for ""neighbor-preserving,"" where each input data point Xi is reconstructed as a weighted sum of K nearest neighboring data points, and the optimal weights are found by minimizing the average squared reconstruction error (i.e., difference between a point and its reconstruction) under the constraint that the weights associated to each point sum up to one. The second step is for ""dimension reduction,"" by looking for vectors in a lower-dimensional space that minimizes the representation error using the optimized weights in the first step. Note that in the first step, the weights are optimized with data being fixed, which can be solved as a least squares problem; while in the second step, lower-dimensional points are optimized with the weights being fixed, which can be solved via sparse eigenvalue decomposition.","The reconstruction weights obtained in the first step captures the ""intrinsic geometric properties"" of a neighborhood in the input data.[13] It is assumed that original data lie on a smooth lower-dimensional manifold, and the ""intrinsic geometric properties"" captured by the weights of the original data are expected also on the manifold. This is why the same weights are used in the second step of LLE. Compared with PCA, LLE is more powerful in exploiting the underlying structure of data.",Independent component analysis (ICA) is a technique for learning a representation of data using a weighted sum of independent non-Gaussian components.[14] The assumption of non-Gaussian is imposed since the weights cannot be uniquely determined when all the components follow Gaussian distribution.,"Different from supervised dictionary learning, unsupervised dictionary learning does not utilize the labels of the data and only exploits the structure underlying the data for optimizing the dictionary elements. An example of unsupervised dictionary learning is sparse coding, which aims to learn basis functions (dictionary elements) for data representation from unlabeled input data. Sparse coding can be applied to learn overcomplete dictionary, where the number of dictionary elements is larger than the dimension of the input data.[15] Aharon et al. proposed an algorithm known as K-SVD for learning from unlabeled input data a dictionary of elements that enables sparse representation of the data.[16]","The hierarchical architecture of the neural system inspires deep learning architectures for feature learning by stacking multiple layers of simple learning blocks.[17] These architectures are often designed based on the assumption of distributed representation: observed data is generated by the interactions of many different factors on multiple levels. In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data. Each level uses the representation produced by previous level as input, and produces new representations as output, which is then fed to higher levels. The input of bottom layer is the raw data, and the output of the final layer is the final low-dimensional feature or representation.","Restricted Boltzmann machines (RBMs) are often used as a building block for multilayer learning architectures.[3][18] An RBM can be represented by an undirected bipartite graph consisting of a group of binary hidden variables, a group of visible variables, and edges connecting the hidden and visible nodes. It is a special case of the more general Boltzmann machines with the constraint of no intra-node connections. Each edge in an RBM is associated with a weight. The weights together with the connections define an energy function, based on which a joint distribution of visible and hidden nodes can be devised. Based on the topology of the RBM, the hidden (visible) variables are independent conditioned on the visible (hidden) variables. Such conditional independence facilitates computations on RBM.","An RBM can be viewed as a single layer architecture for unsupervised feature learning. In particular, the visible variables correspond to input data, and the hidden variables correspond to feature detectors. The weights can be trained by maximizing the probability of visible variables using the contrastive divergence (CD) algorithm by Geoffrey Hinton.[18]","In general, the training of RBM by solving the above maximization problem tends to result in non-sparse representations. The sparse RBM, [19] a modification of the RBM, was proposed to enable sparse representations. The idea is to add a regularization term in the objective function of data likelihood, which penalizes the deviation of the expected hidden variables from a small constant 



p


{\displaystyle p}

.","An autoencoder consisting of encoder and decoder is a paradigm for deep learning architectures. An example is provided by Hinton and Salakhutdinov[18] where the encoder uses raw data (e.g., image) as input and produces feature or representation as output, and the decoder uses the extracted feature from the encoder as input and reconstructs the original input raw data as output. The encoder and decoder are constructed by stacking multiple layers of RBMs. The parameters involved in the architecture were originally trained in a greedy layer-by-layer manner: after one layer of feature detectors is learned, they are fed to upper layers as visible variables for training the corresponding RBM. Current approaches typically apply end-to-end training with stochastic gradient descent methods. Training can be repeated until some stopping criteria is satisfied."
"Feature scaling is a method used to standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.",,,"Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization[citation needed]. For example, the majority of classifiers calculate the distance between two points by the Euclidean distance[citation needed]. If one of the features has a broad range of values, the distance will be governed by this particular feature[citation needed]. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance[citation needed].",Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it[citation needed].,"The simplest method is rescaling the range of features to scale the range in [0, 1] or [−1, 1]. Selecting the target range depends on the nature of the data. The general formula is given as:","where 



x


{\displaystyle x}

 is an original value, 




x
′



{\displaystyle x'}

 is the normalized value. For example, suppose that we have the students' weight data, and the students' weights span [160 pounds, 200 pounds]. To rescale this data, we first subtract 160 from each student's weight and divide the result by 40 (the difference between the maximum and minimum weights).","In machine learning, we can handle various types of data, e.g. audio signals and pixel values for image data, and this data can include multiple dimensions. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the enumerator) and unit-variance. This method is widely used for normalization in many machine learning algorithms (e.g., support vector machines, logistic regression, and neural networks)[1][citation needed]. This is typically done by calculating standard scores.[2] The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation.","Where 



x


{\displaystyle x}

 is the original feature vector, 






x
¯





{\displaystyle {\bar {x}}}

 is the mean of that feature vector, and 



σ


{\displaystyle \sigma }

 is its standard deviation.","Another option that is widely used in machine-learning is to scale the components of a feature vector such that the complete vector has length one. This usually means dividing each component by the Euclidean length of the vector. In some applications (e.g. Histogram features) it can be more practical to use the L1 norm (i.e. Manhattan Distance, City-Block Length or Taxicab Geometry) of the feature vector:",This is especially important if in the following learning steps the Scalar Metric is used as a distance measure.,"In stochastic gradient descent, feature scaling can sometimes improve the convergence speed of the algorithm[1][citation needed]. In support vector machines,[3] it can reduce the time to find support vectors. Note that feature scaling changes the SVM result[citation needed]."
"In pattern recognition and machine learning, a feature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis. When representing images, the feature values might correspond to the pixels of an image, when representing texts perhaps term occurrence frequencies. Feature vectors are equivalent to the vectors of explanatory variables used in statistical procedures such as linear regression. Feature vectors are often combined with weights using a dot product in order to construct a linear predictor function that is used to determine a score for making a prediction.","The vector space associated with these vectors is often called the feature space. In order to reduce the dimensionality of the feature space, a number of dimensionality reduction techniques can be employed.","Higher-level features can be obtained from already available features and added to the feature vector, for example for the study of diseases the feature 'Age' is useful and is defined as Age = 'Year of death' - 'Year of birth' . This process is referred to as feature construction.[1][2] Feature construction is the application of a set of constructive operators to a set of existing features resulting in construction of new features. Examples of such constructive operators include checking for the equality conditions {=, ≠}, the arithmetic operators {+,−,×, /}, the array operators {max(S), min(S), average(S)} as well as other more sophisticated operators, for example count(S,C)[3] that counts the number of features in the feature vector S satisfying some condition C or, for example, distances to other recognition classes generalized by some accepting device. Feature construction has long been considered a powerful tool for increasing both accuracy and understanding of structure, particularly in high-dimensional problems.[4] Applications include studies of disease and emotion recognition from speech.[5]",
"In information science, formal concept analysis (FCA) is a principled way of deriving a concept hierarchy or formal ontology from a collection of objects and their properties. Each concept in the hierarchy represents the set of objects sharing the same values for a certain set of properties; and each sub-concept in the hierarchy contains a subset of the objects in the concepts above it. The term was introduced by Rudolf Wille in 1984, and builds on applied lattice and order theory that was developed by Garrett Birkhoff and others in the 1930s.","Formal concept analysis finds practical application in fields including data mining, text mining, machine learning, knowledge management, semantic web, software development, chemistry and biology.",,,"The original motivation of formal concept analysis was the concrete representation of complete lattices and their properties by means of formal contexts, data tables that represent binary relations between objects and attributes, thus tabulating pairs of the form ""object g has attribute m."" In this theory, a formal concept is defined to be a pair (A, B), where A is a set of objects (called the extent) and B is a set of attributes (the intent) such that","In this way, formal concept analysis formalizes the semantic notions of extension and intension.","The formal concepts of any formal context can—as explained below—be ordered in a hierarchy called more formally the context's ""concept lattice."" In French this is called a trellis de Galois (Galois lattice) because of the relation between the sets of objects and attributes is a Galois connection.","The theory in its present form goes back to the early 1980s and a research group led by Rudolf Wille, Bernhard Ganter and Peter Burmeister at the Technische Universität Darmstadt. Its mathematical basis, however, was already created in the 1930s by Garrett Birkhoff as part of general lattice theory. Other previous approaches to the same idea arose from various French research groups, but the Darmstadt group normalised and popularised the field in computer science research circles. Philosophical foundations of formal concept analysis refer in particular to Charles S. Peirce and the educationalist Hartmut von Hentig.","In his article Restructuring Lattice Theory (1982) initiating formal concept analysis as a mathematical discipline, Wille starts from a discontent with the current lattice theory and pure mathematics in general: The production of theoretical results - often achieved by ""elaborate mental gymnastics"" - were impressive, but the connections between neighbouring domains, even parts of a theory were getting weaker.","Restructuring lattice theory is an attempt to reinvigorate connections with our general culture by interpreting the theory as concretely as possible, and in this way to promote better communication between lattice theorists and potential users of lattice theory.[1]","This aim traces back to Hartmut von Hentig, who in 1972 pleaded for restructuring sciences in view of better teaching and in order to make sciences mutually available and more generally (i.e. also without specialized knowledge) criticable.[2] Hence, by its origins formal concept analysis aims at interdisciplinarity and democratic control of research.[3]","It corrects the starting point of lattice theory during the development of formal logic in the 19th century. Then - and later in model theory - a concept as unary predicate had been reduced to its extent. Now again, the philosophy of concepts should become less abstract by considering the intent. Hence, formal concept analysis is oriented towards the categories extension and intension of linguistics and classical conceptual logic.[4]","Formal Concept Analysis aims at the clarity of concepts according to Charles S. Peirce's pragmatic maxim by unfolding observable, elementary properties of the subsumed objects.[3] In his late philosophy, Peirce assumed that logical thinking aims at perceiving reality, by the triade concept, judgement and conclusion. Mathematics is an abstraction of logic, develops patterns of possible realities and therefore may support rational communication. On this background, Wille defines:",The aim and meaning of Formal Concept Analysis as mathematical theory of concepts and concept hierarchies is to support the rational communication of humans by mathematically developing appropriate conceptual structures which can be logically activated.[5],"A formal context is a triple K = (G, M, I), where G is a set of objects, M is a set of attributes, and I ⊆ G × M is a binary relation called incidence that expresses which objects have which attributes.[6] The incidence relation can be regarded as a bipartite graph (or a partial order of height 2). Predicate gIm designates object g's having attribute m. For a subset A ⊆ G of objects and a subset B ⊆ M of attributes, one defines two derivation operators as follows:","A' = {m ∈ M | ∀ g ∈ A, gIm}, and dually","B' = {g ∈ G | ∀ m ∈ B, gIm}.","Applying either derivation operator and then the other constitutes another operator, ″, with three properties (illustrated here for attributes):","Any operator satisfying those three properties is called a closure operator, and any set A such that A″ = A for a closure operator ″ is called closed under ″.","With these derivation operators, it is possible to restate the definition of the term ""formal concept"" more rigorously: a pair (A,B) is a formal concept of a context (G, M, I) provided that:","Equivalently and more intuitively, (A,B) is a formal concept precisely when:","For a set of objects A, the set A′ of their common attributes comprises the similarity characterizing the objects in A, while the closed set A″ is the cluster of objects—within A or beyond—that have every attribute that is common to all the objects in A. [7]","A formal context may be represented as a matrix K in which the rows correspond to the objects, the columns correspond to the attributes, and each entry ki,j is the boolean value of the expression ""Object i has attribute j."" In this matrix representation, each formal concept corresponds to a maximal submatrix (not necessarily contiguous) all of whose elements equal TRUE. For instance, the cells in the table below that are highlighted with a different background color constitute a 3 × 2 submatrix in which every entry is checked. And because no other column contains a check in all three of the highlighted rows and no other row contains a check in both of the highlighted columns, the highlighted submatrix is a formal concept of this context; it amounts to the concept of odd prime numbers.[8]","Consider G = {1,2,3,4,5,6,7,8,9,10}, and M = {composite, even, odd, prime, square}. The smallest concept including number 3 is the one with objects {3,5,7} and attributes {odd, prime}, for 3 has both of those attributes and {3,5,7} is the set of objects having that set of attributes. The largest concept involving the attribute of being square is the one with objects {1,4,9} and attributes {square}, for 1, 4 and 9 are all the square numbers and all three of them have that set of attributes. It can readily be seen that both of these example concepts satisfy the formal definitions above.","The concepts (Gi, Mi) of a context K can be partially ordered by inclusion. We define that partial order ≤ on the concepts as follows: for any concepts (Gi, Mi) and (Gj, Mj) of K, we say that (Gi, Mi) ≤ (Gj, Mj) precisely when Gi ⊆ Gj. Equivalently, (Gi, Mi) ≤ (Gj, Mj) whenever Mi ⊇ Mj.","In this partial order, every pair (Gi, Mi) and (Gj, Mj) of concepts has a greatest lower bound, or meet: the concept with objects Gi ∩ Gj. Its attributes are Mi ∪ Mj along with any additional attributes common to all the objects in Gi ∩ Gj. Dually, every (Gi, Mi) and (Gj, Mj) also have a least upper bound (join); its attributes are Mi ∩ Mj, and its objects are Gi ∪ Gj along with any additional objects that have all the attributes in Mi ∩ Mj.","These meet and join operations satisfy the axioms defining a lattice. In fact, by considering infinite meets and joins, analogously to the binary meets and joins defined above, one sees that this is a complete lattice. It may be viewed as the Dedekind–MacNeille completion of a partially ordered set of height two in which the elements of the partial order are the objects of G and the attributes of M and in which two elements x and y satisfy x ≤ y exactly when x is an object that has attribute y.","Any finite lattice may be generated as the concept lattice for some context. Let L be a finite lattice and form a context in which the objects and the attributes both correspond to elements of L. In this context, let object x have attribute y exactly when x and y are ordered as x ≤ y in the lattice. Then the concept lattice of this context is isomorphic to L itself.[9] This construction may be interpreted as forming the Dedekind–MacNeille completion of L, which is known to produce an isomorphic lattice from any finite lattice.","The full set of concepts for objects and attributes from the above example is shown in the illustration. It includes a concept for each of the original attributes: composite, square, even, odd and prime. Additionally, it includes concepts for even composite numbers, composite square numbers (that is, all square numbers except 1), even composite squares, odd squares, odd composite squares, even primes, and odd primes.","row-clarified: the context does not have duplicate rows (



A
(
o
)
=

o
′



{\displaystyle A(o)=o'}

 is injective)","column-clarified: the context does not have duplicate columns (



O
(
a
)
=

a
′



{\displaystyle O(a)=a'}

 is injective)",clarified: row-clarified and column-clarified,row-reduced: no context rows can be expressed as intersection of other rows. The lattice of such context is meet-reduced.,column-reduced: no context columns can be expressed as intersection of other columns. The lattice of such context is join-reduced.,reduced: row-reduced and column-reduced,"A reducible attribute gets added in a node of the lattice, which would be there also without this attribute.","Modelling negation in a formal context is somewhat problematic because the complement (G\Gi, M\Mi) of a concept (Gi, Mi) is in general not a concept. However, since the concept lattice is complete one can consider the join (Gi, Mi)Δ of all concepts (Gj, Mj) that satisfy Gj ⊆ G\Gi; or dually the meet (Gi, Mi)𝛁 of all concepts satisfying Mj ⊆ M\Mi. These two operations are known as weak negation and weak opposition, respectively.","This can be expressed in terms of the derivative functions. The derivative of a set Gi ⊆ G of objects is the set Gi' ⊆ M of all attributes that hold for all objects in Gi. The derivative of a set Mi ⊆ M of attributes is the set Mi' ⊆ G of all objects that have all attributes in Mi. A pair (Gi, Mi) is a concept if and only if Gi' = Mi and Mi' = Gi. Using this function, weak negation can be written as",and weak opposition can be written as,The concept lattice equipped with the two additional operations Δ and 𝛁 is known as the concept algebra of a context. Concept algebras are a generalization of power sets.,"Weak negation on a concept lattice L is a weak complementation, i.e. an order-reversing map Δ: L → L which satisfies the axioms xΔΔ ≤ x and (x⋀y) ⋁ (x⋀yΔ) = x. Weak composition is a dual weak complementation. A (bounded) lattice such as a concept algebra, which is equipped with a weak complementation and a dual weak complementation, is called a weakly dicomplemented lattice. Weakly dicomplemented lattices generalize distributive orthocomplemented lattices, i.e. Boolean algebras.[10][11]","The line diagram of the concept lattice encodes enough information to recover the original context from which it was formed. Each object of the context corresponds to a lattice element, the element with the minimal object set that contains that object, and with an attribute set consisting of all attributes of the object. Symmetrically, each attribute of the context corresponds to a lattice element, the one with the minimal attribute set containing that attribute, and with an object set consisting of all objects with that attribute. We may label the nodes of the line diagram with the objects and attributes they correspond to; with this labeling, object x has attribute y if and only if there exists a monotonic path from x to y in the diagram.[12]","In FCA implication A → B for subsets A, B of the set of attributes M (A,B ⊆ M ) holds if A′ ⊆ B′, i.e. every object possessing each attribute from A also has each attribute from B. Implications obey Armstrong rules:","






Y
⊆
X


X
→
Y



,



X
→
Y


X
Z
→
Y
Z



,



X
→
Y
,
Y
→
Z


X
→
Z





{\displaystyle {\frac {Y\subseteq X}{X\rightarrow Y}},{\frac {X\rightarrow Y}{XZ\rightarrow YZ}},{\frac {X\rightarrow Y,Y\rightarrow Z}{X\rightarrow Z}}}

","Kuznetsov and Obiedkov [13] survey many algorithms that have been developed for constructing concept lattices. These algorithms vary in many details, but are in general based on the idea that each edge of the line diagram of the concept lattice connects some concept C to the concept formed by the join of C with a single object. Thus, one can build up the concept lattice one concept at a time, by finding the neighbors in the line diagram of known concepts, starting from the concept with an empty set of objects. The amount of time spent to traverse the entire concept lattice in this way is polynomial in the number of input objects and attributes per generated concept.","Generation of the concept set presents two main problems: how to generate all concepts and how to avoid repetitive generation of the same concept or, at least, to determine whether a concept is generated for the first time. There are several ways to generate a new intent. Some algorithms (in particular, incremental ones) intersect a generated intent with some object intent. Other algorithms compute an intent explicitly intersecting all objects of the corresponding extent. There are algorithms that, starting from object intents, create new intents by intersecting already obtained intents. Lastly, one of the algorithms [14] does not use the intersection operation to generate intents. It forms new intents by adding attributes to those already generated and tests some condition on supports of attribute sets (a support of an attribute set is the number of objects whose intents contain all attributes from this set) to realize whether an attribute set is an intent.","The Close by One algorithm, for instance, generates concepts in the lexicographical order of their extents assuming that there is a linear order on the set of objects. At each step of the algorithm there is a current object. The generation of a concept is considered canonical if its extent contains no object preceding the current object. Close by One uses the described canonicity test, a method for selecting subsets of a set of objects G and an intermediate structure that helps to compute closures more efficiently using the generated concepts. Its time complexity is O(|G|2|M||L|), and its polynomial delay is O(|G|3|M|) where |G| stands for the cardinality of the set of objects G, |M|, similarly, is the number of all attributes from M and |L| is the size of the concept lattice.",The Chein Algorithm [15] represents the objects by extent–intent pairs and generates each new concept intent as the intersection of intents of two existent concepts. At every iteration step of the Chein algorithm a new layer of concepts is created by intersecting pairs of concept intents from the current layer and the new intent is searched for in the new layer. The time complexity of the modified version of Chein algorithm is O(|G|3|M||L|) while its polynomial delay is O(|G|3|M|).,The algorithm by Bordat [16] uses a tree for fast storing and retrieval of concepts. This algorithm uses a technique that requires O(|M|) time to realize whether a concept is generated for the first time without any search - the uniqueness of a concept is tested by intersecting its intent with the content of the cache. The time complexity of Bordat is O(|G||M|2|L|). This algorithm has a polynomial delay O(|G||M|3).,The algorithm proposed by Norris [17] is essentially an incremental version of the Close by One algorithm with time complexity - O(|G|2|M||L|).,The algorithm proposed by Godin [18] has the worst-case time complexity quadratic in the number of concepts. This algorithm is based on the use of an efficiently computable hash function f (which is actually the cardinality of an intent) defined on the set of concepts.,"The choice of an algorithm for construction of the concept lattice should be based on the properties of input data. According to the survey [13] , recommendations are as follows: the Godin algorithm should be used for small and sparse contexts. For dense contexts the algorithms based on the canonicity test, linear in the number of input objects, such as Close by One and Norris, should be used. Bordat performs well on contexts of average density, especially, when the diagram graph is to be constructed.","Fast Close by One (FCbO) [19] can be seen as an extended version of Close by One which involves improved canonicity test that significantly reduces the number of formal concepts computed multiple times. It also combines depth-first and breadth-first search and employs an additional test that can be performed before a new formal concept is computed eliminating thus some unnecessary computations. FCbO performs well in case of both sparse and dense data of reasonable size. From the point of view of the asymptotic worst-case complexity, FCbO has time delay O(|G|3|M|) [20] , and asymptotic time complexity O(|G|2|M||L|) because in the worst case FCbO can degenerate into the original Close by One.",In-Close [21] is another Close by One variant that introduces a 'partial-closure' canonicity test to further improve efficiency. In-Close has the same time complexity of FCbO. The latest version of In-Close also incorporates the breadth-and-depth approach of FCbO to produce a 'best of Close by One breed'.[22],"The AddIntent algorithm [23] produces not only the concept set, but also the diagram graph. Being incremental, it relies on the graph constructed from the first objects of the context to integrate the next object into the lattice. Therefore, its use is most appropriate in those applications that require both the concept set and diagram graph, for example, in applications related to information retrieval and document browsing. The best estimate for the algorithm’s upper bound complexity to construct a concept lattice L whose context has a set of objects G, each of which possesses at most max(|g′|) attributes, is O(|L||G|2max(|g′|)). The AddIntent algorithm outperformed a selection of other published algorithms in experimental comparison for most types of contexts and was close to the most efficient algorithm in other cases.","There are several types of biclusters (co-clusters) known in the literature: [24] biclusters of equal values, similar values, coherent values, the commonality of them being the existence of inclusion-maximal set of objects described by inclusion-maximal set of attributes with some special pattern of behavior. Clustering objects based on sets of attributes taking similar values dates back to the work of Hartigan [25] and was called biclustering by Mirkin. [26] Attention to biclustering approaches started to grow from the beginning of the 2000s with the growth of the need to analyze similarities in gene expression data [27] and design of recommender systems. [28]","Given an object-attribute numerical data-table (many-valued context in terms of FCA), the goal of biclustering is to group together some objects having similar values of some attributes. For example, in gene expression data, it is known that genes (objects) may share a common behavior for a subset of biological situations (attributes) only: one should accordingly produce local patterns to characterize biological processes, the latter should possibly overlap, since a gene may be involved in several processes. The same remark applies for recommender systems where one is interested in local patterns characterizing groups of users that strongly share almost the same tastes for a subset of items. [28]","A bicluster in a binary object-attribute data-table is a pair (A,B) consisting of an inclusion-maximal set of objects A and an inclusion-maximal set of attributes B such that almost all objects from A have almost all attributes from B and vice versa. Of course, formal concepts can be considered as ""rigid"" biclusters where all objects have all attributes and vice versa. Hence, it is not surprising that some bicluster definitions coming from practice [29] are just definitions of a formal concept. [30] A bicluster of similar values in a numerical object-attribute data-table is usually defined [31] [32] [33] as a pair consisting of an inclusion-maximal set of objects and an inclusion-maximal set of attributes having similar values for the objects. Such a pair can be represented as an inclusion-maximal rectangle in the numerical table, modulo rows and columns permutations. In [30] it was shown that biclusters of similar values correspond to triconcepts of a triadic context [34] where the third dimension is given by a scale that represents numerical attribute values by binary attributes. This fact can be generalized to n-dimensional case, where n-dimensional clusters of similar values in n-dimensional data are represented by n+1-dimensional concepts. This reduction allows one to use standard definitions and algorithms from multidimensional concept analysis [33] [35] for computing multidimensional clusters.","Many FCA software applications are available today. The main purpose of these tools varies from formal context creation to formal concept mining and generating the concepts lattice of a given formal context and the corresponding association rules. Most of these tools are academic and still under active development. One can find a non exhaustive list of FCA tools in the FCA software website. Most of these tools are open-source applications like ConExp, ToscanaJ, Lattice Miner,[36] Coron, FcaBedrock, etc."
"In probability and statistics, a generative model is a model for randomly generating observable data values, typically given some hidden parameters. It specifies a joint probability distribution over observation and label sequences. Generative models are used in machine learning for either modeling data directly (i.e., modeling observations drawn from a probability density function), or as an intermediate step to forming a conditional probability density function. A conditional distribution can be formed from a generative model through Bayes' rule.","Shannon (1948) gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with ""representing and speedily is an good""; which is not proper English but which will increasingly approximate it as the table is moved from word pairs to word triplets etc.","Generative models contrast with discriminative models, in that a generative model is a full probabilistic model of all variables, whereas a discriminative model provides a model only for the target variable(s) conditional on the observed variables. Thus a generative model can be used, for example, to simulate (i.e. generate) values of any variable in the model, whereas a discriminative model allows only sampling of the target variables conditional on the observed quantities. Despite the fact that discriminative models do not need to model the distribution of the observed variables, they cannot generally express more complex relationships between the observed and target variables. They don't necessarily perform better than generative models at classification and regression tasks. In modern applications the two classes are seen as complementary or as different views of the same procedure.[1]",Examples of generative models include:,"If the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to maximize the data likelihood is a common method. However, since most statistical models are only approximations to the true distribution, if the model's application is to infer about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand. In such cases, it can be more accurate to model the conditional density functions directly using a discriminative model (see above), although application-specific details will ultimately dictate which approach is most suitable in any particular case."
"This glossary of artificial intelligence terms is about artificial intelligence, its sub-disciplines, and related fields."
"Google DeepMind is a British artificial intelligence company founded in September 2010 as DeepMind Technologies. It was renamed when it was acquired by Google in 2014. The company has created a neural network that learns how to play video games in a fashion similar to that of humans,[4] as well as a Neural Turing Machine, or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain.[5] The company made headlines in 2016 after its AlphaGo program beat a human professional Go player for the first time.[6]",,,"In 2010 the start-up was founded by Demis Hassabis, Shane Legg and Mustafa Suleyman.[7][8] Hassabis and Legg first met at University College London's Gatsby Computational Neuroscience Unit.[9]","Since then major venture capital firms Horizons Ventures and Founders Fund have invested in the company,[10] as well as entrepreneurs Scott Banister[11] and Elon Musk.[12] Jaan Tallinn was an early investor and an advisor to the company.[13] In 2014, DeepMind received the ""Company of the Year"" award by Cambridge Computer Laboratory.[14]","On 26 January 2014, Google announced[15] that it had agreed to take over DeepMind Technologies. The sale reportedly took place after Facebook ended negotiations with DeepMind Technologies in 2013.[16] The company was then renamed Google DeepMind.[2]",Google acquired DeepMind in 2014 for $500 million[17][18][19][20][21][22],"One of DeepMind's conditions for Google was that they establish an AI Ethics board.[23] That AI ethics board remained one of the biggest mysteries in technology, with both companies refusing to reveal who sits on it.[24]","In October 2015, a computer Go program called AlphaGo, powered by DeepMind, beat the European Go champion Fan Hui, a 2 dan (out of 9 dan possible) professional, five to zero.[25] This is the first time an artificial intelligence (AI) defeated a professional player.[6] Previously, computers were only known to have played Go at ""amateur"" level.[25][26] Go is considered much more difficult for computers to win compared to other games like chess, due to the much larger number of possibilities, making it prohibitively difficult for traditional AI methods such as brute-force.[25][26] The announcement of the news was delayed until 27 January 2016 to coincide with the publication of a paper in the journal Nature describing the algorithms used.[25] In March 2016 it beat Lee Sedol—a 9th dan Go player and one of the highest ranked players in the world—with 4-1 in a five-game match.","DeepMind Technologies's goal is to ""solve intelligence"",[27] which they are trying to achieve by combining ""the best techniques from machine learning and systems neuroscience to build powerful general-purpose learning algorithms"".[27] They are trying to formalize intelligence[28] in order to not only implement it into machines, but also understand the human brain, as Demis Hassabis explains:",[...] attempting to distil intelligence into an algorithmic construct may prove to be the best path to understanding some of the enduring mysteries of our minds.[29],"Currently the company's focus is on publishing research on computer systems that are able to play games, and developing these systems, ranging from strategy games such as Go[30] to arcade games. According to Shane Legg human-level machine intelligence can be achieved ""when a machine can learn to play a really wide range of games from perceptual stream input and output, and transfer understanding across games[...].""[31] Research describing an AI playing seven different Atari video games (Pong, Breakout, Space Invaders, Seaquest, Beamrider, Enduro, and Q*bert) reportedly led to their acquisition by Google.[4] Hassabis has mentioned the popular e-sport game StarCraft as a possible future challenge, since it requires a high level of strategic thinking and handling imperfect information.[32]","As opposed to other AIs, such as IBM's Deep Blue or Watson, which were developed for a pre-defined purpose and only function within its scope, DeepMind claims that their system is not pre-programmed: it learns from experience, using only raw pixels as data input. Technically it uses deep learning on a convolutional neural network, with a novel form of Q-learning, a form of model-free reinforcement learning.[2][33] They test the system on video games, notably early arcade games, such as Space Invaders or Breakout.[33][34] Without altering the code, the AI begins to understand how to play the game, and after some time plays, for a few games (most notably Breakout), a more efficient game than any human ever could.[34] For most games though (Space Invaders, Ms Pacman, Q*Bert for example), DeepMind plays well below the current World Record. The application of DeepMind's AI to video games is currently for games made in the 1970s and 1980s, with work being done on more complex 3D games such as Doom, which first appeared in the early 1990s.[34]","In July 2016, a collaboration between DeepMind and Moorfields Eye Hospital was announced.[35] DeepMind would be applied to the analysis of anonymized eye scans, searching for early signs of diseases leading to blindness.","In April 2016 New Scientist obtained the secretive data-sharing agreement between DeepMind and the Royal Free London NHS Foundation Trust, which operates the three London hospitals, where an estimated 1.6 million patients are treated annually. The revelation has exposed the ease with which private companies can obtain highly sensitive medical information without patient consent. The agreement shows DeepMind Health is gaining access to all admissions, discharge and transfer data, accident and emergency, pathology and radiology, and critical care at these hospitals. This included highly personal details such as whether patients had been diagnosed with HIV, suffered from depression or had ever undergone an abortion.[36][37] This led to some public outcry and officials from Google have yet to make a statement but many regard this move as controversial and question the legality of the acquisition generally.[24]","The concerns were widely reported and have led to a complaint to the Information Commissioner's Office (ICO), arguing that the data should be pseudonymised and encrypted.[38]","In May 2016, New Scientist published a further article claiming that the project had failed to secure approval from the Confidentiality Advisory Group of the Medicines and Healthcare Products Regulatory Agency.[39]"
"Grammar induction, also known as grammatical inference or syntactic pattern recognition, refers to the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.",,,"Grammatical inference has often been very focused on the problem of learning finite state machines of various types (see the article Induction of regular languages for details on these approaches), since there have been efficient algorithms for this problem since the 1980s.","More recently[when?] these approaches have been extended to the problem of inference of context-free grammars and richer formalisms, such as multiple context-free grammars and parallel multiple context-free grammars. Other classes of grammars for which grammatical inference has been studied are contextual grammars and pattern languages.","The simplest form of learning is where the learning algorithm merely receives a set of examples drawn from the language in question, but other learning models have been studied. One frequently studied alternative is the case where the learner can ask membership queries as in the exact query learning model or minimally adequate teacher model introduced by Angluin.","There is a wide variety of methods for grammatical inference. Two of the classic sources are Fu (1977) and Fu (1982). Duda, Hart & Stork (2001) also devote a brief section to the problem, and cite a number of references. The basic trial-and-error method they present is discussed below. For approaches to infer subclasses of regular languages in particular, see Induction of regular languages. A more recent textbook is de la Higuera (2010),[1] which covers the theory of grammatical inference of regular languages and finite state automata. D'Ulizia, Ferri and Grifoni [2] provide a survey that explores grammatical inference methods for natural languages.","The method proposed in Section 8.7 of Duda, Hart & Stork (2001) suggests successively guessing grammar rules (productions) and testing them against positive and negative observations. The rule set is expanded so as to be able to generate each positive example, but if a given rule set also generates a negative example, it must be discarded. This particular approach can be characterized as ""hypothesis testing"" and bears some similarity to Mitchel's version space algorithm. The Duda, Hart & Stork (2001) text provide a simple example which nicely illustrates the process, but the feasibility of such an unguided trial-and-error approach for more substantial problems is dubious.","Grammatical induction using evolutionary algorithms is the process of evolving a representation of the grammar of a target language through some evolutionary process. Formal grammars can easily be represented as tree structures of production rules that can be subjected to evolutionary operators. Algorithms of this sort stem from the genetic programming paradigm pioneered by John Koza.[citation needed] Other early work on simple formal languages used the binary string representation of genetic algorithms, but the inherently hierarchical structure of grammars couched in the EBNF language made trees a more flexible approach.","Koza represented Lisp programs as trees. He was able to find analogues to the genetic operators within the standard set of tree operators. For example, swapping sub-trees is equivalent to the corresponding process of genetic crossover, where sub-strings of a genetic code are transplanted into an individual of the next generation. Fitness is measured by scoring the output from the functions of the Lisp code. Similar analogues between the tree structured lisp representation and the representation of grammars as trees, made the application of genetic programming techniques possible for grammar induction.","In the case of grammar induction, the transplantation of sub-trees corresponds to the swapping of production rules that enable the parsing of phrases from some language. The fitness operator for the grammar is based upon some measure of how well it performed in parsing some group of sentences from the target language. In a tree representation of a grammar, a terminal symbol of a production rule corresponds to a leaf node of the tree. Its parent nodes corresponds to a non-terminal symbol (e.g. a noun phrase or a verb phrase) in the rule set. Ultimately, the root node might correspond to a sentence non-terminal.","Like all greedy algorithms, greedy grammar inference algorithms make, in iterative manner, decisions that seem to be the best at that stage. The decisions made usually deal with things like the creation of new rules, the removal of existing rules, the choice of a rule to be applied or the merging of some existing rules. Because there are several ways to define 'the stage' and 'the best', there are also several greedy grammar inference algorithms.",These context-free grammar generating algorithms make the decision after every read symbol:,These context-free grammar generating algorithms first read the whole given symbol-sequence and then start to make decisions:,A more recent approach is based on distributional learning. Algorithms using these approaches have been applied to learning context-free grammars and mildly context-sensitive languages and have been proven to be correct and efficient for large subclasses of these grammars.[3],"Angluin defines a pattern to be ""a string of constant symbols from Σ and variable symbols from a disjoint set"". The language of such a pattern is the set of all its nonempty ground instances i.e. all strings resulting from consistent replacement of its variable symbols by nonempty strings of constant symbols.[note 1] A pattern is called descriptive for a finite input set of strings if its language is minimal (with respect to set inclusion) among all pattern languages subsuming the input set.","Angluin gives a polynomial algorithm to compute, for a given input string set, all descriptive patterns in one variable x.[note 2] To this end, she builds an automaton representing all possibly relevant patterns; using sophisticated arguments about word lengths, which rely on x being the only variable, the state count can be drastically reduced.[4]","Erlebach et al. give a more efficient version of Angluin's pattern learning algorithm, as well as a parallelized version.[5]",Arimura et al. show that a language class obtained from limited unions of patterns can be learned in polynomial time.[6],"Pattern theory, formulated by Ulf Grenander,[7] is a mathematical formalism to describe knowledge of the world as patterns. It differs from other approaches to artificial intelligence in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language.","In addition to the new algebraic vocabulary, its statistical approach was novel in its aim to:","Broad in its mathematical coverage, pattern theory spans algebra and statistics, as well as local topological and global entropic properties.","The principle of grammar induction has been applied to other aspects of natural language processing, and has been applied (among many other problems) to morpheme analysis, and place name derivations. Grammar induction has also been used for lossless data compression and statistical inference via minimum message length (MML) and minimum description length (MDL) principles."
"Granular computing (GrC) is an emerging computing paradigm of information processing. It concerns the processing of complex information entities called information granules, which arise in the process of data abstraction and derivation of knowledge from information or data. Generally speaking, information granules are collections of entities that usually originate at the numeric level and are arranged together due to their similarity, functional or physical adjacency, indistinguishability, coherency, or the like.","At present, granular computing is more a theoretical perspective than a coherent set of methods or principles. As a theoretical perspective, it encourages an approach to data that recognizes and exploits the knowledge present in data at various levels of resolution or scales. In this sense, it encompasses all methods which provide flexibility and adaptability in the resolution at which knowledge or information is extracted and represented.",,,"As mentioned above, granular computing is not an algorithm or process; there is not a particular method that is called ""granular computing"". It is rather an approach to looking at data that recognizes how different and interesting regularities in the data can appear at different levels of granularity, much as different features become salient in satellite images of greater or lesser resolution. On a low-resolution satellite image, for example, one might notice interesting cloud patterns representing cyclones or other large-scale weather phenomena, while in a higher-resolution image, one misses these large-scale atmospheric phenomena but instead notices smaller-scale phenomena, such as the interesting pattern that is the streets of Manhattan. The same is generally true of all data: At different resolutions or granularities, different features and relationships emerge. The aim of granular computing is ultimately simply to try to take advantage of this fact in designing more-effective machine-learning and reasoning systems.","There are several types of granularity that are often encountered in data mining and machine learning, and we review them below:","One type of granulation is the quantization of variables. It is very common that in data mining or machine-learning applications that the resolution of variables needs to be decreased in order to extract meaningful regularities. An example of this would be a variable such as ""outside temperature"" (



t
e
m
p


{\displaystyle temp}

), which in a given application might be recorded to several decimal places of precision (depending on the sensing apparatus). However, for purposes of extracting relationships between ""outside temperature"" and, say, ""number of health-club applications"" (



c
l
u
b


{\displaystyle club}

), it will generally be advantageous to quantize ""outside temperature"" into a smaller number of intervals.",There are several interrelated reasons for granulating variables in this fashion:,"For example, a simple learner or pattern recognition system may seek to extract regularities satisfying a conditional probability threshold such as 



p
(
Y
=

y

j



|

X
=

x

i


)
≥
α


{\displaystyle p(Y=y_{j}|X=x_{i})\geq \alpha }

. In the special case where 



α
=
1


{\displaystyle \alpha =1}

, this recognition system is essentially detecting logical implication of the form 



X
=

x

i


→
Y
=

y

j




{\displaystyle X=x_{i}\rightarrow Y=y_{j}}

 or, in words, ""if 



X
=

x

i




{\displaystyle X=x_{i}}

, then 



Y
=

y

j




{\displaystyle Y=y_{j}}

"". The system's ability to recognize such implications (or, in general, conditional probabilities exceeding threshold) is partially contingent on the resolution with which the system analyzes the variables.","As an example of this last point, consider the feature space shown to the right. The variables may each be regarded at two different resolutions. Variable 



X


{\displaystyle X}

 may be regarded at a high (quaternary) resolution wherein it takes on the four values 



{

x

1


,

x

2


,

x

3


,

x

4


}


{\displaystyle \{x_{1},x_{2},x_{3},x_{4}\}}

 or at a lower (binary) resolution wherein it takes on the two values 



{

X

1


,

X

2


}


{\displaystyle \{X_{1},X_{2}\}}

. Similarly, variable 



Y


{\displaystyle Y}

 may be regarded at a high (quaternary) resolution or at a lower (binary) resolution, where it takes on the values 



{

y

1


,

y

2


,

y

3


,

y

4


}


{\displaystyle \{y_{1},y_{2},y_{3},y_{4}\}}

 or 



{

Y

1


,

Y

2


}


{\displaystyle \{Y_{1},Y_{2}\}}

, respectively. It will be noted that at the high resolution, there are no detectable implications of the form 



X
=

x

i


→
Y
=

y

j




{\displaystyle X=x_{i}\rightarrow Y=y_{j}}

, since every 




x

i




{\displaystyle x_{i}}

 is associated with more than one 




y

j




{\displaystyle y_{j}}

, and thus, for all 




x

i




{\displaystyle x_{i}}

, 



p
(
Y
=

y

j



|

X
=

x

i


)
<
1


{\displaystyle p(Y=y_{j}|X=x_{i})<1}

. However, at the low (binary) variable resolution, two bilateral implications become detectable: 



X
=

X

1


↔
Y
=

Y

1




{\displaystyle X=X_{1}\leftrightarrow Y=Y_{1}}

 and 



X
=

X

2


↔
Y
=

Y

2




{\displaystyle X=X_{2}\leftrightarrow Y=Y_{2}}

, since every 




X

1




{\displaystyle X_{1}}

 occurs iff 




Y

1




{\displaystyle Y_{1}}

 and 




X

2




{\displaystyle X_{2}}

 occurs iff 




Y

2




{\displaystyle Y_{2}}

. Thus, a pattern recognition system scanning for implications of this kind would find them at the binary variable resolution, but would fail to find them at the higher quaternary variable resolution.","It is not feasible to exhaustively test all possible discretization resolutions on all variables in order to see which combination of resolutions yields interesting or significant results. Instead, the feature space must be preprocessed (often by an entropy analysis of some kind) so that some guidance can be given as to how the discretization process should proceed. Moreover, one cannot generally achieve good results by naively analyzing and discretizing each variable independently, since this may obliterate the very interactions that we had hoped to discover.","A sample of papers that address the problem of variable discretization in general, and multiple-variable discretization in particular, is as follows: Chiu, Wong & Cheung (1991), Bay (2001), Liu et al. (2002), Wang & Liu (1998), Zighed, Rabaséda & Rakotomalala (1998), Catlett (1991), Dougherty, Kohavi & Sahami (1995), Monti & Cooper (1999), Fayyad & Irani (1993), Chiu, Cheung & Wong (1990), Nguyen & Nguyen (1998), Grzymala-Busse & Stefanowski (2001), Ting (1994), Ludl & Widmer (2000), Pfahringer (1995), An & Cercone (1999), Chiu & Cheung (1989), Chmielewski & Grzymala-Busse (1996), Lee & Shin (1994), Liu & Wellman (2002), Liu & Wellman (2004).","Variable granulation is a term that could describe a variety of techniques, most of which are aimed at reducing dimensionality, redundancy, and storage requirements. We briefly describe some of the ideas here, and present pointers to the literature.","A number of classical methods, such as principal component analysis, multidimensional scaling, factor analysis, and structural equation modeling, and their relatives, fall under the genus of ""variable transformation."" Also in this category are more modern areas of study such as dimensionality reduction, projection pursuit, and independent component analysis. The common goal of these methods in general is to find a representation of the data in terms of new variables, which are a linear or nonlinear transformation of the original variables, and in which important statistical relationships emerge. The resulting variable sets are almost always smaller than the original variable set, and hence these methods can be loosely said to impose a granulation on the feature space. These dimensionality reduction methods are all reviewed in the standard texts, such as Duda, Hart & Stork (2001), Witten & Frank (2005), and Hastie, Tibshirani & Friedman (2001).","A different class of variable granulation methods derive more from data clustering methodologies than from the linear systems theory informing the above methods. It was noted fairly early that one may consider ""clustering"" related variables in just the same way that one considers clustering related data. In data clustering, one identifies a group of similar entities (using a measure of ""similarity"" suitable to the domain), and then in some sense replaces those entities with a prototype of some kind. The prototype may be the simple average of the data in the identified cluster, or some other representative measure. But the key idea is that in subsequent operations, we may be able to use the single prototype for the data cluster (along with perhaps a statistical model describing how exemplars are derived from the prototype) to stand in for the much larger set of exemplars. These prototypes are generally such as to capture most of the information of interest concerning the entities.","Similarly, it is reasonable to ask whether a large set of variables might be aggregated into a smaller set of prototype variables that capture the most salient relationships between the variables. Although variable clustering methods based on linear correlation have been proposed (Duda, Hart & Stork 2001;Rencher 2002), more powerful methods of variable clustering are based on the mutual information between variables. Watanabe has shown (Watanabe 1960;Watanabe 1969) that for any set of variables one can construct a polytomic (i.e., n-ary) tree representing a series of variable agglomerations in which the ultimate ""total"" correlation among the complete variable set is the sum of the ""partial"" correlations exhibited by each agglomerating subset (see figure). Watanabe suggests that an observer might seek to thus partition a system in such a way as to minimize the interdependence between the parts ""... as if they were looking for a natural division or a hidden crack.""","One practical approach to building such a tree is to successively choose for agglomeration the two variables (either atomic variables or previously agglomerated variables) which have the highest pairwise mutual information (Kraskov et al. 2003). The product of each agglomeration is a new (constructed) variable that reflects the local joint distribution of the two agglomerating variables, and thus possesses an entropy equal to their joint entropy. (From a procedural standpoint, this agglomeration step involves replacing two columns in the attribute-value table—representing the two agglomerating variables—with a single column that has a unique value for every unique combination of values in the replaced columns (Kraskov et al. 2003). No information is lost by such an operation; however, it should be noted that if one is exploring the data for inter-variable relationships, it would generally not be desirable to merge redundant variables in this way, since in such a context it is likely to be precisely the redundancy or dependency between variables that is of interest; and once redundant variables are merged, their relationship to one another can no longer be studied.","In database systems, aggregations (see e.g. OLAP aggregation and Business intelligence systems) result in transforming original data tables (often called information systems) into the tables with different semantics of rows and columns, wherein the rows correspond to the groups (granules) of original tuples and the columns express aggregated information about original values within each of the groups. Such aggregations are usually based on SQL and its extensions. The resulting granules usually correspond to the groups of original tuples with the same values (or ranges) over some pre-selected original columns.","There are also other approaches wherein the groups are defined basing on, e.g., physical adjacency of rows. For example, Infobright implements a database engine wherein data is partitioned onto rough rows, each consisting of 64K of physically consecutive (or almost consecutive) rows. Rough rows are automatically labeled with compact information about their values on data columns, often involving multi-column and multi-table relationships. It results in a higher layer of granulated information systems where objects correspond to rough rows and attributes - to various flavors of rough information. Database operations can be efficiently supported within such a new framework, with an access to the original data pieces still available.","The origins of the granular computing ideology are to be found in the rough sets and fuzzy sets literatures. One of the key insights of rough set research—although by no means unique to it—is that, in general, the selection of different sets of features or variables will yield different concept granulations. Here, as in elementary rough set theory, by ""concept"" we mean a set of entities that are indistinguishable or indiscernible to the observer (i.e., a simple concept), or a set of entities that is composed from such simple concepts (i.e., a complex concept). To put it in other words, by projecting a data set (value-attribute system) onto different sets of variables, we recognize alternative sets of equivalence-class ""concepts"" in the data, and these different sets of concepts will in general be conducive to the extraction of different relationships and regularities.",We illustrate with an example. Consider the attribute-value system below:,"When the full set of attributes 



P
=
{

P

1


,

P

2


,

P

3


,

P

4


,

P

5


}


{\displaystyle P=\{P_{1},P_{2},P_{3},P_{4},P_{5}\}}

 is considered, we see that we have the following seven equivalence classes or primitive (simple) concepts:","Thus, the two objects within the first equivalence class, 



{

O

1


,

O

2


}


{\displaystyle \{O_{1},O_{2}\}}

, cannot be distinguished from one another based on the available attributes, and the three objects within the second equivalence class, 



{

O

3


,

O

7


,

O

10


}


{\displaystyle \{O_{3},O_{7},O_{10}\}}

, cannot be distinguished from one another based on the available attributes. The remaining five objects are each discernible from all other objects. Now, let us imagine a projection of the attribute value system onto attribute 




P

1




{\displaystyle P_{1}}

 alone, which would represent, for example, the view from an observer which is only capable of detecting this single attribute. Then we obtain the following much coarser equivalence class structure.","This is in a certain regard the same structure as before, but at a lower degree of resolution (larger grain size). Just as in the case of value granulation (discretization/quantization), it is possible that relationships (dependencies) may emerge at one level of granularity that are not present at another. As an example of this, we can consider the effect of concept granulation on the measure known as attribute dependency (a simpler relative of the mutual information).","To establish this notion of dependency (see also rough sets), let 



[
x

]

Q


=
{

Q

1


,

Q

2


,

Q

3


,
…
,

Q

N


}


{\displaystyle [x]_{Q}=\{Q_{1},Q_{2},Q_{3},\dots ,Q_{N}\}}

 represent a particular concept granulation, where each 




Q

i




{\displaystyle Q_{i}}

 is an equivalence class from the concept structure induced by attribute set 



Q


{\displaystyle Q}

. For example, if the attribute set 



Q


{\displaystyle Q}

 consists of attribute 




P

1




{\displaystyle P_{1}}

 alone, as above, then the concept structure 



[
x

]

Q




{\displaystyle [x]_{Q}}

 will be composed of 




Q

1


=
{

O

1


,

O

2


}


{\displaystyle Q_{1}=\{O_{1},O_{2}\}}

, 




Q

2


=
{

O

3


,

O

5


,

O

7


,

O

9


,

O

10


}


{\displaystyle Q_{2}=\{O_{3},O_{5},O_{7},O_{9},O_{10}\}}

, and 




Q

3


=
{

O

4


,

O

6


,

O

8


}


{\displaystyle Q_{3}=\{O_{4},O_{6},O_{8}\}}

. The dependency of attribute set 



Q


{\displaystyle Q}

 on another attribute set 



P


{\displaystyle P}

, 




γ

P


(
Q
)


{\displaystyle \gamma _{P}(Q)}

, is given by","That is, for each equivalence class 




Q

i




{\displaystyle Q_{i}}

 in 



[
x

]

Q




{\displaystyle [x]_{Q}}

, we add up the size of its ""lower approximation"" (see rough sets) by the attributes in 



P


{\displaystyle P}

, i.e., 





P
_



Q

i




{\displaystyle {\underline {P}}Q_{i}}

. More simply, this approximation is the number of objects which on attribute set 



P


{\displaystyle P}

 can be positively identified as belonging to target set 




Q

i




{\displaystyle Q_{i}}

. Added across all equivalence classes in 



[
x

]

Q




{\displaystyle [x]_{Q}}

, the numerator above represents the total number of objects which—based on attribute set 



P


{\displaystyle P}

—can be positively categorized according to the classification induced by attributes 



Q


{\displaystyle Q}

. The dependency ratio therefore expresses the proportion (within the entire universe) of such classifiable objects, in a sense capturing the ""synchronization"" of the two concept structures 



[
x

]

Q




{\displaystyle [x]_{Q}}

 and 



[
x

]

P




{\displaystyle [x]_{P}}

. The dependency 




γ

P


(
Q
)


{\displaystyle \gamma _{P}(Q)}

 ""can be interpreted as a proportion of such objects in the information system for which it suffices to know the values of attributes in 



P


{\displaystyle P}

 to determine the values of attributes in 



Q


{\displaystyle Q}

"" (Ziarko & Shan 1995).","Having gotten definitions now out of the way, we can make the simple observation that the choice of concept granularity (i.e., choice of attributes) will influence the detected dependencies among attributes. Consider again the attribute value table from above:","Let us consider the dependency of attribute set 



Q
=
{

P

4


,

P

5


}


{\displaystyle Q=\{P_{4},P_{5}\}}

 on attribute set 



P
=
{

P

2


,

P

3


}


{\displaystyle P=\{P_{2},P_{3}\}}

. That is, we wish to know what proportion of objects can be correctly classified into classes of 



[
x

]

Q




{\displaystyle [x]_{Q}}

 based on knowledge of 



[
x

]

P




{\displaystyle [x]_{P}}

. The equivalence classes of 



[
x

]

Q




{\displaystyle [x]_{Q}}

 and of 



[
x

]

P




{\displaystyle [x]_{P}}

 are shown below.","The objects that can be definitively categorized according to concept structure 



[
x

]

Q




{\displaystyle [x]_{Q}}

 based on 



[
x

]

P




{\displaystyle [x]_{P}}

 are those in the set 



{

O

1


,

O

2


,

O

3


,

O

7


,

O

8


,

O

10


}


{\displaystyle \{O_{1},O_{2},O_{3},O_{7},O_{8},O_{10}\}}

, and since there are six of these, the dependency of 



Q


{\displaystyle Q}

 on 



P


{\displaystyle P}

, 




γ

P


(
Q
)
=
6

/

10


{\displaystyle \gamma _{P}(Q)=6/10}

. This might be considered an interesting dependency in its own right, but perhaps in a particular data mining application only stronger dependencies are desired.","We might then consider the dependency of the smaller attribute set 



Q
=
{

P

4


}


{\displaystyle Q=\{P_{4}\}}

 on the attribute set 



P
=
{

P

2


,

P

3


}


{\displaystyle P=\{P_{2},P_{3}\}}

. The move from 



Q
=
{

P

4


,

P

5


}


{\displaystyle Q=\{P_{4},P_{5}\}}

 to 



Q
=
{

P

4


}


{\displaystyle Q=\{P_{4}\}}

 induces a coarsening of the class structure 



[
x

]

Q




{\displaystyle [x]_{Q}}

, as will be seen shortly. We wish again to know what proportion of objects can be correctly classified into the (now larger) classes of 



[
x

]

Q




{\displaystyle [x]_{Q}}

 based on knowledge of 



[
x

]

P




{\displaystyle [x]_{P}}

. The equivalence classes of the new 



[
x

]

Q




{\displaystyle [x]_{Q}}

 and of 



[
x

]

P




{\displaystyle [x]_{P}}

 are shown below.","Clearly, 



[
x

]

Q




{\displaystyle [x]_{Q}}

 has a coarser granularity than it did earlier. The objects that can now be definitively categorized according to the concept structure 



[
x

]

Q




{\displaystyle [x]_{Q}}

 based on 



[
x

]

P




{\displaystyle [x]_{P}}

 constitute the complete universe 



{

O

1


,

O

2


,
…
,

O

10


}


{\displaystyle \{O_{1},O_{2},\ldots ,O_{10}\}}

, and thus the dependency of 



Q


{\displaystyle Q}

 on 



P


{\displaystyle P}

, 




γ

P


(
Q
)
=
1


{\displaystyle \gamma _{P}(Q)=1}

. That is, knowledge of membership according to category set 



[
x

]

P




{\displaystyle [x]_{P}}

 is adequate to determine category membership in 



[
x

]

Q




{\displaystyle [x]_{Q}}

 with complete certainty; In this case we might say that 



P
→
Q


{\displaystyle P\rightarrow Q}

. Thus, by coarsening the concept structure, we were able to find a stronger (deterministic) dependency. However, we also note that the classes induced in 



[
x

]

Q




{\displaystyle [x]_{Q}}

 from the reduction in resolution necessary to obtain this deterministic dependency are now themselves large and few in number; as a result, the dependency we found, while strong, may be less valuable to us than the weaker dependency found earlier under the higher resolution view of 



[
x

]

Q




{\displaystyle [x]_{Q}}

.","In general it is not possible to test all sets of attributes to see which induced concept structures yield the strongest dependencies, and this search must be therefore be guided with some intelligence. Papers which discuss this issue, and others relating to intelligent use of granulation, are those by Y.Y. Yao and Lotfi Zadeh listed in the #References below.","Another perspective on concept granulation may be obtained from work on parametric models of categories. In mixture model learning, for example, a set of data is explained as a mixture of distinct Gaussian (or other) distributions. Thus, a large amount of data is ""replaced"" by a small number of distributions. The choice of the number of these distributions, and their size, can again be viewed as a problem of concept granulation. In general, a better fit to the data is obtained by a larger number of distributions or parameters, but in order to extract meaningful patterns, it is necessary to constrain the number of distributions, thus deliberately coarsening the concept resolution. Finding the ""right"" concept resolution is a tricky problem for which many methods have been proposed (e.g., AIC, BIC, MDL, etc.), and these are frequently considered under the rubric of ""model regularization"".","Granular computing can be conceived as a framework of theories, methodologies, techniques, and tools that make use of information granules in the process of problem solving. In this sense, granular computing is used as an umbrella term to cover topics that have been studied in various fields in isolation. By examining all of these existing studies in light of the unified framework of granular computing and extracting their commonalities, it may be possible to develop a general theory for problem solving.","In a more philosophical sense, granular computing can describe a way of thinking that relies on the human ability to perceive the real world under various levels of granularity (i.e., abstraction) in order to abstract and consider only those things that serve a specific interest and to switch among different granularities. By focusing on different levels of granularity, one can obtain different levels of knowledge, as well as a greater understanding of the inherent knowledge structure. Granular computing is thus essential in human problem solving and hence has a very significant impact on the design and implementation of intelligent systems."
"In many areas of information science, finding predictive relationships from data is a very important task. Initial discovery of relationships is usually done with a training set while a test set and validation set are used for evaluating whether the discovered relationships hold. More formally, a training set is a set of data used to discover potentially predictive relationships. A test set is a set of data used to assess the strength and utility of a predictive relationship. Test and training sets are used in intelligent systems, machine learning, genetic programming and statistics.",,,"Regression analysis was one of the earliest such approaches to be developed. The data used to construct or discover a predictive relationship are called the training data set. Most approaches that search through training data for empirical relationships tend to overfit the data, meaning that they can identify apparent relationships in the training data that do not hold in general. A test set is a set of data that is independent of the training data, but that follows the same probability distribution as the training data. If a model fit to the training set also fits the test set well, minimal overfitting has taken place. A better fitting of the training set as opposed to the test set usually points to overfitting.","In order to avoid overfitting, when any classification parameter needs to be adjusted, it is necessary to have a validation set in addition to the training and test sets. For example if the most suitable classifier for the problem is sought, the training set is used to train the candidate algorithms, the validation set is used to compare their performances and decide which one to take, and finally, the test set is used to obtain the performance characteristics such as accuracy, sensitivity, specificity, F-measure and so on. The validation set functions as a hybrid: it is training data used by testing, but neither as part of the low-level training, nor as part of the final testing.","Most simply, part of the training set can be set aside and used as a validation set; this is known as the holdout method, and common proportions are 70%/30% training/validation. Alternatively, this process can be repeated, repeatedly partitioning the original training set into a training set and a validation set; this is known as cross-validation. These repeated partitions can be done in various ways, such as dividing into 2 equal sets and using them as training/validation and then validation/training, or repeatedly selecting a random subset as a validation set.",These can be defined as:[1][2],"The basic process of using a validation set for model selection (as part of training set, validation set, and test set) is:[3][2]","Since our goal is to find the network having the best performance on new data, the simplest approach to the comparison of different networks is to evaluate the error function using data which is independent of that used for training. Various networks are trained by minimization of an appropriate error function defined with respect to a training data set. The performance of the networks is then compared by evaluating the error function using an independent validation set, and the network having the smallest error with respect to the validation set is selected. This approach is called the hold out method. Since this procedure can itself lead to some overfitting to the validation set, the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set.","An application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error).","Sometimes the training set and validation set are referred to collectively as design set: the first part of the design set is the training set, the second part is the validation step.[4]","Another example of parameter adjustment is hierarchical classification (sometimes referred to as instance space decomposition [5]), which splits a complete multi-class problem into a set of smaller classiﬁcation problems. It serves for learning more accurate concepts due to simpler classiﬁcation boundaries in subtasks and individual feature selection procedures for subtasks. When doing classiﬁcation decomposition, the central choice is the order of combination of smaller classiﬁcation steps, called the classiﬁcation path. Depending on the application, it can be derived from the confusion matrix and, uncovering the reasons for typical errors and finding ways to prevent the system make those in the future. For example,[6] on the validation set one can see which classes are most frequently mutually confused by the system and then the instance space decomposition is done as follows: firstly, the classification is done among well recognizable classes, and the difficult to separate classes are treated as a single joint class, and finally, as a second classification step the joint class is classified into the two initially mutually confused classes.","In artificial intelligence or machine learning, a training set consists of an input vector and an answer vector, and is used together with a supervised learning method to train a knowledge database (e.g. a neural net or a naive Bayes classifier) used by an AI machine. Validation sets can be used for regularization by early stopping: stop training when the error on the validation set increases, as this is a sign of overfitting to the training set.[7]","This simple procedure is complicated in practice by the fact that the validation error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when overfitting has truly begun.[7]","In statistical modeling, a training set is used to fit a model that can be used to predict a ""response value"" from one or more ""predictors."" The fitting can include both variable selection and parameter estimation. Statistical models used for prediction are often called regression models, of which linear regression and logistic regression are two examples.","In these fields, a major emphasis is placed on avoiding overfitting, so as to achieve the best possible performance on an independent test set that follows the same probability distribution as the training set.","In general, an intelligent system consists of a function taking one or more arguments and results in an output vector, and the learning method's task is to run the system once with the input vector as the arguments, calculating the output vector, comparing it with the answer vector and then changing somewhat in order to get an output vector more like the answer vector next time the system is simulated.",
"In the context of machine learning, hyperparameter optimization or model selection is the problem of choosing a set of hyperparameters for a learning algorithm, usually with the goal of optimizing a measure of the algorithm's performance on an independent data set. Often cross-validation is used to estimate this generalization performance.[1] Hyperparameter optimization contrasts with actual learning problems, which are also often cast as optimization problems, but optimize a loss function on the training set alone. In effect, learning algorithms learn parameters that model/reconstruct their inputs well, while hyperparameter optimization is to ensure the model does not overfit its data by tuning, e.g., regularization.",,,"The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set[2] or evaluation on a held-out validation set.","Since the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search.","For example, a typical soft-margin SVM classifier equipped with an RBF kernel has at least two hyperparameters that need to be tuned for good performance on unseen data: a regularization constant C and a kernel hyperparameter γ. Both parameters are continuous, so to perform grid search, one selects a finite set of ""reasonable"" values for each, say","Grid search then trains an SVM with each pair (C, γ) in the Cartesian product of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.","Grid search suffers from the curse of dimensionality, but is often embarrassingly parallel because typically the hyperparameter settings it evaluates are independent of each other.[1]","Bayesian optimization is a methodology for the global optimization of noisy black-box functions. Applied to hyperparameter optimization, Bayesian optimization consists of developing a statistical model of the function from hyperparameter values to the objective evaluated on a validation set. Intuitively, the methodology assumes that there is some smooth but noisy function that acts as a mapping from hyperparameters to the objective. In Bayesian optimization, one aims to gather observations in such a manner as to evaluate the machine learning model the least number of times while revealing as much information as possible about this function and, in particular, the location of the optimum. Bayesian optimization relies on assuming a very general prior over functions which when combined with observed hyperparameter values and corresponding outputs yields a distribution over functions. The methodology proceeds by iteratively picking hyperparameters to observe (experiments to run) in a manner that trades off exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparameters which are expected to have a good outcome). In practice, Bayesian optimization has been shown[3][4][5][6] to obtain better results in fewer experiments than grid search and random search, due to the ability to reason about the quality of experiments before they are run.","Since grid searching is an exhaustive and therefore potentially expensive method, several alternatives have been proposed. In particular, a randomized search that simply samples parameter settings a fixed number of times has been found to be more effective in high-dimensional spaces than exhaustive search.[1]","For specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks.[7] Since then, these methods have been extended to other models such as support vector machines[8] or logistic regression.[9]",A different approach in order to obtain a gradient with respect to hyperparameters consists in differentiating the steps of an iteratative optimization algorithm using automatic differentiation.[10][11]
"An inauthentic text is a computer-generated expository document meant to appear as genuine, but which is actually meaningless. Frequently they are created in order to be intermixed with genuine documents and thus manipulate the results of search engines, as with Spam blogs. They are also carried along in email in order to fool spam filters by giving the spam the superficial characteristics of legitimate text.","Sometimes nonsensical documents are created with computer assistance for humorous effect, as with Dissociated press or Flarf poetry. They have also been used to challenge the veracity of a publication—MIT students submitted papers generated by a computer program called SCIgen to a conference, where they were initially accepted. This led the students to claim that the bar for submissions was too low.","With the amount of computer generated text outpacing the ability of people to humans to curate it, there needs some means of distinguishing between the two. Yet automated approaches to determining absolutely whether a text is authentic or not face intrinsic challenges of semantics. Noam Chomsky coined the phrase ""Colorless green ideas sleep furiously"" giving an example of grammatically-correct, but semantically incoherent sentence; some will point out that in certain contexts one could give this sentence (or any phrase) meaning.","The first group to use the expression in this regard can be found below from Indiana University. Their work explains in detail an attempt to detect inauthentic texts and identify pernicious problems of inauthentic texts in cyberspace. The site has a means of submitting text that assesses, based on supervised learning, whether a corpus is inauthentic or not. Many users have submitted incorrect types of data and have correspondingly commented on the scores. This application is meant for a specific kind of data; therefore, submitting, say, an email, will not return a meaningful score."
The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered. [1],"In machine learning, one aims to construct algorithms that are able to learn to predict a certain target output. To achieve this, the learning algorithm is presented some training examples that demonstrate the intended relation of input and output values. Then the learner is supposed to approximate the correct output, even for examples that have not been shown during training. Without any additional assumptions, this problem cannot be solved exactly since unseen situations might have an arbitrary output value. The kind of necessary assumptions about the nature of the target function are subsumed in the phrase inductive bias.[1][2]","A classical example of an inductive bias is Occam's razor, assuming that the simplest consistent hypothesis about the target function is actually the best. Here consistent means that the hypothesis of the learner yields correct outputs for all of the examples that have been given to the algorithm.","Approaches to a more formal definition of inductive bias are based on mathematical logic. Here, the inductive bias is a logical formula that, together with the training data, logically entails the hypothesis generated by the learner. Unfortunately, this strict formalism fails in many practical cases, where the inductive bias can only be given as a rough description (e.g. in the case of neural networks), or not at all.",,,The following is a list of common inductive biases in machine learning algorithms.,"Although most learning algorithms have a static bias, some algorithms are designed to shift their bias as they acquire more data.[3] This does not avoid bias, since the bias shifting process itself must have a bias."
"Inductive Functional Programming (IFP) is a special kind of inductive programming that uses functional programs as representation for examples, programs and background knowledge. The term is frequently used to make a distinction from inductive logic programming, which uses logic programs.",
"Inductive probability attempts to give the probability of future events based on past events. It is the basis for inductive reasoning, and gives the mathematical basis for learning and the perception of patterns. It is a source of knowledge about the world.","There are three sources of knowledge: inference, communication, and deduction. Communication relays information found using other methods. Deduction establishes new facts based on existing facts. Only inference establishes new facts from data.",The basis of inference is Bayes' theorem. But this theorem is sometimes hard to apply and understand. The simpler method to understand inference is in terms of quantities of information.,"Information describing the world is written in a language. For example, a simple mathematical language of propositions may be chosen. Sentences may be written down in this language as strings of characters. But in the computer it is possible to encode these sentences as strings of bits (1s and 0s). Then the language may be encoded so that the most commonly used sentences are the shortest. This internal language implicitly represents probabilities of statements.","Occam's razor says the ""simplest theory, consistent with the data is most likely to be correct"". The ""simplest theory"" is interpreted as the representation of the theory written in this internal language. The theory with the shortest encoding in this internal language is most likely to be correct.",,,"Probability and statistics was focused on probability distributions and tests of significance. Probability was formal, well defined, but limited in scope. In particular its application was limited to situations that could be defined as an experiment or trial, with a well defined population.","Bayes's theorem is named after Rev. Thomas Bayes 1701–1761. Bayesian inference broadened the application of probability to many situations where a population was not well defined. But Bayes' theorem always depended on prior probabilities, to generate new probabilities. It was unclear where these prior probabilities should come from.","Ray Solomonoff developed algorithmic probability which gave an explanation for what randomness is and how patterns in the data may be represented by computer programs, that give shorter representations of the data circa 1964.","Chris Wallace and D. M. Boulton developed minimum message length circa 1968. Later Jorma Rissanen developed the minimum description length circa 1978. These methods allow information theory to be related to probability, in a way that can be compared to the application of Bayes' theorem, but which give a source and explanation for the role of prior probabilities.","Marcus Hutter combined decision theory with the work of Ray Solomonoff and Andrey Kolmogorov to give a theory for the Pareto optimal behavior for an Intelligent agent, circa 1998.",The program with the shortest length that matches the data is the most likely to predict future data. This is the thesis behind the Minimum message length[1] and Minimum description length[2] methods.,At first sight Bayes' theorem appears different from the minimimum message/description length principle. At closer inspection it turns out to be the same. Bayes' theorem is about conditional probabilities. What is the probability that event B happens if firstly event A happens?,"Becomes in terms of message length L,","What this means is that in describing an event, if all the information is given describing the event then the length of the information may be used to give the raw probability of the event. So if the information describing the occurrence of A is given, along with the information describing B given A, then all the information describing A and B has been given.[3] [4]","Overfitting is where the model matches the random noise and not the pattern in the data. For example, take the situation where a curve is fitted to a set of points. If polynomial with many terms is fitted then it can more closely represent the data. Then the fit will be better, and the information needed to describe the deviances from the fitted curve will be smaller. Smaller information length means more probable.","However the information needed to describe the curve must also be considered. The total information for a curve with many terms may be greater than for a curve with fewer terms, that has not as good a fit, but needs less information to describe the polynomial.","Solomonoff's theory of inductive inference is also inductive inference. A bit string x is observed. Then consider all programs that generate strings starting with x. Cast in the form of inductive inference, the programs are theories that imply the observation of the bit string x.",The method used here to give probabilities for inductive inference is based on Solomonoff's theory of inductive inference.,"If all the bits are 1, then people infer that there is a bias in the coin and that it is more likely also that the next bit is 1 also. This is described as learning from, or detecting a pattern in the data.","Such a pattern may be represented by a computer program. A short computer program may be written that produces a series of bits which are all 1. If the length of the program K is 



L
(
K
)


{\displaystyle L(K)}

 bits then its prior probability is,",The length of the shortest program that represents the string of bits is called the Kolmogorov complexity.,Kolmogorov complexity is not computable. This is related to the halting problem. When searching for the shortest program some programs may go into an infinite loop.,"The Greek philosopher Epicurus is quoted as saying ""If more than one theory is consistent with the observations, keep all theories"".[5]","As in a crime novel all theories must be considered in determining the likely murderer, so with inductive probability all programs must be considered in determining the likely future bits arising from the stream of bits.","Programs that are already longer than n have no predictive power. The raw (or prior) probability that the pattern of bits is random (has no pattern) is 




2

−
n




{\displaystyle 2^{-n}}

.","Each program that produces the sequence of bits, but is shorter than the n is a theory/pattern about the bits with a probability of 




2

−
k




{\displaystyle 2^{-k}}

 where k is the length of the program.","The probability of receiving a sequence of bits y after receiving a series of bits x is then the conditional probability of receiving y given x, which is the probability of x with y appended, divided by the probability of x. [6] [7] [8]","The programming language effects the predictions of the next bit in the string. The language acts as a prior probability. This is particularly a problem where the programming language codes for numbers and other data types. Intuitively we think that 0 and 1 are simple numbers, and that prime numbers are somehow more complex the numbers may be factorized.","Using the Kolmogorov complexity gives an unbiased estimate (a universal prior) of the prior probability of a number. As a thought experiment an intelligent agent may be fitted with a data input device giving a series of numbers, after applying some transformation function to the raw numbers. Another agent might have the same input device with a different transformation function. The agents do not see or know about these transformation functions. Then there appears no rational basis for preferring one function over another. A universal prior insures that although two agents may have different initial probability distributions for the data input, the difference will be bounded by a constant.","So universal priors do not eliminate an initial bias, but they reduce and limit it. Whenever we describe an event in a language, either using a natural language or other, the language has encoded in it our prior expectations. So some reliance on prior probabilities are inevitable.",A problem arises where an intelligent agents prior expectations interact with the environment to form a self reinforcing feed back loop. This is the problem of bias or prejudice. Universal priors reduce but do not eliminate this problem.,The theory of universal artificial intelligence applies decision theory to inductive probabilities. The theory shows how the best actions to optimize a reward function may be chosen. The result is a theoretical model of intelligence. [9],"It is a fundamental theory of intelligence, which optimizes the agents behavior in,","In general no agent will always provide the best actions in all situations. A particular choice made by an agent may be wrong, and the environment may provide no way for the agent to recover from an initial bad choice. However the agent is Pareto optimal in the sense that no other agent will do better than this agent in this environment, without doing worse in another environment. No other agent may, in this sense, be said to be better.",At present the theory is limited by incomputability (the halting problem). Approximations may be used to avoid this. Processing speed and combinatorial explosion remain the primary limiting factors for artificial intelligence.,Probability is the representation of uncertain or partial knowledge about the truth of statements. Probabilities are subjective and personal estimates of likely outcomes based on past experience and inferences made from the data.,"This description of probability may seem strange at first. In natural language we refer to ""the probability"" that the sun will rise tomorrow. We do not refer to ""your probability"" that the sun will rise. But in order for inference to be correctly modeled probability must be personal, and the act of inference generates new posterior probabilities from prior probabilities.","Probabilities are personal because they are conditional on the knowledge of the individual. Probabilities are subjective because they always depend, to some extent, on prior probabilities assigned by the individual. Subjective should not be taken here to mean vague or undefined.",The term intelligent agent is used to refer to the holder of the probabilities. The intelligent agent may be a human or a machine. If the intelligent agent does not interact with the environment then the probability will converge over time to the frequency of the event.,"If however the agent uses the probability to interact with the environment there may be a feedback, so that two agents in the identical environment starting with only slightly different priors, end up with completely different probabilities. In this case optimal decision theory as in Marcus Hutter's Universal Artificial Intelligence will give Pareto optimal performance for the agent. This means that no other intelligent agent could do better in one environment without doing worse in another environment.","In deductive probability theories, probabilities are absolutes, independent of the individual making the assessment. But deductive probabilities are based on,","For example, in a trial the participants are aware the outcome of all the previous history of trials. They also assume that each outcome is equally probable. Together this allows a single unconditional value of probability to be defined.","But in reality each individual does not have the same information. And in general the probability of each outcome is not equal. The dice may be loaded, and this loading needs to be inferred from the data.",The principle of indifference has played a key role in probability theory. It says that if N statements are symmetric so that one condition cannot be preferred over another then all statements are equally probable.[10],"Taken seriously, in evaluating probability this principle leads to contradictions. Suppose there are 3 bags of gold in the distance and one is asked to select one. Then because of the distance one cannot see the bag sizes. You estimate using the principle of indifference that each bag has equal amounts of gold, and each bag has one third of the gold.","Now, while one of us is not looking, the other takes one of the bags and divide it into 3 bags. Now there are 5 bags of gold. The principle of indifference now says each bag has one fifth of the gold. A bag that was estimated to have one third of the gold is now estimated to have one fifth of the gold.","Taken as a value associated with the bag the values are different therefore contradictory. But taken as an estimate given under a particular scenario, both values are separate estimates given under different circumstances and there is no reason to believe they are equal.",Estimates of prior probabilities are particularly suspect. Estimates will be constructed that do not follow any consistent frequency distribution. For this reason prior probabilities are considered as estimates of probabilities rather than probabilities.,"A full theoretical treatment would associate with each probability,",Inductive probability combines two different approaches to probability.,Each approach gives a slightly different viewpoint. Information theory is used in relating probabilities to quantities of information. This approach is often used in giving estimates of prior probabilities.,Frequentist probability defines probabilities as objective statements about how often an event occurs. This approach may be stretched by defining the trials to be over possible worlds. Statements about possible worlds define events.,"Whereas logic represents only two values; true and false as the values of statement, probability associates a number between 0.0 and 1.0 with each statement. If the probability of a statement is 0 the statement is false. If the probability of a statement is 1 the statement is true.","In considering some data as a string of bits the prior probabilities for a sequence of 1 and 0s, the probability of 1 and 0 is equal. Therefore, each extra bit halves the probability of a sequence of bits. This leads to the conclusion that,",Where,The prior probability of any statement is calculated from the number of bits needed to state it. See also information theory.,"Two statements A and B may be represented by two separate encodings. Then the length of the encoding is,","or in terms of probability,","But this law is not always true because there may be a shorter method of encoding B if we assume A. So the above probability law applies only if A and B are ""independent"".","The primary use of the information approach to probability is to provide estimates of the complexity of statements. Recall that Occam's razor states that ""All things being equal, the simplest theory is the most likely to be correct"". In order to apply this rule, first there needs to be a definition of what ""simplest"" means. Information theory defines simplest to mean having the shortest encoding.",Knowledge is represented as statements. Each statement is a Boolean expression. Expressions are encoded by a function that takes a description (as against the value) of the expression and encodes it as a bit string.,The length of the encoding of a statement gives an estimate of the probability of a statement. This probability estimate will often be used as the prior probability of a statement.,Technically this estimate is not a probability because it is not constructed from a frequency distribution. The probability estimates given by it do not always obey the law of total of probability. Applying the law of total probability to various scenarios will usually give a more accurate probability estimate of the prior probability than the estimate from the length of the statement.,"An expression is constructed from sub expressions,",A Huffman code must distinguish the 3 cases. The length of each code is based on the frequency of each type of sub expressions.,"Initially constants are all assigned the same length/probability. Later constants may be assigned a probability using the Huffman code based on the number of uses of the function id in all expressions recorded so far. In using a Huffman code the goal is to estimate probabilities, not to compress the data.",The length of a function application is the length of the function identifier constant plus the sum of the sizes of the expressions for each parameter.,The length of a quantifier is the length of the expression being quantified over.,"No explicit representation of natural numbers is given. However natural numbers may be constructed by applying the successor function to 0, and then applying other arithmetic functions. A distribution of natural numbers is implied by this, based on the complexity of constructing each number.",Rational numbers are constructed by the division of natural numbers. The simplest representation has no common factors between the numerator and the denominator. This allows the probability distribution of natural numbers may be extended to rational numbers.,The probability of an event may be interpreted as the frequencies of outcomes where the statement is true divided by the total number of outcomes. If the outcomes form a continuum the frequency may need to be replaced with a measure.,"Events are sets of outcomes. Statements may be related to events. A Boolean statement B about outcomes defines a set of outcomes b,","Each probability is always associated with the state of knowledge at a particular point in the argument. Probabilities before an inference are known as prior probabilities, and probabilities after are known as posterior probabilities.","Probability depends on the facts known. The truth of a fact limits the domain of outcomes to the outcomes consistent with the fact. Prior probabilities are the probabilities before a fact is known. Posterior probabilities are after a fact is known. The posterior probabilities are said to be conditional on the fact. Conditional probabilities are written,",This means the probability that B is true given that A is true.,"All probabilities are in some sense conditional. The prior probability of B is,","In the frequentist approach, probabilities are defined as the ratio of the number of outcomes within an event to the total number of outcomes. In the possible world model each possible world is an outcome, and statements about possible worlds define events. The probability of a statement being true is the number of possible worlds divided by the total number of worlds.","The total number of worlds may be infinite. In this case instead of counting the elements of the set a measure must be used. In general the cardinality |S|, where S is a set, is a measure.","The probability of a statement A being true about possible worlds is then,",For a conditional probability.,then,Using symmetry this equation may be written out as Bayes' law.,This law describes the relationship between prior and posterior probabilities when new facts are learnt.,"Written as quantities of information Bayes' Theorem becomes,","Two statements A and B are said to be independent if knowing the truth of A does not change the probability of B. Mathematically this is,","then Bayes' Theorem reduces to,","For a set of mutually exclusive possibilities 




A

i




{\displaystyle A_{i}}

, the sum of the posterior probabilities must be 1.",Substituting using Bayes' theorem gives the law of total probability,"This result is used to give the extended form of Bayes' theorem,","This is the usual form of Bayes' theorem used in practice, because it guarantees the sum of all the posterior probabilities for 




A

i




{\displaystyle A_{i}}

 is 1.","For mutually exclusive possibilities, the probabilities add.",Using,Then the alternatives,are all mutually exclusive,"Also,","so, putting it all together,","As,",then,"Implication is related to conditional probability by the following equation,","Derivation,","Bayes' theorem may be used to estimate the probability of a hypothesis or theory H, given some facts F. The posterior probability of H is then","or in terms of information,","By assuming the hypothesis is true, a simpler representation of the statement F may be given. The length of the encoding of this simpler representation is L(F \mid H).","



L
(
H
)
+
L
(
F
∣
H
)


{\displaystyle L(H)+L(F\mid H)}

 represents the amount of information needed to represent the facts F, if H is true. L(F) is the amount of information needed to represent F without the hypothesis H. The difference is how much the representation of the facts has been compressed by assuming that H is true. This is the evidence that the hypothesis H is true.","If L(F) is estimated from encoding length then the probability obtained will not be between 0 and 1. The value obtained is proportional to the probability, without being a good probability estimate. The number obtained is sometimes referred to as a relative probability, being how much more probable the theory is than not holding the theory.","If a full set of mutually exclusive hypothesis that provide evidence is known, a proper estimate may be given for the prior probability 



P
(
F
)


{\displaystyle P(F)}

.","Probabilities may be calculated from the extended form of Bayes' theorem. Given all mutually exclusive hypothesis 




H

i




{\displaystyle H_{i}}

 which give evidence, such that,","and also the hypothesis R, that none of the hypothesis is true, then,","In terms of information,","In most situations it is a good approximation to assume that F is independent of R,","giving,","Abductive inference [11] [12] [13] [14] starts with a set of facts F which is a statement (Boolean expression). Abductive reasoning is of the form,","The theory T, also called an explanation of the condition F, is an answer to the ubiquitous factual ""why"" question. For example, for the condition F is ""Why do apples fall?"". The answer is a theory T that implies that apples fall;","Inductive inference is of the form,","In terms of abductive inference, all objects in a class C or set have a property P is a theory that implies the observed condition, All observed objects in a class C have a property P.",So inductive inference is a special case of abductive inference. In common usage the term inductive inference is often used to refer to both abductive and inductive inference.,"Inductive inference is related to generalization. Generalizations may be formed from statements by replacing a specific value with membership of a category, or by replacing membership of a category with membership of a broader category. In deductive logic, generalization is a powerful method of generating new theories that may be true. In inductive inference generalization generates theories that have a probability of being true.","The opposite of generalization is specialization. Specialization is used in applying a general rule to a specific case. Specializations are created from generalizations by replacing membership of a category by a specific value, or by replacing a category with a sub category.","The Linnaen classification of living things and objects forms the basis for generalization and specification. The ability to identify, recognize and classify is the basis for generalization. Perceiving the world as a collection of objects appears to be a key aspect of human intelligence. It is the object oriented model, in the non computer science sense.",The object oriented model is constructed from our perception. In particularly vision is based on the ability to compare two images and calculate how much information is needed to morph or map one image into another. Computer vision uses this mapping to construct 3D images from stereo image pairs.,"Inductive logic programming is a means of constructing theory that implies a condition. Plotkin's [15][16] ""relative least general generalization (rlgg)"" approach constructs the simplest generalization consistent with the condition.","Isaac Newton used inductive arguments in constructing his law of universal gravitation.[17] Starting with the statement,","Generalizing by replacing apple for object, and earth for object gives, in a two body system,","The theory explains all objects falling, so there is strong evidence for it. The second observation,","After some complicated mathematical calculus, it can be seen that if the acceleration follows the inverse square law then objects will follow an ellipse. So induction gives evidence for the inverse square law.","Using Galileo's observation that all objects drop with the same speed,","where 




i

1




{\displaystyle i_{1}}

 and 




i

2




{\displaystyle i_{2}}

 vectors towards the center of the other object. Then using Newton's third law 




F

1


=
−

F

2




{\displaystyle F_{1}=-F_{2}}

","Implication determines condition probability as,","So,","This result may be used in the probabilities given for Bayesian hypothesis testing. For a single theory, H = T and,","or in terms of information, the relative probability is,","Note that this estimate for P(T|F) is not a true probability. If 



L
(

T

i


)
<
L
(
F
)


{\displaystyle L(T_{i})<L(F)}

 then the theory has evidence to support it. Then for a set of theories 




T

i


=

H

i




{\displaystyle T_{i}=H_{i}}

, such that 



L
(

T

i


)
<
L
(
F
)


{\displaystyle L(T_{i})<L(F)}

,","giving,","Make a list of all the shortest programs 




K

i




{\displaystyle K_{i}}

 that each produce a distinct infinite string of bits, and satisfy the relation,","where,","The problem is to calculate the probability that the source is produced by program 




K

i




{\displaystyle K_{i}}

, given that the truncated source after n bits is x. This is represented by the conditional probability,",Using the extended form of Bayes' theorem,"where,","The extended form relies on the law of total probability. This means that the 




A

i




{\displaystyle A_{i}}

 must be distinct possibilities, which is given by the condition that each 




K

i




{\displaystyle K_{i}}

 produce a different infinite string. Also one of the conditions 




A

i




{\displaystyle A_{i}}

 must be true. This must be true, as in the limit as n tends to infinity, there is always at least one program that produces 




T

n


(
s
)


{\displaystyle T_{n}(s)}

.","Then using the extended form and substituting for 



B


{\displaystyle B}

 and 




A

i




{\displaystyle A_{i}}

 gives,","As 




K

i




{\displaystyle K_{i}}

 are chosen so that 




T

n


(
R
(

K

i


)
)
=
x


{\displaystyle T_{n}(R(K_{i}))=x}

, then,","The a-priori probability of the string being produced from the program, given no information about the string, is based on the size of the program,","giving,","Programs that are the same or longer than the length of x provide no predictive power. Separate them out giving,","Then identify the two probabilities as,","The opposite of this,","But the prior probability that x is a random set of bits is 




2

−
n




{\displaystyle 2^{-n}}

. So,","The probability that the source is random, or unpredictable is,","A model of how worlds are constructed is used in determining the probabilities of theories,","If w is the bit string then the world is created such that 



R
(
w
)


{\displaystyle R(w)}

 is true. An intelligent agent has some facts about the word, represented by the bit string c, which gives the condition,","The set of bit strings identical with any condition x is 



E
(
x
)


{\displaystyle E(x)}

.","A theory is a simpler condition that explains (or implies) C. The set of all such theories is called T,",extended form of Bayes' theorem may be applied,"where,","To apply Bayes' theorem the following must hold,","For 



T
(
C
)


{\displaystyle T(C)}

 to be a partition, no bit string n may belong to two theories. To prove this assume they can and derive a contradiction,","Secondly prove that T includes all outcomes consistent with the condition. As all theories consistent with C are included then 



R
(
w
)


{\displaystyle R(w)}

 must be in this set.","So Bayes theorem may be applied as specified giving,","Using the implication and condition probability law, the definition of 



T
(
C
)


{\displaystyle T(C)}

 implies,","The probability of each theory in T is given by,","so,","Finally the probabilities of the events may be identified with the probabilities of the condition which the outcomes in the event satisfy,",giving,This is the probability of the theory t after observing that the condition C holds.,"Theories that are less probable than the condition C have no predictive power. Separate them out giving,","The probability of the theories without predictive power on C is the same as the probability of C. So,",So the probability,"and the probability of no prediction for C, written as 



random
⁡
(
C
)


{\displaystyle \operatorname {random} (C)}

,","The probability of a condition was given as,","Bit strings for theories that are more complex than the bit string given to the agent as input have no predictive power. There probabilities are better included in the random case. To implement this a new definition is given as F in,","Using F, an improved version of the abductive probabilities is,"
"Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.","Depending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming.",,,"Inductive programming incorporates all approaches which are concerned with learning programs or algorithms from incomplete (formal) specifications. Possible inputs in an IP system are a set of training inputs and corresponding outputs or an output evaluation function, describing the desired behavior of the intended program, traces or action sequences which describe the process of calculating specific outputs, constraints for the program to be induced concerning its time efficiency or its complexity, various kinds of background knowledge such as standard data types, predefined functions to be used, program schemes or templates describing the data flow of the intended program, heuristics for guiding the search for a solution or other biases.","Output of an IP system is a program in some arbitrary programming language containing conditionals and loop or recursive control structures, or any other kind of Turing-complete representation language.","In many applications the output program must be correct with respect to the examples and partial specification, and this leads to the consideration of inductive programming as a special area inside automatic programming or program synthesis,[1][2] usually opposed to 'deductive' program synthesis,[3][4][5] where the specification is usually complete.","In other cases, inductive programming is seen as a more general area where any declarative programming or representation language can be used and we may even have some degree of error in the examples, as in general machine learning, the more specific area of structure mining or the area of symbolic artificial intelligence. A distinctive feature is the number of examples or partial specification needed. Typically, inductive programming techniques can learn from just a few examples.","The diversity of inductive programming usually comes from the applications and the languages that are used: apart from logic programming and functional programming, other programming paradigms and representation languages have been used or suggested in inductive programming, such as functional logic programming, constraint programming, probabilistic programming, abductive logic programming, modal logic, action languages, agent languages and many types of imperative languages.","Research on the inductive synthesis of recursive functional programs started in the early 1970s and was brought onto firm theoretical foundations with the seminal THESIS system of Summers[6] and work of Biermann.[7] These approaches were split into two phases: first, input-output examples are transformed into non-recursive programs (traces) using a small set of basic operators; second, regularities in the traces are searched for and used to fold them into a recursive program. The main results until the mid 1980s are surveyed by Smith.[8] Due to limited progress with respect to the range of programs that could be synthesized, research activities decreased significantly in the next decade.","The advent of logic programming brought a new elan but also a new direction in the early 1980s, especially due to the MIS system of Shapiro[9] eventually spawning the new field of inductive logic programming (ILP).[10] The early works of Plotkin,[11][12] and his ""relative least general generalization (rlgg)"", had an enormous impact in inductive logic programming. Most of ILP work addresses a wider class of problems, as the focus is not only on recursive logic programs but on machine learning of symbolic hypotheses from logical representations. However, there were some encouraging results on learning recursive Prolog programs such as quicksort from examples together with suitable background knowledge, for example with GOLEM.[13] But again, after initial success, the community got disappointed by limited progress about the induction of recursive programs[14][15][16] with ILP less and less focusing on recursive programs and leaning more and more towards a machine learning setting with applications in relational data mining and knowledge discovery.[17]","In parallel to work in ILP, Koza[18] proposed genetic programming in the early 1990s as a generate-and-test based approach to learning programs. The idea of genetic programming was further developed into the inductive programming system ADATE[19] and the systematic-search-based system MagicHaskeller.[20] Here again, functional programs are learned from sets of positive examples together with an output evaluation (fitness) function which specifies the desired input/output behavior of the program to be learned.","The early work in grammar induction (also known as grammatical inference) is related to inductive programming, as rewriting systems or logic programs can be used to represent production rules. In fact, early works in inductive inference considered grammar induction and Lisp program inference as basically the same problem.[21] The results in terms of learnability were related to classical concepts, such as identification-in-the-limit, as introduced in the seminal work of Gold.[22] More recently, the language learning problem was addressed by the inductive programming community.[23][24]","In the recent years, the classical approaches have been resumed and advanced with great success. Therefore, the synthesis problem has been reformulated on the background of constructor-based term rewriting systems taking into account modern techniques of functional programming, as well as moderate use of search-based strategies and usage of background knowledge as well as automatic invention of subprograms. Many new and successful applications have recently appeared beyond program synthesis, most especially in the area of data manipulation, programming by example and cognitive modelling (see below).","Other ideas have also been explored with the common characteristic of using declarative languages for the representation of hypotheses. For instance, the use of higher-order features, schemes or structured distances have been advocated for a better handling of recursive data types and structures;[25][26][27] abstraction has also been explored as a more powerful approach to cumulative learning and function invention.[28][29]","One powerful paradigm that has been recently used for the representation of hypotheses in inductive programming (generally in the form of generative models) is probabilistic programming (and related paradigms, such as stochastic logic programs and Bayesian logic programming).[30][31][32][33]","The first workshop on Approaches and Applications of Inductive Programming (AAIP) held in conjunction with ICML 2005 identified all applications where ""learning of programs or recursive rules are called for, [...] first in the domain of software engineering where structural learning, software assistants and software agents can help to relieve programmers from routine tasks, give programming support for end users, or support of novice programmers and programming tutor systems. Further areas of application are language learning, learning recursive control rules for AI-planning, learning recursive concepts in web-mining or for data-format transformations"".","Since then, these and many other areas have shown to be successful application niches for inductive programming, such as end-user programming,[34] the related areas of programming by example[35] and programming by demonstration,[36] and intelligent tutoring systems.","Other areas where inductive inference has been recently applied are knowledge acquisition,[37] artificial general intelligence,[38] reinforcement learning and theory evaluation,[39][40] and cognitive science in general.[41][42] There may also be prospective applications in intelligent agents, games, robotics, personalisation, ambient intelligence and human interfaces."
"Inductive transfer, or transfer learning, is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.[1] For example, the abilities acquired while learning to walk presumably apply when one learns to run, and knowledge gained while learning to recognize cars could apply when recognizing trucks. This area of research bears some relation to the long history of psychological literature on transfer of learning, although formal ties between the two fields are limited.","The earliest cited work on transfer in machine learning is attributed to Lorien Pratt [5] who formulated the discriminability-based transfer (DBT) algorithm in 1993.[2] In 1997, the journal Machine Learning [6] published a special issue devoted to Inductive Transfer[3] and by 1998, the field had advanced to include multi-task learning,[4] along with a more formal analysis of its theoretical foundations.[5] Learning to Learn,[6] edited by Sebastian Thrun and Pratt, is a comprehensive overview of the state of the art of inductive transfer at the time of its publication.","Inductive transfer has also been applied in cognitive science, with the journal Connection Science publishing a special issue on Reuse of Neural Networks through Transfer in 1996.[7]","Notably, scientists have developed algorithms for inductive transfer in Markov logic networks[8] and Bayesian networks.[9] Furthermore, researchers have applied techniques for transfer to problems in text classification,[10][11] spam filtering,[12] and urban combat simulation.[13] [14] [15]","There still exists much potential in this field while the ""transfer"" hasn't yet led to significant improvement in learning. Also, an intuitive understanding could be that ""transfer means a learner can directly learn from other correlated learners"". However, in this way, such a methodology in transfer learning, whose direction is illustrated by,[16][17] is not a hot spot in the area yet."
"Inferential theory of learning (ITL) is an area of machine learning which describes inferential processes performed by learning agents. ITL has been developed by Ryszard S. Michalski in 1980s. In ITL learning process is viewed as a search (inference) through hypotheses space guided by a specific goal. Results of learning need to be stored, in order to be used in the future.",
"In machine learning, instance-based learning (sometimes called memory-based learning[1]) is a family of learning algorithms that, instead of performing explicit generalization, compares new problem instances with instances seen in training, which have been stored in memory. Instance-based learning is a kind of lazy learning.","It is called instance-based because it constructs hypotheses directly from the training instances themselves.[2] This means that the hypothesis complexity can grow with the data:[2] in the worst case, a hypothesis is a list of n training items and the computational complexity of classifying a single new instance is O(n). One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data: instance-based learners may simply store a new instance or throw an old instance away.","Examples of instance-based learning algorithm are the k-nearest neighbor algorithm, kernel machines and RBF networks.[3]:ch. 8 These store (a subset of) their training set; when predicting a value/class for a new instance, they compute distances or similarities between this instance and the training instances to make a decision.","To battle the memory complexity of storing all training instances, as well as the risk of overfitting to noise in the training set, instance reduction algorithms have been proposed.[4]",Gagliardi[5] applies this family of classifiers in medical field as second-opinion diagnostic tools and as tools for the knowledge extraction phase in the process of knowledge discovery in databases. One of these classifiers (called Prototype exemplar learning classifier (PEL-C) is able to extract a mixture of abstracted prototypical cases (that are syndromes) and selected atypical clinical cases.
"Instantaneously trained neural networks are feedforward artificial neural networks that create a new hidden neuron node for each novel training sample. The weights to this hidden neuron separate out not only this training sample but others that are near it, thus providing generalization.[1][2] This training can be done in a variety of ways and the most popular network in this family is called the CC4 network where the separation is done using the nearest hyperplane that can be written down instantaneously. These networks use unary coding for an effective representation of the data sets.[3]","Instantaneously trained neural networks have been proposed as models of short term learning and used in web search, and financial time series prediction applications.[4] They have also been used in instant classification of documents[5] and for deep learning and data mining.[6][7]","As in other neural networks, their normal use is as software, but they have also been implemented in hardware using FPGAs[8] and by optical implementation.[9]","In the CC4 network, which is a three-stage network, the number of input nodes is one more than the size of the training vector, with the extra node serving as the biasing node whose input is always 1. For binary input vectors, the weights from the input nodes to the hidden neuron (say of index j) corresponding to the trained vector is given by the following formula:","where 



r


{\displaystyle r}

 is the radius of generalization and 



s


{\displaystyle s}

 is the Hamming weight (the number of 1s) of the binary sequence. From the hidden layer to the output layer the weights are 1 or -1 depending on whether the vector belongs to a given output class or not. The neurons in the hidden and output layers output 1 if the weighted sum to the input is 0 or positive and 0, if the weighted sum to the input is negative:",In feedback networks the Willshaw network as well as the Hopfield network are able to learn instantaneously. But these networks are plagued with spurious memories.[10]
"In the fields of machine learning, the theory of computation, and random matrix theory, a probability distribution over vectors is said to be in isotropic position if its covariance matrix is equal to the identity matrix.","Let 



D


{\textstyle D}

 be a distribution over vectors in the vector space 





R


n




{\textstyle \mathbb {R} ^{n}}

. Then 



D


{\textstyle D}

 is in isotropic position if, for vector 



v


{\textstyle v}

 sampled from the distribution,","A set of vectors is said to be in isotropic position if the uniform distribution over that set is in isotropic position. In particular, every orthonormal set of vectors is isotropic.","As a related definition, a convex body 



K


{\textstyle K}

 in 





R


n




{\textstyle \mathbb {R} ^{n}}

 is in isotropic position if, for all vectors 



x


{\textstyle x}

 in 





R


n




{\textstyle \mathbb {R} ^{n}}

, we have"
"The Journal of Machine Learning Research (usually abbreviated JMLR), is a scientific journal focusing on machine learning, a subfield of artificial intelligence. It was founded in 2000.","The journal was founded as an open-access alternative to the journal Machine Learning. In 2001, forty editors of Machine Learning resigned in order to support JMLR, saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.[1]","Print editions of JMLR were published by MIT Press until 2004, and by Microtome Publishing thereafter.",Since Summer 2007 JMLR is also publishing Machine Learning Open Source Software .
"In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen–Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form.[1][2]",,,"Let (x1, x2, …, xn) be an independent and identically distributed sample drawn from some distribution with an unknown density ƒ. We are interested in estimating the shape of this function ƒ. Its kernel density estimator is","where K(•) is the kernel — a non-negative function that integrates to one and has mean zero — and h > 0 is a smoothing parameter called the bandwidth. A kernel with subscript h is called the scaled kernel and defined as Kh(x) = 1/h K(x/h). Intuitively one wants to choose h as small as the data allow, however there is always a trade-off between the bias of the estimator and its variance; more on the choice of bandwidth below.","A range of kernel functions are commonly used: uniform, triangular, biweight, triweight, Epanechnikov, normal, and others. The Epanechnikov kernel is optimal in a mean square error sense,[3] though the loss of efficiency is small for the kernels listed previously,[4] and due to its convenient mathematical properties, the normal kernel is often used, which means K(x) = ϕ(x), where ϕ is the standard normal density function.","The construction of a kernel density estimate finds interpretations in fields outside of density estimation.[5] For example, in thermodynamics, this is equivalent to the amount of heat generated when heat kernels (the fundamental solution to the heat equation) are placed at each data point locations xi. Similar methods are used to construct discrete Laplace operators on point clouds for manifold learning.","Kernel density estimates are closely related to histograms, but can be endowed with properties such as smoothness or continuity by using a suitable kernel. To see this, we compare the construction of histogram and kernel density estimators, using these 6 data points: x1 = −2.1, x2 = −1.3, x3 = −0.4, x4 = 1.9, x5 = 5.1, x6 = 6.2. For the histogram, first the horizontal axis is divided into sub-intervals or bins which cover the range of the data. In this case, we have 6 bins each of width 2. Whenever a data point falls inside this interval, we place a box of height 1/12. If more than one data point falls inside the same bin, we stack the boxes on top of each other.","For the kernel density estimate, we place a normal kernel with variance 2.25 (indicated by the red dashed lines) on each of the data points xi. The kernels are summed to make the kernel density estimate (solid blue curve). The smoothness of the kernel density estimate is evident compared to the discreteness of the histogram, as kernel density estimates converge faster to the true underlying density for continuous random variables.[6]","The bandwidth of the kernel is a free parameter which exhibits a strong influence on the resulting estimate. To illustrate its effect, we take a simulated random sample from the standard normal distribution (plotted at the blue spikes in the rug plot on the horizontal axis). The grey curve is the true density (a normal density with mean 0 and variance 1). In comparison, the red curve is undersmoothed since it contains too many spurious data artifacts arising from using a bandwidth h = 0.05, which is too small. The green curve is oversmoothed since using the bandwidth h = 2 obscures much of the underlying structure. The black curve with a bandwidth of h = 0.337 is considered to be optimally smoothed since its density estimate is close to the true density.","The most common optimality criterion used to select this parameter is the expected L2 risk function, also termed the mean integrated squared error:","Under weak assumptions on ƒ and K,[1][2] MISE (h) = AMISE(h) + o(1/(nh) + h4) where o is the little o notation. The AMISE is the Asymptotic MISE which consists of the two leading terms","where 



R
(
g
)
=
∫
g
(
x

)

2



d
x


{\displaystyle R(g)=\int g(x)^{2}\,dx}

 for a function g, 




m

2


(
K
)
=
∫

x

2


K
(
x
)

d
x


{\displaystyle m_{2}(K)=\int x^{2}K(x)\,dx}

 and ƒ'' is the second derivative of ƒ. The minimum of this AMISE is the solution to this differential equation",or,"Neither the AMISE nor the hAMISE formulas are able to be used directly since they involve the unknown density function ƒ or its second derivative ƒ'', so a variety of automatic, data-based methods have been developed for selecting the bandwidth. Many review studies have been carried out to compare their efficacies,[7][8][9][10][11][12][13] with the general consensus that the plug-in selectors[5] [14] and cross validation selectors[15][16][17] are the most useful over a wide range of data sets.","Substituting any bandwidth h which has the same asymptotic order n−1/5 as hAMISE into the AMISE gives that AMISE(h) = O(n−4/5), where O is the big o notation. It can be shown that, under weak assumptions, there cannot exist a non-parametric estimator that converges at a faster rate than the kernel estimator.[18] Note that the n−4/5 rate is slower than the typical n−1 convergence rate of parametric methods.","If the bandwidth is not held fixed, but is varied depending upon the location of either the estimate (balloon estimator) or the samples (pointwise estimator), this produces a particularly powerful method termed adaptive or variable bandwidth kernel density estimation.",Bandwidth selection for kernel density estimation of heavy-tailed distributions is said to be relatively difficult.[19],"If Gaussian basis functions are used to approximate univariate data, and the underlying density being estimated is Gaussian, the optimal choice for h (that is, the bandwidth that minimises the mean integrated squared error) is[20]","where 






σ
^





{\displaystyle {\hat {\sigma }}}

 is the standard deviation of the samples. This approximation is termed the normal distribution approximation, Gaussian approximation, or Silverman's (1986) rule of thumb. While this rule of thumb is easy to compute, it should be used with caution as it can yield widely inaccurate estimates when the density is not close to being normal. For example, consider estimating the bimodal Gaussian mixture:","from a sample of 200 points. The figure on the right below shows the true density and two kernel density estimates --- one using the rule-of-thumb bandwidth, and the other using a solve-the-equation bandwidth.[5][14] The estimate based on the rule-of-thumb bandwidth is significantly oversmoothed. The Matlab script for this example uses kde.m and is given below.","Given the sample (x1, x2, …, xn), it is natural to estimate the characteristic function φ(t) = E[eitX] as","Knowing the characteristic function, it is possible to find the corresponding probability density function through the Fourier transform formula. One difficulty with applying this inversion formula is that it leads to a diverging integral, since the estimate 







φ
^



(
t
)



{\displaystyle \scriptstyle {\hat {\varphi }}(t)}

 is unreliable for large t’s. To circumvent this problem, the estimator 







φ
^



(
t
)



{\displaystyle \scriptstyle {\hat {\varphi }}(t)}

 is multiplied by a damping function ψh(t) = ψ(ht), which is equal to 1 at the origin and then falls to 0 at infinity. The “bandwidth parameter” h controls how fast we try to dampen the function 







φ
^



(
t
)



{\displaystyle \scriptstyle {\hat {\varphi }}(t)}

. In particular when h is small, then ψh(t) will be approximately one for a large range of t’s, which means that 







φ
^



(
t
)



{\displaystyle \scriptstyle {\hat {\varphi }}(t)}

 remains practically unaltered in the most important region of t’s.","The most common choice for function ψ is either the uniform function ψ(t) = 1{−1 ≤ t ≤ 1}, which effectively means truncating the interval of integration in the inversion formula to [−1/h, 1/h], or the gaussian function ψ(t) = e−π t2. Once the function ψ has been chosen, the inversion formula may be applied, and the density estimator will be",where K is the Fourier transform of the damping function ψ. Thus the kernel density estimator coincides with the characteristic function density estimator.,A non-exhaustive list of software implementations of kernel density estimators includes:
"In machine learning, the kernel embedding of distributions (also called the kernel mean or mean map) comprises a class of nonparametric methods in which a probability distribution is represented as an element of a reproducing kernel Hilbert space (RKHS).[1] A generalization of the individual data-point feature mapping done in classical kernel methods, the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, while allowing one to compare and manipulate distributions using Hilbert space operations such as inner products, distances, projections, linear transformations, and spectral analysis.[2] This learning framework is very general and can be applied to distributions over any space 



Ω


{\displaystyle \Omega }

 on which a sensible kernel function (measuring similarity between elements of 



Ω


{\displaystyle \Omega }

) may be defined. For example, various kernels have been proposed for learning from data which are: vectors in 





R


d




{\displaystyle \mathbb {R} ^{d}}

, discrete classes/categories, strings, graphs/networks, images, time series, manifolds, dynamical systems, and other structured objects.[3][4] The theory behind kernel embeddings of distributions has been primarily developed by Alex Smola, Le Song , Arthur Gretton, and Bernhard Schölkopf.","The analysis of distributions is fundamental in machine learning and statistics, and many algorithms in these fields rely on information theoretic approaches such as entropy, mutual information, or Kullback–Leibler divergence. However, to estimate these quantities, one must first either perform density estimation, or employ sophisticated space-partitioning/bias-correction strategies which are typically infeasible for high-dimensional data.[5] Commonly, methods for modeling complex distributions rely on parametric assumptions that may be unfounded or computationally challenging (e.g. Gaussian mixture models), while nonparametric methods like kernel density estimation (Note: the smoothing kernels in this context have a different interpretation than the kernels discussed here) or characteristic function representation (via the Fourier transform of the distribution) break down in high-dimensional settings.[2]",Methods based on the kernel embedding of distributions sidestep these problems and also possess the following advantages:[5],"Thus, learning via the kernel embedding of distributions offers a principled drop-in replacement for information theoretic approaches and is a framework which not only subsumes many popular methods in machine learning and statistics as special cases, but also can lead to entirely new learning algorithms.",,,"Let 



X


{\displaystyle X}

 denote a random variable with codomain 



Ω


{\displaystyle \Omega }

 and distribution 



P
(
X
)


{\displaystyle P(X)}

. Given a kernel 



k


{\displaystyle k}

 on 



Ω
×
Ω


{\displaystyle \Omega \times \Omega }

, the Moore-Aronszajn Theorem asserts the existence of a RKHS 





H




{\displaystyle {\mathcal {H}}}

 (a Hilbert space of functions 



f
:
Ω
↦

R



{\displaystyle f:\Omega \mapsto \mathbb {R} }

 equipped with inner products 



⟨
⋅
,
⋅

⟩


H





{\displaystyle \langle \cdot ,\cdot \rangle _{\mathcal {H}}}

 and norms 




|


|

⋅

|



|



H





{\displaystyle ||\cdot ||_{\mathcal {H}}}

) in which the element 



 
k
(
x
,
⋅
)


{\displaystyle \ k(x,\cdot )}

 satisfies the reproducing property 



⟨
f
,
k
(
x
,
⋅
)

⟩


H



=
f
(
x
)
 
∀
f
∈


H


,
∀
x
∈
Ω


{\displaystyle \langle f,k(x,\cdot )\rangle _{\mathcal {H}}=f(x)\ \forall f\in {\mathcal {H}},\forall x\in \Omega }

. One may alternatively consider 



 
k
(
x
,
⋅
)


{\displaystyle \ k(x,\cdot )}

 an implicit feature mapping 



ϕ
(
x
)


{\displaystyle \phi (x)}

 from 



Ω


{\displaystyle \Omega }

 to 





H




{\displaystyle {\mathcal {H}}}

 (which is therefore also called the feature space), so that 



 
k
(
x
,

x
′

)
=
⟨
ϕ
(
x
)
,
ϕ
(

x
′

)

⟩


H





{\displaystyle \ k(x,x')=\langle \phi (x),\phi (x')\rangle _{\mathcal {H}}}

 can be viewed as a measure of similarity between points 



x
,

x
′

∈
Ω


{\displaystyle x,x'\in \Omega }

. While the similarity measure is linear in the feature space, it may be highly nonlinear in the original space depending on the choice of kernel.","The kernel embedding of the distribution 



P
(
X
)


{\displaystyle P(X)}

 in 





H




{\displaystyle {\mathcal {H}}}

 (also called the kernel mean or mean map) is given by:[1]","A kernel is characteristic if the mean embedding 



μ
:
{

family of distributions over 

Ω
}
↦


H




{\displaystyle \mu :\{{\text{family of distributions over }}\Omega \}\mapsto {\mathcal {H}}}

 is injective.[6] Each distribution can thus be uniquely represented in the RKHS and all statistical features of distributions are preserved by the kernel embedding if a characteristic kernel is used.","Given 



n


{\displaystyle n}

 training examples 



{

x

1


,
…
,

x

n


}


{\displaystyle \{x_{1},\dots ,x_{n}\}}

 drawn independently and identically distributed (i.i.d.) from 



P


{\displaystyle P}

, the kernel embedding of 



P


{\displaystyle P}

 can be empirically estimated as","If 



Y


{\displaystyle Y}

 denotes another random variable (for simplicity, assume the domain of 



Y


{\displaystyle Y}

 is also 



Ω


{\displaystyle \Omega }

 with the same kernel 



k


{\displaystyle k}

 which satisfies 



⟨
ϕ
(
x
)
⊗
ϕ
(
y
)
,
ϕ
(

x
′

)
⊗
ϕ
(

y
′

)
⟩
=
k
(
x
,

x
′

)
⊗
k
(
y
,

y
′

)


{\displaystyle \langle \phi (x)\otimes \phi (y),\phi (x')\otimes \phi (y')\rangle =k(x,x')\otimes k(y,y')}

), then the joint distribution 



P
(
X
,
Y
)


{\displaystyle P(X,Y)}

 can be mapped into a tensor product feature space 





H


⊗


H




{\displaystyle {\mathcal {H}}\otimes {\mathcal {H}}}

 via [2]","By the equivalence between a tensor and a linear map, this joint embedding may be interpreted as an uncentered cross-covariance operator 






C



X
Y


:


H


↦


H




{\displaystyle {\mathcal {C}}_{XY}:{\mathcal {H}}\mapsto {\mathcal {H}}}

 from which the cross-covariance of mean-zero functions 



f
,
g
∈


H




{\displaystyle f,g\in {\mathcal {H}}}

 can be computed as [7]","Given 



n


{\displaystyle n}

 pairs of training examples 



{
(

x

1


,

y

1


)
,
…
,
(

x

n


,

y

n


)
}


{\displaystyle \{(x_{1},y_{1}),\dots ,(x_{n},y_{n})\}}

 drawn i.i.d. from 



P


{\displaystyle P}

, we can also empirically estimate the joint distribution kernel embedding via","Given a conditional distribution 



P
(
Y
∣
X
)


{\displaystyle P(Y\mid X)}

, one can define the corresponding RKHS embedding as [2]","Note that the embedding of 



P
(
Y
∣
X
)


{\displaystyle P(Y\mid X)}

 thus defines a family of points in the RKHS indexed by the values 



x


{\displaystyle x}

 taken by conditioning variable 



X


{\displaystyle X}

. By fixing 



X


{\displaystyle X}

 to a particular value, we obtain a single element in 





H




{\displaystyle {\mathcal {H}}}

, and thus it is natural to define the operator","which given the feature mapping of 



x


{\displaystyle x}

 outputs the conditional embedding of 



Y


{\displaystyle Y}

 given 



X
=
x


{\displaystyle X=x}

. Assuming that for all 



g
∈


H


:
 


E


Y
∣
X


[
g
(
Y
)
]
∈


H




{\displaystyle g\in {\mathcal {H}}:\ \mathbb {E} _{Y\mid X}[g(Y)]\in {\mathcal {H}}}

, it can be shown that [7]","This assumption is always true for finite domains with characteristic kernels, but may not necessarily hold for continuous domains.[2] Nevertheless, even in cases where the assumption fails, 






C



Y
∣
X


ϕ
(
x
)


{\displaystyle {\mathcal {C}}_{Y\mid X}\phi (x)}

 may still be used to approximate the conditional kernel embedding 




μ

Y
∣
x




{\displaystyle \mu _{Y\mid x}}

, and in practice, the inversion operator is replaced with a regularized version of itself 



(



C



X
X


+
λ

I


)

−
1




{\displaystyle ({\mathcal {C}}_{XX}+\lambda \mathbf {I} )^{-1}}

 (where 




I



{\displaystyle \mathbf {I} }

 denotes the identity matrix).","Given training examples 



{
(

x

1


,

y

1


)
,
…
,
(

x

n


,

y

n


)
}


{\displaystyle \{(x_{1},y_{1}),\dots ,(x_{n},y_{n})\}}

, the empirical kernel conditional embedding operator may be estimated as [2]","where 




Φ

=

(
ϕ
(

y

i


)
,
…
,
(

y

n


)
)

,

Υ

=

(
ϕ
(

x

i


)
,
…
,
(

x

n


)
)



{\displaystyle {\boldsymbol {\Phi }}=\left(\phi (y_{i}),\dots ,(y_{n})\right),{\boldsymbol {\Upsilon }}=\left(\phi (x_{i}),\dots ,(x_{n})\right)}

 are implicitly formed feature matrices, 




K

=


Υ


T



Υ



{\displaystyle \mathbf {K} ={\boldsymbol {\Upsilon }}^{T}{\boldsymbol {\Upsilon }}}

 is the Gram matrix for samples of 



X


{\displaystyle X}

, and 



λ


{\displaystyle \lambda }

 is a regularization parameter needed to avoid overfitting.","Thus, the empirical estimate of the kernel conditional embedding is given by a weighted sum of samples of 



Y


{\displaystyle Y}

 in the feature space:","on compact subsets of 





R


d




{\displaystyle \mathbb {R} ^{d}}

 is universal.",This section illustrates how basic probabilistic rules may be reformulated as (multi)linear algebraic operations in the kernel embedding framework and is primarily based on the work of Song et al.[2][7] The following notation is adopted:,"In practice, all embeddings are empirically estimated from data 



{
(

x

1


,

y

1


)
,
…
,
(

x

n


,

y

n


)
}


{\displaystyle \{(x_{1},y_{1}),\dots ,(x_{n},y_{n})\}}

 and it assumed that a set of samples 



{




y
~




1


,
…
,




y
~






n
~




}


{\displaystyle \{{\widetilde {y}}_{1},\dots ,{\widetilde {y}}_{\widetilde {n}}\}}

 may be used to estimate the kernel embedding of the prior distribution 



π
(
Y
)


{\displaystyle \pi (Y)}

.","In probability theory, the marginal distribution of 



X


{\displaystyle X}

 can be computed by integrating out 



Y


{\displaystyle Y}

 from the joint density (including the prior distribution on 



Y


{\displaystyle Y}

)","The analog of this rule in the kernel embedding framework states that 




μ

X


π




{\displaystyle \mu _{X}^{\pi }}

, the RKHS embedding of 



Q
(
X
)


{\displaystyle Q(X)}

, can be computed via","In practical implementations, the kernel sum rule takes the following form","where 




μ

Y


π


=

∑

i
=
1




n
~





α

i


ϕ
(




y
~




i


)


{\displaystyle \mu _{Y}^{\pi }=\sum _{i=1}^{\widetilde {n}}\alpha _{i}\phi ({\widetilde {y}}_{i})}

 is the empirical kernel embedding of the prior distribution, 




α

=
(

α

1


,
…
,

α



n
~





)

T




{\displaystyle {\boldsymbol {\alpha }}=(\alpha _{1},\dots ,\alpha _{\widetilde {n}})^{T}}

, 




Υ

=

(
ϕ
(

x

1


)
,
…
,
ϕ
(

x

n


)
)



{\displaystyle {\boldsymbol {\Upsilon }}=\left(\phi (x_{1}),\dots ,\phi (x_{n})\right)}

, and 




G

,




G

~





{\displaystyle \mathbf {G} ,{\widetilde {\mathbf {G} }}}

 are Gram matrices with entries 





G


i
j


=
k
(

y

i


,

y

j


)
,





G

~




i
j


=
k
(

y

i


,




y
~




j


)


{\displaystyle \mathbf {G} _{ij}=k(y_{i},y_{j}),{\widetilde {\mathbf {G} }}_{ij}=k(y_{i},{\widetilde {y}}_{j})}

 respectively.","In probability theory, a joint distribution can be factorized into a product between conditional and marginal distributions","The analog of this rule in the kernel embedding framework states that 






C



X
Y


π




{\displaystyle {\mathcal {C}}_{XY}^{\pi }}

, the joint embedding of 



Q
(
X
,
Y
)


{\displaystyle Q(X,Y)}

, can be factorized as a composition of conditional embedding operator with the auto-covariance operator associated with 



π
(
Y
)


{\displaystyle \pi (Y)}

","In practical implementations, the kernel chain rule takes the following form","In probability theory, a posterior distribution can be expressed in terms of a prior distribution and a likelihood function as",The analog of this rule in the kernel embedding framework expresses the kernel embedding of the conditional distribution in terms of conditional embedding operators which are modified by the prior distribution,"In practical implementations, the kernel Bayes' rule takes the following form","where 




Λ

=


(

G

+



λ
~




I

)


−
1






G

~




diag

(

α

)
,

D

=

diag


(


(

G

+



λ
~




I

)


−
1






G

~




α

)



{\displaystyle {\boldsymbol {\Lambda }}=\left(\mathbf {G} +{\widetilde {\lambda }}\mathbf {I} \right)^{-1}{\widetilde {\mathbf {G} }}{\text{diag}}({\boldsymbol {\alpha }}),\mathbf {D} ={\text{diag}}\left(\left(\mathbf {G} +{\widetilde {\lambda }}\mathbf {I} \right)^{-1}{\widetilde {\mathbf {G} }}{\boldsymbol {\alpha }}\right)}

. Two regularization parameters are used in this framework: 



λ


{\displaystyle \lambda }

 for the estimation of 








C

^




Y
X


π


,





C

^




X
X


π


=

Υ


D



Υ


T




{\displaystyle {\widehat {\mathcal {C}}}_{YX}^{\pi },{\widehat {\mathcal {C}}}_{XX}^{\pi }={\boldsymbol {\Upsilon }}\mathbf {D} {\boldsymbol {\Upsilon }}^{T}}

 and 






λ
~





{\displaystyle {\widetilde {\lambda }}}

 for the estimation of the final conditional embedding operator 








C

^




Y
∣
X


π


=





C

^




Y
X


π




(
(





C

^




X
X


π



)

2


+



λ
~




I

)


−
1







C

^




X
X


π




{\displaystyle {\widehat {\mathcal {C}}}_{Y\mid X}^{\pi }={\widehat {\mathcal {C}}}_{YX}^{\pi }\left(({\widehat {\mathcal {C}}}_{XX}^{\pi })^{2}+{\widetilde {\lambda }}\mathbf {I} \right)^{-1}{\widehat {\mathcal {C}}}_{XX}^{\pi }}

. The latter regularization is done on square of 








C

^




X
X


π




{\displaystyle {\widehat {\mathcal {C}}}_{XX}^{\pi }}

 because 



D


{\displaystyle D}

 may not be positive definite.","The maximum mean discrepancy (MMD) is a distance-measure between distributions 



P
(
X
)


{\displaystyle P(X)}

 and 



Q
(
Y
)


{\displaystyle Q(Y)}

 which is defined as the squared distance between their embeddings in the RKHS [5]","While most distance-measures between distributions such as the widely used Kullback–Leibler divergence either require density estimation (either parametrically or nonparametrically) or space partitioning/bias correction strategies,[5] the MMD is easily estimated as an empirical mean which is concentrated around the true value of the MMD. The characterization of this distance as the maximum mean discrepancy refers to the fact that computing the MMD is equivalent to finding the RKHS function that maximizes the difference in expectations between the two probability distributions","Given n training examples from 



P
(
X
)


{\displaystyle P(X)}

 and m samples from 



Q
(
Y
)


{\displaystyle Q(Y)}

, one can formulate a test statistic based on the empirical estimate of the MMD","to obtain a two-sample test [10] of the null hypothesis that both samples stem from the same distribution (i.e. 



P
=
Q


{\displaystyle P=Q}

) against the broad alternative 



P
≠
Q


{\displaystyle P\neq Q}

.","Although learning algorithms in the kernel embedding framework circumvent the need for intermediate density estimation, one may nonetheless use the empirical embedding to perform density estimation based on n samples drawn from an underlying distribution 




P

X


∗




{\displaystyle P_{X}^{*}}

. This can be done by solving the following optimization problem [5][11]","where the maximization is done over the entire space of distributions on 



Ω


{\displaystyle \Omega }

. Here, 




μ

X


[

P

X


]


{\displaystyle \mu _{X}[P_{X}]}

 is the kernel embedding of the proposed density 




P

X




{\displaystyle P_{X}}

 and 



H


{\displaystyle H}

 is an entropy-like quantity (e.g. Entropy, KL divergence, Bregman divergence). The distribution which solves this optimization may be interpreted as a compromise between fitting the empirical kernel means of the samples well, while still allocating a substantial portion of the probability mass to all regions of the probability space (much of which may not be represented in the training examples). In practice, a good approximate solution of the difficult optimization may be found by restricting the space of candidate densities to a mixture of M candidate distributions with regularized mixing proportions. Connections between the ideas underlying Gaussian processes and conditional random fields may be drawn with the estimation of conditional probability distributions in this fashion, if one views the feature mappings associated with the kernel as sufficient statistics in generalized (possibly infinite-dimensional) exponential families.[5]","A measure of the statistical dependence between random variables 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

 (from any domains on which sensible kernels can be defined) can be formulated based on the Hilbert–Schmidt Independence Criterion [12]","and can be used as a principled replacement for mutual information, Pearson correlation or any other dependence measure used in learning algorithms. Most notably, HSIC can detect arbitrary dependencies (when a characteristic kernel is used in the embeddings, HSIC is zero if and only if the variables are independent), and can be used to measure dependence between different types of data (e.g. images and text captions). Given n i.i.d. samples of each random variable, a simple parameter-free unbiased estimator of HSIC which exhibits concentration about the true value can be computed in 



O
(
n
(

d

f


2


+

d

g


2


)
)


{\displaystyle O(n(d_{f}^{2}+d_{g}^{2}))}

 time,[5] where the Gram matrices of the two datasets are approximated using 




A



A


T


,

B



B


T




{\displaystyle \mathbf {A} \mathbf {A} ^{T},\mathbf {B} \mathbf {B} ^{T}}

 with 




A

∈


R


n
×

d

f




,

B

∈


R


n
×

d

g






{\displaystyle \mathbf {A} \in \mathbb {R} ^{n\times d_{f}},\mathbf {B} \in \mathbb {R} ^{n\times d_{g}}}

. The desirable properties of HSIC have led to the formulation of numerous algorithms which utilize this dependence measure for a variety of common machine learning tasks such as: feature selection (BAHSIC [13]), clustering (CLUHSIC [14]), and dimensionality reduction (MUHSIC [15]).","Belief propagation is a fundamental algorithm for inference in graphical models in which nodes repeatedly pass and receive messages corresponding to the evaluation of conditional expectations. In the kernel embedding framework, the messages may be represented as RKHS functions and the conditional distribution embeddings can be applied to efficiently compute message updates. Given n samples of random variables represented by nodes in a Markov Random Field, the incoming message to node t from node u can be expressed as 




m

u
t


(
⋅
)
=

∑

i
=
1


n



β

u
t


i


ϕ
(

x

t


i


)


{\displaystyle m_{ut}(\cdot )=\sum _{i=1}^{n}\beta _{ut}^{i}\phi (x_{t}^{i})}

 if it assumed to lie in the RKHS. The kernel belief propagation update message from t to node s is then given by [2]","where 



⊙


{\displaystyle \odot }

 denotes the element-wise vector product, 



N
(
t
)
∖
s


{\displaystyle N(t)\backslash s}

 is the set of nodes connected to t excluding node s, 





β


u
t


=

(

β

u
t


1


,
…
,

β

u
t


n


)



{\displaystyle {\boldsymbol {\beta }}_{ut}=\left(\beta _{ut}^{1},\dots ,\beta _{ut}^{n}\right)}

, 





K


t


,


K


s




{\displaystyle \mathbf {K} _{t},\mathbf {K} _{s}}

 are the Gram matrices of the samples from variables 




X

t


,

X

s




{\displaystyle X_{t},X_{s}}

, respectively, and 





Υ


s


=

(
ϕ
(

x

s


1


)
,
…
,
ϕ
(

x

s


n


)
)



{\displaystyle {\boldsymbol {\Upsilon }}_{s}=\left(\phi (x_{s}^{1}),\dots ,\phi (x_{s}^{n})\right)}

 is the feature matrix for the samples from 




X

s




{\displaystyle X_{s}}

.","Thus, if the incoming messages to node t are linear combinations of feature mapped samples from 




X

t




{\displaystyle X_{t}}

, then the outgoing message from this node is also a linear combination of feature mapped samples from 




X

s




{\displaystyle X_{s}}

. This RKHS function representation of message-passing updates therefore produces an efficient belief propagation algorithm in which the potentials are nonparametric functions inferred from the data so that arbitrary statistical relationships may be modeled.[2]","In the hidden Markov model (HMM), two key quantities of interest are the transition probabilities between hidden states 



P
(

S

t


∣

S

t
−
1


)


{\displaystyle P(S^{t}\mid S^{t-1})}

 and the emission probabilities 



P
(

O

t


∣

S

t


)


{\displaystyle P(O^{t}\mid S^{t})}

 for observations. Using the kernel conditional distribution embedding framework, these quantities may be expressed in terms of samples from the HMM. A serious limitation of the embedding methods in this domain is the need for training samples containing hidden states, as otherwise inference with arbitrary distributions in the HMM is not possible.","One common use of HMMs is filtering in which the goal is to estimate posterior distribution over the hidden state 




s

t




{\displaystyle s^{t}}

 at time step t given a history of previous observations 




h

t


=
(

o

1


,
…
,

o

t


)


{\displaystyle h^{t}=(o^{1},\dots ,o^{t})}

 from the system. In filtering, a belief state 



P
(

S

t
+
1


∣

h

t
+
1


)


{\displaystyle P(S^{t+1}\mid h^{t+1})}

 is recursively maintained via a prediction step (where updates 



P
(

S

t
+
1


∣

h

t


)
=


E



S

t


∣

h

t




[
P
(

S

t
+
1


∣

S

t


)
]


{\displaystyle P(S^{t+1}\mid h^{t})=\mathbb {E} _{S^{t}\mid h^{t}}[P(S^{t+1}\mid S^{t})]}

 are computed by marginalizing out the previous hidden state) followed by a conditioning step (where updates 



P
(

S

t
+
1


∣

h

t


,

o

t
+
1


)
∝
P
(

o

t
+
1


∣

S

t
+
1


)
P
(

S

t
+
1


∣

h

t


)


{\displaystyle P(S^{t+1}\mid h^{t},o^{t+1})\propto P(o^{t+1}\mid S^{t+1})P(S^{t+1}\mid h^{t})}

 are computed by applying Bayes' rule to condition on a new observation).[2] The RKHS embedding of the belief state at time t+1 can be recursively expressed as","by computing the embeddings of the prediction step via the kernel sum rule and the embedding of the conditioning step via kernel Bayes' rule. Assuming a training sample 



(




s
~




1


,
…
,




s
~




T


,




o
~




1


,
…
,




o
~




T


)


{\displaystyle ({\widetilde {s}}^{1},\dots ,{\widetilde {s}}^{T},{\widetilde {o}}^{1},\dots ,{\widetilde {o}}^{T})}

 is given, one can in practice estimate 







μ
^





S

t
+
1


∣

h

t
+
1




=

∑

i
=
1


T



α

i


t


ϕ
(




s
~




t


)


{\displaystyle {\widehat {\mu }}_{S^{t+1}\mid h^{t+1}}=\sum _{i=1}^{T}\alpha _{i}^{t}\phi ({\widetilde {s}}^{t})}

 and filtering with kernel embeddings is thus implemented recursively using the following updates for the weights 




α

=
(

α

1


,
…
,

α

T


)


{\displaystyle {\boldsymbol {\alpha }}=(\alpha _{1},\dots ,\alpha _{T})}

 [2]","where 




G

,

K



{\displaystyle \mathbf {G} ,\mathbf {K} }

 denote the Gram matrices of 







s
~




1


,
…
,




s
~




T




{\displaystyle {\widetilde {s}}^{1},\dots ,{\widetilde {s}}^{T}}

 and 







o
~




1


,
…
,




o
~




T




{\displaystyle {\widetilde {o}}^{1},\dots ,{\widetilde {o}}^{T}}

 respectively, 







G

~





{\displaystyle {\widetilde {\mathbf {G} }}}

 is a transfer Gram matrix defined as 








G

~




i
j


=
k
(




s
~




i


,




s
~




j
+
1


)


{\displaystyle {\widetilde {\mathbf {G} }}_{ij}=k({\widetilde {s}}_{i},{\widetilde {s}}_{j+1})}

, and 





K



o

t
+
1




=
(
k
(




o
~




1


,

o

t
+
1


)
,
…
,
k
(




o
~




T


,

o

t
+
1


)

)

T




{\displaystyle \mathbf {K} _{o^{t+1}}=(k({\widetilde {o}}^{1},o^{t+1}),\dots ,k({\widetilde {o}}^{T},o^{t+1}))^{T}}

.","The support measure machine (SMM) is a generalization of the support vector machine (SVM) in which the training examples are probability distributions paired with labels 



{

P

i


,

y

i



}

i
=
1


n


,
 

y

i


∈
{
+
1
,
−
1
}


{\displaystyle \{P_{i},y_{i}\}_{i=1}^{n},\ y_{i}\in \{+1,-1\}}

.[16] SMMs solve the standard SVM dual optimization problem using the following expected kernel","which is computable in closed form for many common specific distributions 




P

i




{\displaystyle P_{i}}

 (such as the Gaussian distribution) combined with popular embedding kernels 



k


{\displaystyle k}

 (e.g. the Gaussian kernel or polynomial kernel), or can be accurately empirically estimated from i.i.d. samples 



{

x

i



}

i
=
1


n


∼
P
(
X
)
,
{

z

j



}

j
=
1


m


∼
Q
(
Z
)


{\displaystyle \{x_{i}\}_{i=1}^{n}\sim P(X),\{z_{j}\}_{j=1}^{m}\sim Q(Z)}

 via","Under certain choices of the embedding kernel 



k


{\displaystyle k}

, the SMM applied to training examples 



{

P

i


,

y

i



}

i
=
1


n




{\displaystyle \{P_{i},y_{i}\}_{i=1}^{n}}

 is equivalent to a SVM trained on samples 



{

x

i


,

y

i



}

i
=
1


n




{\displaystyle \{x_{i},y_{i}\}_{i=1}^{n}}

, and thus the SMM can be viewed as a flexible SVM in which a different data-dependent kernel (specified by the assumed form of the distribution 




P

i




{\displaystyle P_{i}}

) may be placed on each training point.[16]","The goal of domain adaptation is the formulation of learning algorithms which generalize well when the training and test data have different distributions. Given training examples 



{
(

x

i


t
r


,

y

i


t
r


)

}

i
=
1


n




{\displaystyle \{(x_{i}^{tr},y_{i}^{tr})\}_{i=1}^{n}}

 and a test set 



{
(

x

j


t
e


,

y

j


t
e


)

}

j
=
1


m




{\displaystyle \{(x_{j}^{te},y_{j}^{te})\}_{j=1}^{m}}

 where the 




y

j


t
e




{\displaystyle y_{j}^{te}}

 are unknown, three types of differences are commonly assumed between the distribution of the training examples 




P

t
r


(
X
,
Y
)


{\displaystyle P^{tr}(X,Y)}

 and the test distribution 




P

t
e


(
X
,
Y
)


{\displaystyle P^{te}(X,Y)}

:[17][18]","By utilizing the kernel embedding of marginal and conditional distributions, practical approaches to deal with the presence of these types of differences between training and test domains can be formulated. Covariate shift may be accounted for by reweighting examples via estimates of the ratio 




P

t
e


(
X
)

/


P

t
r


(
X
)


{\displaystyle P^{te}(X)/P^{tr}(X)}

 obtained directly from the kernel embeddings of the marginal distributions of 



X


{\displaystyle X}

 in each domain without any need for explicit estimation of the distributions.[18] Target shift, which cannot be similarly dealt with since no samples from 



Y


{\displaystyle Y}

 are available in the test domain, is accounted for by weighting training examples using the vector 





β


∗


(


y


t
r


)


{\displaystyle {\boldsymbol {\beta }}^{*}(\mathbf {y} ^{tr})}

 which solves the following optimization problem (where in practice, empirical approximations must be used) [17]","To deal with location scale conditional shift, one can perform a LS transformation of the training points to obtain new transformed training data 





X


n
e
w


=


X


t
r


⊙

W

+

B



{\displaystyle \mathbf {X} ^{new}=\mathbf {X} ^{tr}\odot \mathbf {W} +\mathbf {B} }

 (where 



⊙


{\displaystyle \odot }

 denotes the element-wise vector product). To ensure similar distributions between the new transformed training samples and the test data, 




W

,

B



{\displaystyle \mathbf {W} ,\mathbf {B} }

 are estimated by minimizing the following empirical kernel embedding distance [17]","In general, the kernel embedding methods for dealing with LS conditional shift and target shift may be combined to find a reweighted transformation of the training data which mimics the test distribution, and these methods may perform well even in the presence of conditional shifts other than location-scale changes.[17]","Given N sets of training examples sampled i.i.d. from distributions 




P

(
1
)


(
X
,
Y
)
,

P

(
2
)


(
X
,
Y
)
,
…
,

P

(
N
)


(
X
,
Y
)


{\displaystyle P^{(1)}(X,Y),P^{(2)}(X,Y),\dots ,P^{(N)}(X,Y)}

, the goal of domain generalization is to formulate learning algorithms which perform well on test examples sampled from a previously unseen domain 




P

∗


(
X
,
Y
)


{\displaystyle P^{*}(X,Y)}

 where no data from the test domain is available at training time. If conditional distributions 



P
(
Y
∣
X
)


{\displaystyle P(Y\mid X)}

 are assumed to be relatively similar across all domains, then a learner capable of domain generalization must estimate a functional relationship between the variables which is robust to changes in the marginals 



P
(
X
)


{\displaystyle P(X)}

. Based on kernel embeddings of these distributions, Domain Invariant Component Analysis (DICA) is a method which determines the transformation of the training data that minimizes the difference between marginal distributions while preserving a common conditional distribution shared between all training domains.[19] DICA thus extracts invariants, features that transfer across domains, and may be viewed as a generalization of many popular dimension-reduction methods such as kernel principal component analysis, transfer component analysis, and covariance operator inverse regression.[19]","Defining a probability distribution 





P




{\displaystyle {\mathcal {P}}}

 on the RKHS 





H




{\displaystyle {\mathcal {H}}}

 with 





P


(

μ


X

(
i
)



Y

(
i
)




)
=
1

/

N

 for 

i
=
1
,
…
,
N


{\displaystyle {\mathcal {P}}(\mu _{X^{(i)}Y^{(i)}})=1/N{\text{ for }}i=1,\dots ,N}

, DICA measures dissimilarity between domains via distributional variance which is computed as","so 




G



{\displaystyle \mathbf {G} }

 is a 



N
×
N


{\displaystyle N\times N}

 Gram matrix over the distributions from which the training data are sampled. Finding an orthogonal transform onto a low-dimensional subspace B (in the feature space) which minimizes the distributional variance, DICA simultaneously ensures that B aligns with the bases of a central subspace C for which 



Y


{\displaystyle Y}

 becomes independent of 



X


{\displaystyle X}

 given 




C

T


X


{\displaystyle C^{T}X}

 across all domains. In the absence of target values 



Y


{\displaystyle Y}

, an unsupervised version of DICA may be formulated which finds a low-dimensional subspace that minimizes distributional variance while simultaneously maximizing the variance of 



X


{\displaystyle X}

 (in the feature space) across all domains (rather than preserving a central subspace).[19]","In distribution regression, the goal is to regress from probability distributions to reals (or vectors). Many important machine learning and statistical tasks fit into this framework, including multi-instance learning, and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). In practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. Distribution regression has been successfully applied for example in supervised entropy learning, and aerosol prediction using multispectral satellite images.[20]","Given 






(
{

X

i
,
n



}

n
=
1



N

i




,

y

i


)



i
=
1


ℓ




{\displaystyle {\left(\{X_{i,n}\}_{n=1}^{N_{i}},y_{i}\right)}_{i=1}^{\ell }}

 training data, where the 







X

i


^



:=
{

X

i
,
n



}

n
=
1



N

i






{\displaystyle {\hat {X_{i}}}:=\{X_{i,n}\}_{n=1}^{N_{i}}}

 bag contains samples from a probability distribution 




X

i




{\displaystyle X_{i}}

 and the 




i

t
h




{\displaystyle i^{th}}

 output label is 




y

i


∈

R



{\displaystyle y_{i}\in \mathbb {R} }

, one can tackle the distribution regression task by taking the embeddings of the distributions, and learning the regressor from the embeddings to the outputs. In other words, one can consider the following kernel ridge regression problem 



(
λ
>
0
)


{\displaystyle (\lambda >0)}

","where 




μ





X
^




i




=

∫

Ω


k
(
⋅
,
u
)

d





X
^




i


(
u
)
=


1

N

i





∑

n
=
1



N

i




k
(
⋅
,

X

i
,
n


)


{\displaystyle \mu _{{\hat {X}}_{i}}=\int _{\Omega }k(\cdot ,u)\mathrm {d} {\hat {X}}_{i}(u)={\frac {1}{N_{i}}}\sum _{n=1}^{N_{i}}k(\cdot ,X_{i,n})}

 with a 



k


{\displaystyle k}

 kernel on the domain of 




X

i




{\displaystyle X_{i}}

-s 



(
k
:
Ω
×
Ω
→

R

)


{\displaystyle (k:\Omega \times \Omega \rightarrow \mathbb {R} )}

, 



K


{\displaystyle K}

 is a kernel on the embedded distributions, and 





H


(
K
)


{\displaystyle {\mathcal {H}}(K)}

 is the RKHS determined by 



K


{\displaystyle K}

. Examples for 



K


{\displaystyle K}

 include the linear kernel 




[
K
(

μ

P


,

μ

Q


)
=
⟨

μ

P


,

μ

Q



⟩



H


(
k
)


]



{\displaystyle \left[K(\mu _{P},\mu _{Q})=\langle \mu _{P},\mu _{Q}\rangle _{{\mathcal {H}}(k)}\right]}

, the Gaussian kernel 




[
K
(

μ

P


,

μ

Q


)
=

e

−


∥

μ

P


−

μ

Q


∥


H
(
k
)


2



/

(
2

σ

2


)


]



{\displaystyle \left[K(\mu _{P},\mu _{Q})=e^{-\left\|\mu _{P}-\mu _{Q}\right\|_{H(k)}^{2}/(2\sigma ^{2})}\right]}

, the exponential kernel 




[
K
(

μ

P


,

μ

Q


)
=

e

−


∥

μ

P


−

μ

Q


∥


H
(
k
)



/

(
2

σ

2


)


]



{\displaystyle \left[K(\mu _{P},\mu _{Q})=e^{-\left\|\mu _{P}-\mu _{Q}\right\|_{H(k)}/(2\sigma ^{2})}\right]}

, the Cauchy kernel 




[
K
(

μ

P


,

μ

Q


)
=


(
1
+


∥

μ

P


−

μ

Q


∥


H
(
k
)


2



/


σ

2


)


−
1


]



{\displaystyle \left[K(\mu _{P},\mu _{Q})=\left(1+\left\|\mu _{P}-\mu _{Q}\right\|_{H(k)}^{2}/\sigma ^{2}\right)^{-1}\right]}

, the generalized t-student kernel 




[
K
(

μ

P


,

μ

Q


)
=


(
1
+


∥

μ

P


−

μ

Q


∥


H
(
k
)


σ


)


−
1


,
(
σ
≤
2
)
]



{\displaystyle \left[K(\mu _{P},\mu _{Q})=\left(1+\left\|\mu _{P}-\mu _{Q}\right\|_{H(k)}^{\sigma }\right)^{-1},(\sigma \leq 2)\right]}

, or the inverse multiquadrics kernel 




[
K
(

μ

P


,

μ

Q


)
=


(


∥

μ

P


−

μ

Q


∥


H
(
k
)


2


+

σ

2


)


−


1
2




]



{\displaystyle \left[K(\mu _{P},\mu _{Q})=\left(\left\|\mu _{P}-\mu _{Q}\right\|_{H(k)}^{2}+\sigma ^{2}\right)^{-{\frac {1}{2}}}\right]}

.","The prediction on a new distribution 



(



X
^



)


{\displaystyle ({\hat {X}})}

 takes the simple, analytical form","where 




k

=


[


K


(



μ





X
^




i




,

μ



X
^






)




]


∈


R


1
×
ℓ




{\displaystyle \mathbf {k} ={\big [}K{\big (}\mu _{{\hat {X}}_{i}},\mu _{\hat {X}}{\big )}{\big ]}\in \mathbb {R} ^{1\times \ell }}

, 




G

=
[

G

i
j


]
∈


R


ℓ
×
ℓ




{\displaystyle \mathbf {G} =[G_{ij}]\in \mathbb {R} ^{\ell \times \ell }}

, 




G

i
j


=
K


(



μ





X
^




i




,

μ





X
^




j






)


∈

R



{\displaystyle G_{ij}=K{\big (}\mu _{{\hat {X}}_{i}},\mu _{{\hat {X}}_{j}}{\big )}\in \mathbb {R} }

, 




y

=
[

y

1


;
.
.
.
;

y

l


]
∈


R


ℓ




{\displaystyle \mathbf {y} =[y_{1};...;y_{l}]\in \mathbb {R} ^{\ell }}

. Under mild regularity conditions this estimator can be shown to be consistent and it can achieve the one-stage sampled (as if one had access to the true 




X

i




{\displaystyle X_{i}}

-s) minimax optimal rate.[20] In the 



J


{\displaystyle J}

 objective function 




y

i




{\displaystyle y_{i}}

-s are real numbers; the results can also be extended to the case when 




y

i




{\displaystyle y_{i}}

-s are 



d


{\displaystyle d}

-dimensional vectors, or more generally elements of a separable Hilbert space using operator-valued 



K


{\displaystyle K}

 kernels.","In this simple example, which is taken from Song et al.,[2] 



X
,
Y


{\displaystyle X,Y}

 are assumed to be discrete random variables which take values in the set 



{
1
,
…
,
K
}


{\displaystyle \{1,\dots ,K\}}

 and the kernel is chosen to be the Kronecker delta function, so 



k
(
x
,

x
′

)
=
δ
(
x
,

x
′

)


{\displaystyle k(x,x')=\delta (x,x')}

. The feature map corresponding to this kernel is the standard basis vector 



ϕ
(
x
)
=


e


x




{\displaystyle \phi (x)=\mathbf {e} _{x}}

. The kernel embeddings of such a distributions are thus vectors of marginal probabilities while the embeddings of joint distributions in this setting are 



K
×
K


{\displaystyle K\times K}

 matrices specifying joint probability tables, and the explicit form of these embeddings is","The conditional distribution embedding operator 






C



Y
∣
X


=



C



Y
X





C



X
X


−
1




{\displaystyle {\mathcal {C}}_{Y\mid X}={\mathcal {C}}_{YX}{\mathcal {C}}_{XX}^{-1}}

 is in this setting a conditional probability table","Thus, the embeddings of the conditional distribution under a fixed value of 



X


{\displaystyle X}

 may be computed as","In this discrete-valued setting with the Kronecker delta kernel, the kernel sum rule becomes",The kernel chain rule in this case is given by
"In machine learning, kernel random forests establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.[1]",,,"Leo Breiman[2] was the first person to notice the link between random forest and kernel methods. He pointed out that random forests which are grown using i.i.d random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon [3] established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani[4] proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet[1] first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centred random forest[5] and uniform random forest,[6] two simplified models of random forest. He named these two KeRFs by Centred KeRF and Uniform KeRF,and proved upper bounds on their rates of consistency.","Centred forest[5] is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level 



k


{\displaystyle k}

 is built, where 



k
∈

N



{\displaystyle k\in \mathbb {N} }

 is a parameter of the algorithm.","Uniform forest[6] is another simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at a point uniformly drawn on the side of the cell, along the preselected attribute.","Given a training sample 






D



n


=
{
(


X


i


,

Y

i


)

}

i
=
1


n




{\displaystyle {\mathcal {D}}_{n}=\{(\mathbf {X} _{i},Y_{i})\}_{i=1}^{n}}

 of 



[
0
,
1

]

p


×

R



{\displaystyle [0,1]^{p}\times \mathbb {R} }

-valued independent random variables distributed as the independent prototype pair 



(

X

,
Y
)


{\displaystyle (\mathbf {X} ,Y)}

, where 




E

[

Y

2


]
<
∞


{\displaystyle \mathbb {E} [Y^{2}]<\infty }

. We aim at predicting the response 



Y


{\displaystyle Y}

,associated with the random variable 




X



{\displaystyle \mathbf {X} }

, by estimating the regression function 



m
(

x

)
=

E

[
Y

|


X

=

x

]


{\displaystyle m(\mathbf {x} )=\mathbb {E} [Y|\mathbf {X} =\mathbf {x} ]}

. A random regression forest is an ensemble of 



M


{\displaystyle M}

 randomized regression trees. Denote 




m

n


(

x

,


Θ


j


)


{\displaystyle m_{n}(\mathbf {x} ,\mathbf {\Theta } _{j})}

 the predicted value at point 




x



{\displaystyle \mathbf {x} }

 by the 



j


{\displaystyle j}

-th tree, where 





Θ


1


,
⋯
,


Θ


M




{\displaystyle \mathbf {\Theta } _{1},\cdots ,\mathbf {\Theta } _{M}}

 are independent random variables, distributed as a generic random variable 




Θ



{\displaystyle \mathbf {\Theta } }

, independent of the sample 






D



n




{\displaystyle {\mathcal {D}}_{n}}

. This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate 




m

M
,
n


(

x

,

Θ

1


,
⋯
,

Θ

M


)
=


1
M



∑

j
=
1


M



m

n


(

x

,

Θ

j


)


{\displaystyle m_{M,n}(\mathbf {x} ,\Theta _{1},\cdots ,\Theta _{M})={\frac {1}{M}}\sum _{j=1}^{M}m_{n}(\mathbf {x} ,\Theta _{j})}

. For regression trees, we have 




m

n


=

∑

i
=
1


n






Y

i




1




X


i


∈

A

n


(

x

,

Θ

j


)





N

n


(

x

,

Θ

j


)





{\displaystyle m_{n}=\sum _{i=1}^{n}{\frac {Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}{N_{n}(\mathbf {x} ,\Theta _{j})}}}

, where 




A

n


(

x

,

Θ

j


)


{\displaystyle A_{n}(\mathbf {x} ,\Theta _{j})}

 is the cell containing 




x



{\displaystyle \mathbf {x} }

, designed with randomness 




Θ

j




{\displaystyle \Theta _{j}}

 and dataset 






D



n




{\displaystyle {\mathcal {D}}_{n}}

, and 




N

n


(

x

,

Θ

j


)
=

∑

i
=
1


n




1




X


i


∈

A

n


(

x

,

Θ

j


)




{\displaystyle N_{n}(\mathbf {x} ,\Theta _{j})=\sum _{i=1}^{n}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}

.","Thus random forest estimates satisfy, for all 




x

∈
[
0
,
1

]

d




{\displaystyle \mathbf {x} \in [0,1]^{d}}

, 




m

M
,
n


(

x

,

Θ

1


,
…
,

Θ

M


)
=


1
M



∑

j
=
1


M



(

∑

i
=
1


n






Y

i




1




X


i


∈

A

n


(

x

,

Θ

j


)





N

n


(

x

,

Θ

j


)



)



{\displaystyle m_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {1}{M}}\sum _{j=1}^{M}\left(\sum _{i=1}^{n}{\frac {Y_{i}\mathbf {1} _{\mathbf {X} _{i}\in A_{n}(\mathbf {x} ,\Theta _{j})}}{N_{n}(\mathbf {x} ,\Theta _{j})}}\right)}

. Random regression forest has two level of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet[1] defined KeRF by","which is equal to the mean of the 




Y

i




{\displaystyle Y_{i}}

's falling in the cells containing 




x



{\displaystyle \mathbf {x} }

 in the forest. If we define the connection function of the 



M


{\displaystyle M}

 finite forest as 




K

M
,
n


(

x

,

z

)
=


1
M



∑

j
=
1


M




1



z

∈

A

n


(

x

,

Θ

j


)




{\displaystyle K_{M,n}(\mathbf {x} ,\mathbf {z} )={\frac {1}{M}}\sum _{j=1}^{M}\mathbf {1} _{\mathbf {z} \in A_{n}(\mathbf {x} ,\Theta _{j})}}

, i.e. the proportion of cells shared between 




x



{\displaystyle \mathbf {x} }

 and 




z



{\displaystyle \mathbf {z} }

, then almost surely we have 







m
~




M
,
n


(

x

,

Θ

1


,
…
,

Θ

M


)
=




∑

i
=
1


n



Y

i



K

M
,
n


(

x

,


x


i


)



∑

ℓ
=
1


n



K

M
,
n


(

x

,


x


ℓ


)





{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})={\frac {\sum _{i=1}^{n}Y_{i}K_{M,n}(\mathbf {x} ,\mathbf {x} _{i})}{\sum _{\ell =1}^{n}K_{M,n}(\mathbf {x} ,\mathbf {x} _{\ell })}}}

, which defines the KeRF.","The construction of Centred KeRF of level 



k


{\displaystyle k}

 is the same as for centred forest, except that predictions are made by 







m
~




M
,
n


(

x

,

Θ

1


,
…
,

Θ

M


)


{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})}

, the corresponding kernel function, or connection function is","Uniform KeRF is built in the same way as uniform forest, except that predictions are made by 







m
~




M
,
n


(

x

,

Θ

1


,
…
,

Θ

M


)


{\displaystyle {\tilde {m}}_{M,n}(\mathbf {x} ,\Theta _{1},\ldots ,\Theta _{M})}

, the corresponding kernel function, or connection function is",Predictions given by KeRF and random forests are close if the number of points in each cell is controlled:,"When the number of trees 



M


{\displaystyle M}

 goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:","




Then almost surely,


|


m

∞
,
n


(

x

−




m
~




∞
,
n


(

x

)

|

≤




b

n


−

a

n




a

n








m
~




∞
,
n


(

x

)
+
n

ε

n



(

max

1
≤
i
≤
n



Y

i


)



{\displaystyle {\text{Then almost surely,}}|m_{\infty ,n}(\mathbf {x} -{\tilde {m}}_{\infty ,n}(\mathbf {x} )|\leq {\frac {b_{n}-a_{n}}{a_{n}}}{\tilde {m}}_{\infty ,n}(\mathbf {x} )+n\varepsilon _{n}\left(\max _{1\leq i\leq n}Y_{i}\right)}

.","Assume that 



Y
=
m
(

X

)
+
ε


{\displaystyle Y=m(\mathbf {X} )+\varepsilon }

, where 



ε


{\displaystyle \varepsilon }

 is a centred Gaussian noise, independent of 




X



{\displaystyle \mathbf {X} }

, with finite variance 




σ

2


<
∞


{\displaystyle \sigma ^{2}<\infty }

. Moreover, 




X



{\displaystyle \mathbf {X} }

 is uniformly distributed on 



[
0
,
1

]

d




{\displaystyle [0,1]^{d}}

 and 



m


{\displaystyle m}

 is Lipschitz. Scornet[1] proved upper bounds on the rates of consistency for centred KeRF and uniform KeRF.","Providing 



k
→
∞


{\displaystyle k\rightarrow \infty }

 and 



n

/


2

k


→
∞


{\displaystyle n/2^{k}\rightarrow \infty }

, there exists a constant 




C

1


>
0


{\displaystyle C_{1}>0}

 such that, for all 



n


{\displaystyle n}

, 




E

[




m
~




n


c
c


(

X

)
−
m
(

X

)

]

2


≤

C

1



n

−
1

/

(
3
+
d
log
⁡
2
)


(
log
⁡
n

)

2




{\displaystyle \mathbb {E} [{\tilde {m}}_{n}^{cc}(\mathbf {X} )-m(\mathbf {X} )]^{2}\leq C_{1}n^{-1/(3+d\log 2)}(\log n)^{2}}

.","Providing 



k
→
∞


{\displaystyle k\rightarrow \infty }

 and 



n

/


2

k


→
∞


{\displaystyle n/2^{k}\rightarrow \infty }

, there exists a constant 



C
>
0


{\displaystyle C>0}

 such that, 




E

[




m
~




n


u
f


(

X

)
−
m
(

X

)

]

2


≤
C

n

−
2

/

(
6
+
3
d
log
⁡
2
)


(
log
⁡
n

)

2




{\displaystyle \mathbb {E} [{\tilde {m}}_{n}^{uf}(\mathbf {X} )-m(\mathbf {X} )]^{2}\leq Cn^{-2/(6+3d\log 2)}(\log n)^{2}}

."
Knowledge integration is the process of synthesizing multiple knowledge models (or representations) into a common model (representation).,"Compared to information integration, which involves merging information having different schemas and representation models, knowledge integration focuses more on synthesizing the understanding of a given subject from different perspectives.","For example, multiple interpretations are possible of a set of student grades, typically each from a certain perspective. An overall, integrated view and understanding of this information can be achieved if these interpretations can be put under a common model, say, a student performance index.","The Web-based Inquiry Science Environment (WISE), from the University of California at Berkeley has been developed along the lines of knowledge integration theory.","Knowledge integration has also been studied as the process of incorporating new information into a body of existing knowledge with an interdisciplinary approach. This process involves determining how the new information and the existing knowledge interact, how existing knowledge should be modified to accommodate the new information, and how the new information should be modified in light of the existing knowledge.","A learning agent that actively investigates the consequences of new information can detect and exploit a variety of learning opportunities; e.g., to resolve knowledge conflicts and to fill knowledge gaps. By exploiting these learning opportunities the learning agent is able to learn beyond the explicit content of the new information.","The machine learning program KI, developed by Murray and Porter at the University of Texas at Austin, was created to study the use of automated and semi-automated knowledge integration to assist knowledge engineers constructing a large knowledge base.","A possible technique which can be used is semantic matching. More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on Minimal Mappings. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i).",The University of Waterloo operates a Bachelor of Knowledge Integration undergraduate degree program as an academic major or minor. The program started in 2008.
"The Knowledge Vault is a knowledge base created by Google. As of 2014, it contained 1.6 billion facts which had been collated automatically from the Internet.[1]","Knowledge Vault is a potential successor to Google's Knowledge Graph. The Knowledge Graph pulled in information from structured sources like Freebase, Wikidata and Wikipedia, while the Knowledge Vault is an accumulation of facts from across the entire web, including unstructured sources.[1] ""Facts"" in Knowledge Vault also include a confidence value, giving the capability of distinguishing between knowledge statements that have a high probability of being true from others that may be less likely to be true (based on the source that Google obtained the data from and other factors).",The concept behind the Knowledge Vault was presented in a paper authored by a Google Research team.[2],"Google has indicated that Knowledge Vault is a research paper and not an active product in development, as of August 2014.[3]"
"Large margin nearest neighbor (LMNN)[1] classification is a statistical machine learning algorithm for metric learning. It learns a pseudometric designed for k-nearest neighbor classification. The algorithm is based on semidefinite programming, a sub-class of convex optimization.",The goal of supervised learning (more specifically classification) is to learn a decision rule that can categorize data instances into pre-defined classes. The k-nearest neighbor rule assumes a training data set of labeled instances (i.e. the classes are known). It classifies a new data instance with the class obtained from the majority vote of the k closest (labeled) training instances. Closeness is measured with a pre-defined metric. Large margin nearest neighbors is an algorithm that learns this global (pseudo-)metric in a supervised fashion to improve the classification accuracy of the k-nearest neighbor rule.,,,"The main intuition behind LMNN is to learn a pseudometric under which all data instances in the training set are surrounded by at least k instances that share the same class label. If this is achieved, the leave-one-out error (a special case of cross validation) is minimized. Let the training data consist of a data set 



D
=
{
(




x
→




1


,

y

1


)
,
…
,
(




x
→




n


,

y

n


)
}
⊂

R

d


×
C


{\displaystyle D=\{({\vec {x}}_{1},y_{1}),\dots ,({\vec {x}}_{n},y_{n})\}\subset R^{d}\times C}

, where the set of possible class categories is 



C
=
{
1
,
…
,
c
}


{\displaystyle C=\{1,\dots ,c\}}

.",The algorithm learns a pseudometric of the type,"For 



d
(
⋅
,
⋅
)


{\displaystyle d(\cdot ,\cdot )}

 to be well defined, the matrix 




M



{\displaystyle \mathbf {M} }

 needs to be positive semi-definite. The Euclidean metric is a special case, where 




M



{\displaystyle \mathbf {M} }

 is the identity matrix. This generalization is often (falsely) referred to as Mahalanobis metric.","Figure 1 illustrates the effect of the metric under varying 




M



{\displaystyle \mathbf {M} }

. The two circles show the set of points with equal distance to the center 







x
→




i




{\displaystyle {\vec {x}}_{i}}

. In the Euclidean case this set is a circle, whereas under the modified (Mahalanobis) metric it becomes an ellipsoid.",The algorithm distinguishes between two types of special data points: target neighbors and impostors.,"Target neighbors are selected before learning. Each instance 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 has exactly 



k


{\displaystyle k}

 different target neighbors within 



D


{\displaystyle D}

, which all share the same class label 




y

i




{\displaystyle y_{i}}

. The target neighbors are the data points that should become nearest neighbors under the learned metric. Let us denote the set of target neighbors for a data point 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 as 




N

i




{\displaystyle N_{i}}

.","An impostor of a data point 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 is another data point 







x
→




j




{\displaystyle {\vec {x}}_{j}}

 with a different class label (i.e. 




y

i


≠

y

j




{\displaystyle y_{i}\neq y_{j}}

) which is one of the nearest neighbors of 







x
→




i




{\displaystyle {\vec {x}}_{i}}

. During learning the algorithm tries to minimize the number of impostors for all data instances in the training set.","Large margin nearest neighbors optimizes the matrix 




M



{\displaystyle \mathbf {M} }

 with the help of semidefinite programming. The objective is twofold: For every data point 







x
→




i




{\displaystyle {\vec {x}}_{i}}

, the target neighbors should be close and the impostors should be far away. Figure 1 shows the effect of such an optimization on an illustrative example. The learned metric causes the input vector 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 to be surrounded by training instances of the same class. If it was a test point, it would be classified correctly under the 



k
=
3


{\displaystyle k=3}

 nearest neighbor rule.",The first optimization goal is achieved by minimizing the average distance between instances and their target neighbors,"The second goal is achieved by constraining impostors 







x
→




l




{\displaystyle {\vec {x}}_{l}}

 to be one unit further away than target neighbors 







x
→




j




{\displaystyle {\vec {x}}_{j}}

 (and therefore pushing them out of the local neighborhood of 







x
→




i




{\displaystyle {\vec {x}}_{i}}

). The resulting inequality constraint can be stated as:","The margin of exactly one unit fixes the scale of the matrix 



M


{\displaystyle M}

. Any alternative choice 



c
>
0


{\displaystyle c>0}

 would result in a rescaling of 



M


{\displaystyle M}

 by a factor of 



1

/

c


{\displaystyle 1/c}

.",The final optimization problem becomes:,"Here the slack variables 




ξ

i
j
l




{\displaystyle \xi _{ijl}}

 absorb the amount of violations of the impostor constraints. Their overall sum is minimized. The last constraint ensures that 




M



{\displaystyle \mathbf {M} }

 is positive semi-definite. The optimization problem is an instance of semidefinite programming (SDP). Although SDPs tend to suffer from high computational complexity, this particular SDP instance can be solved very efficiently due to the underlying geometric properties of the problem. In particular, most impostor constraints are naturally satisfied and do not need to be enforced during runtime. A particularly well suited solver technique is the working set method, which keeps a small set of constraints that are actively enforced and monitors the remaining (likely satisfied) constraints only occasionally to ensure correctness.","LMNN was extended to multiple local metrics in the 2008 paper.[2] This extension significantly improves the classification error, but involves a more expensive optimization problem. In their 2009 publication in the Journal of Machine Learning Research,[3] Weinberger and Saul derive an efficient solver for the semi-definite program. It can learn a metric for the MNIST handwritten digit data set in several hours, involving billions of pairwise constraints. An open source Matlab implementation is freely available at the authors web page.",Kumal et al.[4] extended the algorithm to incorporate local invariances to multivariate polynomial transformations and improved regularization.
"In machine learning, lazy learning is a learning method in which generalization beyond the training data is delayed until a query is made to the system, as opposed to in eager learning, where the system tries to generalize the training data before receiving queries.","The main advantage gained in employing a lazy learning method, such as Case based reasoning, is that the target function will be approximated locally, such as in the k-nearest neighbor algorithm. Because the target function is approximated locally for each query to the system, lazy learning systems can simultaneously solve multiple problems and deal successfully with changes in the problem domain.","The disadvantages with lazy learning include the large space requirement to store the entire training dataset. Particularly noisy training data increases the case base unnecessarily, because no abstraction is made during the training phase. Another disadvantage is that lazy learning methods are usually slower to evaluate, though this is coupled with a faster training phase.",Lazy classifiers are most useful for large datasets with few attributes.,
"In statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions. The concept of learnable classes are closely related to regularization in machine learning, and provides large sample justifications for certain learning algorithms.",,,"Let 



Ω
=


X


×


Y


=
{
(
x
,
y
)
}


{\displaystyle \Omega ={\mathcal {X}}\times {\mathcal {Y}}=\{(x,y)\}}

 be the sample space, where 



y


{\displaystyle y}

 are the labels and 



x


{\displaystyle x}

 are the covariates (predictors). 





F


=
{
f
:


X


↦


Y


}


{\displaystyle {\mathcal {F}}=\{f:{\mathcal {X}}\mapsto {\mathcal {Y}}\}}

 is a collection of mappings (functions) under consideration to link 



x


{\displaystyle x}

 to 



y


{\displaystyle y}

. 



L
:


Y


×


Y


↦

R



{\displaystyle L:{\mathcal {Y}}\times {\mathcal {Y}}\mapsto \mathbb {R} }

 is a pre-given loss function (usually non-negative). Given a probability distribution 



P
(
x
,
y
)


{\displaystyle P(x,y)}

 on 



Ω


{\displaystyle \Omega }

, define the expected risk 




I

P


(
f
)


{\displaystyle I_{P}(f)}

 to be:","The general goal in statistical learning is to find the function in 





F




{\displaystyle {\mathcal {F}}}

 that minimizes the expected risk. That is, to find solutions to the following problem:[1]","But in practice the distribution 



P


{\displaystyle P}

 is unknown, and any learning task can only be based on finite samples. Thus we seek instead to find an algorithm that asymptotically minimizes the empirical risk, i.e., to find a sequence of functions 



{




f
^




n



}

n
=
1


∞




{\displaystyle \{{\hat {f}}_{n}\}_{n=1}^{\infty }}

 that satisfies",One usual algorithm to find such a sequence is through empirical risk minimization.,We can make the condition given in the above equation stronger by requiring that the convergence is uniform for all probability distributions. That is:,"




lim

n
→
∞



sup

P



P

(

I

P


(




f
^




n


)
−

inf

f
∈


F





I

P


(
f
)
>
ϵ
)
=
0


{\displaystyle \lim _{n\rightarrow \infty }\sup _{P}\mathbb {P} (I_{P}({\hat {f}}_{n})-\inf _{f\in {\mathcal {F}}}I_{P}(f)>\epsilon )=0}

",, , , , ,(1),"The intuition behind the more strict requirement is as such: the rate at which sequence 



{




f
^




n


}


{\displaystyle \{{\hat {f}}_{n}\}}

 converges to the minimizer of the expected risk can be very different for different 



P
(
x
,
y
)


{\displaystyle P(x,y)}

. Because in real world the true distribution 



P


{\displaystyle P}

 is always unknown, we would want to select a sequence that performs well under all cases.","However, by the no free lunch theorem, such a sequence that satisfies (1) does not exist if 





F




{\displaystyle {\mathcal {F}}}

 is too complex. This means we need to be careful and not allow too ""many"" functions in 





F




{\displaystyle {\mathcal {F}}}

 if we want (1) to be a meaningful requirement. Specifically, function classes that ensure the existence of a sequence 



{




f
^




n


}


{\displaystyle \{{\hat {f}}_{n}\}}

 that satisfies (1) are known as learnable classes.[1]","It is worth noting that at least for supervised classification and regression problems, if a function class is learnable, then the empirical risk minimization automatically satisfies (1).[2] Thus in these settings not only do we know that the problem posed by (1) is solvable, we also immediately have an algorithm that gives the solution.","If the true relationship between 



y


{\displaystyle y}

 and 



x


{\displaystyle x}

 is 



y
∼

f

∗


(
x
)


{\displaystyle y\sim f^{*}(x)}

, then by selecting the appropriate loss function, 




f

∗




{\displaystyle f^{*}}

 can always be expressed as the minimizer of the expected loss across all possible functions. That is,","Here we let 






F



∗




{\displaystyle {\mathcal {F}}^{*}}

 be the collection of all possible functions mapping 





X




{\displaystyle {\mathcal {X}}}

 onto 





Y




{\displaystyle {\mathcal {Y}}}

. 




f

∗




{\displaystyle f^{*}}

 can be interpreted as the actual data generating mechanism. However, the no free lunch theorem tells us that in practice, with finite samples we cannot hope to search for the expected risk minimizer over 






F



∗




{\displaystyle {\mathcal {F}}^{*}}

. Thus we often consider a subset of 






F



∗




{\displaystyle {\mathcal {F}}^{*}}

, 





F




{\displaystyle {\mathcal {F}}}

, to carry out searches on. By doing so, we risk that 




f

∗




{\displaystyle f^{*}}

 might not be an element of 





F




{\displaystyle {\mathcal {F}}}

. This tradeoff can be mathematically expressed as","




I

P


(




f
^




n


)
−

inf

f
∈



F



∗





I

P


(
f
)
=





I

P


(




f
^




n


)
−

inf

f
∈


F





I

P


(
f
)

⏟



(
a
)


+





inf

f
∈


F





I

P


(
f
)
−

inf

f
∈



F



∗





I

P


(
f
)

⏟



(
b
)




{\displaystyle I_{P}({\hat {f}}_{n})-\inf _{f\in {\mathcal {F}}^{*}}I_{P}(f)=\underbrace {I_{P}({\hat {f}}_{n})-\inf _{f\in {\mathcal {F}}}I_{P}(f)} _{(a)}+\underbrace {\inf _{f\in {\mathcal {F}}}I_{P}(f)-\inf _{f\in {\mathcal {F}}^{*}}I_{P}(f)} _{(b)}}

",, , , , ,(2),"In the above decomposition, part 



(
b
)


{\displaystyle (b)}

 does not depend on the data and is non-stochastic. It describes how far away our assumptions (





F




{\displaystyle {\mathcal {F}}}

) are from the truth (






F



∗




{\displaystyle {\mathcal {F}}^{*}}

). 



(
b
)


{\displaystyle (b)}

 will be strictly greater than 0 if we make assumptions that are too strong (





F




{\displaystyle {\mathcal {F}}}

 too small). On the other hand, failing to put enough restrictions on 





F




{\displaystyle {\mathcal {F}}}

 will cause it to be not learnable, and part 



(
a
)


{\displaystyle (a)}

 will not stochastically converge to 0. This is the well-known overfitting problem in statistics and machine learning literature.","A good example where learnable classes are used is the so-called Tikhonov regularization in reproducing kernel Hilbert space (RKHS). Specifically, let 






F

∗






{\displaystyle {\mathcal {F^{*}}}}

 be an RKHS, and 




|


|

⋅

|



|


2




{\displaystyle ||\cdot ||_{2}}

 be the norm on 






F

∗






{\displaystyle {\mathcal {F^{*}}}}

 given by its inner product. It is shown in [3] that 





F


=
{
f
:

|


|

f

|



|


2


≤
γ
}


{\displaystyle {\mathcal {F}}=\{f:||f||_{2}\leq \gamma \}}

 is a learnable class for any finite, positive 



γ


{\displaystyle \gamma }

. The empirical minimization algorithm to the dual form[disambiguation needed] of this problem is","This was first introduced by Tikhonov[4] to solve ill-posed problems. Many statistical learning algorithms can be expressed in such a form (for example, the well-known ridge regression).","The tradeoff between 



(
a
)


{\displaystyle (a)}

 and 



(
b
)


{\displaystyle (b)}

 in (2) is geometrically more intuitive with Tikhonov regularization in RKHS. We can consider a sequence of 



{



F



γ


}


{\displaystyle \{{\mathcal {F}}_{\gamma }\}}

, which are essentially balls in 






F

∗






{\displaystyle {\mathcal {F^{*}}}}

 with centers at 0. As 



γ


{\displaystyle \gamma }

 gets larger, 






F



γ




{\displaystyle {\mathcal {F}}_{\gamma }}

 gets closer to the entire space, and 



(
b
)


{\displaystyle (b)}

 is likely to become smaller. However we will also suffer smaller convergence rates in 



(
a
)


{\displaystyle (a)}

. The way to choose an optimal 



γ


{\displaystyle \gamma }

 in finite sample settings is usually through cross-validation.","Part 



(
a
)


{\displaystyle (a)}

 in (2) is closely linked to empirical process theory in statistics, where the empirical risk 



{

∑

i
=
1


n


L
(

y

i


,
f
(

x

i


)
)
,
f
∈


F


}


{\displaystyle \{\sum _{i=1}^{n}L(y_{i},f(x_{i})),f\in {\mathcal {F}}\}}

 are known as empirical processes.[5] In this field, the function class 





F




{\displaystyle {\mathcal {F}}}

 that satisfies the stochastic convergence","




sup

P



E


sup

f
∈


F





|


∑

i
=
1


n


L
(

y

i


,
f
(

x

i


)
)
−

I

P


(
f
)

|

=
0


{\displaystyle \sup _{P}\mathbb {E} \sup _{f\in {\mathcal {F}}}|\sum _{i=1}^{n}L(y_{i},f(x_{i}))-I_{P}(f)|=0}

",, , , , ,(3),"are known as uniform Glivenko–Cantelli classes. It has been shown that under certain regularity conditions, learnable classes and uniformly Glivenko-Cantelli classes are equivalent.[1] Interplay between 



(
a
)


{\displaystyle (a)}

 and 



(
b
)


{\displaystyle (b)}

 in statistics literature is often known as the bias-variance tradeoff.","However, note that in [2] the authors gave an example of stochastic convex optimization for General Setting of Learning where learnability is not equivalent with uniform convergence."
"Learning automata is one type of Machine Learning algorithm studied since 1970s. Compared to other learning scheme, a branch of the theory of adaptive control is devoted to learning automata surveyed by Narendra and Thathachar (1974) which were originally described explicitly as finite state automata. Learning automata select their current action based on past experiences from the environment. It will fall into the range of reinforcement learning if the environment is stochastic and Markov Decision Process (MDP is used).",,,"Research in learning automata can be traced back to the work of Tsetlin in the early 1960s in the Soviet Union. Together with some colleagues, he published a collection of papers on how to use matrices to describe automata functions. Additionally, Tsetlin worked on reasonable and collective automata behaviour, and on automata games. Learning automata were also investigated by researches in the United States in the 1960s. However, the term learning automaton was not used until Narendra and Thathachar introduced it in a survey paper in 1974.",A learning automaton is an adaptive decision-making unit situated in a random environment that learns the optimal action through repeated interactions with its environment. The actions are chosen according to a specific probability distribution which is updated based on the environment response the automaton obtains by performing a particular action.,"With respect to the field of reinforcement learning, learning automata are characterized as policy iterators. In contrast to other reinforcement learners, policy iterators directly manipulate the policy π. Another example for policy iterators are evolutionary algorithms.","Formally, Narendra and Thathachar define a stochastic automaton to consist of:","In their paper, they investigate only stochastic automata with r=s and G being bijective, allowing them to confuse actions and states. The states of such an automaton correspond to the states of a ""discrete-state discrete-parameter Markov process"".[1] At each time step t=0,1,2,3,..., the automaton reads an input from its environment, updates p(t) to p(t+1) by A, randomly chooses a successor state according to the probabilities p(t+1) and outputs the corresponding action. The automaton's environment, in turn, reads the action and sends the next input to the automaton. Frequently, the input set x = { 0,1 } is used, with 0 and 1 corresponding to a nonpenalty and a penalty response of the environment, respectively; in this case, the automaton should learn to minimize the number of penalty responses, and the feedback loop of automaton and environment is called a ""P-model"". More generally, a ""Q-model"" allows an arbitrary finite input set x, and an ""S-model"" uses the interval [0,1] of real numbers as x.[2]","Finite action-set learning automata (FALA) are a class of learning automata for which the number of possible actions is finite or, in more mathematical terms, for which the size of the action-set is finite."
"Learning to rank[1] or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems.[2] Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. ""relevant"" or ""not relevant"") for each item. The ranking model's purpose is to rank, i.e. produce a permutation of items in new, unseen lists in a way which is ""similar"" to rankings in the training data in some sense.",,,"Ranking is a central part of many information retrieval problems, such as document retrieval, collaborative filtering, sentiment analysis, and online advertising.",A possible architecture of a machine-learned search engine is shown in the figure to the right.,"Training data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human assessors (or raters, as Google calls them), who check results for some queries and determine relevance of each result. It is not feasible to check relevance of all documents, and so typically a technique called pooling is used — only the top few documents, retrieved by some existing ranking models are checked. Alternatively, training data may be derived automatically by analyzing clickthrough logs (i.e. search results which got clicks from users),[3] query chains,[4] or such search engines' features as Google's SearchWiki.",Training data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries.,"Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.[5] First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the vector space model, boolean model, weighted AND,[6] and BM25. This phase is called top-



k


{\displaystyle k}

 document retrieval and many heuristics were proposed in the literature to accelerate it, such as using a document's static quality score and tiered indexes.[7] In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.",Learning to rank algorithms have been applied in areas other than information retrieval:,"For convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called feature vectors. Such an approach is sometimes called bag of features and is analogous to the bag of words model and vector space model used in information retrieval for representation of documents.","Components of such vectors are called features, factors or ranking signals. They may be divided into three groups (features from document retrieval are shown as examples):","Some examples of features, which were used in the well-known LETOR dataset:[12]","Selecting and designing good features is an important area in machine learning, which is called feature engineering.",There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.,Examples of ranking quality measures:,"DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.[13] Other metrics such as MAP, MRR and precision, are defined only for binary judgements.","Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:","Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.","Tie-Yan Liu of Microsoft Research Asia in his paper ""Learning to Rank for Information Retrieval""[1] and talks at several leading conferences has analyzed existing algorithms for learning to rank problems and categorized them into three groups by their input representation and loss function:","In this case it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score.","A number of existing supervised machine learning algorithms can be readily used for this purpose. Ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict score of a single query-document pair, and it takes a small, finite number of values.",In this case learning-to-rank problem is approximated by a classification problem — learning a binary classifier that can tell which document is better in a given pair of documents. The goal is to minimize average number of inversions in ranking.,"These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.",A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:,Regularized least-squares based ranking. The work is extended in [20] to learning to rank from general preference graphs.,"Note: as most supervised learning algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.","Norbert Fuhr introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;[26] a specific variant of this approach (using polynomial regression) had been published by him three years earlier.[16] Bill Cooper proposed logistic regression for the same purpose in 1992 [17] and used it with his Berkeley research group to train a successful ranking function for TREC. Manning et al.[27] suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.","Several conferences, such as NIPS, SIGIR and ICML had workshops devoted to the learning-to-rank problem since mid-2000s (decade).","Commercial web search engines began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was AltaVista (later its technology was acquired by Overture, and then Yahoo), which launched a gradient boosting-trained ranking function in April 2003.[28][29]","Bing's search is said to be powered by RankNet algorithm,[30][when?] which was invented at Microsoft Research in 2005.","In November 2009 a Russian search engine Yandex announced[31] that it had significantly increased its search quality due to deployment of a new proprietary MatrixNet algorithm, a variant of gradient boosting method which uses oblivious decision trees.[32] Recently they have also sponsored a machine-learned ranking competition ""Internet Mathematics 2009""[33] based on their own search engine's production data. Yahoo has announced a similar competition in 2010.[34]","As of 2008, Google's Peter Norvig denied that their search engine exclusively relies on machine-learned ranking.[35] Cuil's CEO, Tom Costello, suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models ""learn what people say they like, not what people actually like"".[36]"
"Learning with errors (LWE) is a problem in machine learning that is conjectured to be hard to solve. Introduced[1] by Oded Regev in 2005, it is a generalization of the parity learning problem. Regev showed, furthermore, that the LWE problem is as hard to solve as several worst-case lattice problems. The LWE problem has recently[1][2] been used as a hardness assumption to create public-key cryptosystems, such as the ring learning with errors key exchange by Peikert.[3]","An algorithm is said to solve the LWE problem if, when given access to samples 



(
x
,
y
)


{\displaystyle (x,y)}

 where 



x
∈


Z


q


n




{\displaystyle x\in \mathbb {Z} _{q}^{n}}

 and 



y
∈


Z


q




{\displaystyle y\in \mathbb {Z} _{q}}

, with the assurance, for some fixed linear function 



f
:


Z


q


n


→


Z


q


,


{\displaystyle f:\mathbb {Z} _{q}^{n}\rightarrow \mathbb {Z} _{q},}

 that 



y
=
f
(
x
)


{\displaystyle y=f(x)}

 with high probability and deviates from it according to some known noise model, the algorithm can recreate 



f


{\displaystyle f}

 or some close approximation of it with high probability.",,,"Denote by 




T

=

R


/


Z



{\displaystyle \mathbb {T} =\mathbb {R} /\mathbb {Z} }

 the additive group on reals modulo one. Denote by 




A


s

,
ϕ




{\displaystyle A_{\mathbf {s} ,\phi }}

 the distribution on 





Z


q


n


×

T



{\displaystyle \mathbb {Z} _{q}^{n}\times \mathbb {T} }

 obtained by choosing a vector 




a

∈


Z


q


n




{\displaystyle \mathbf {a} \in \mathbb {Z} _{q}^{n}}

 uniformly at random, choosing 



e


{\displaystyle e}

 according to a probability distribution 



ϕ


{\displaystyle \phi }

 on 




T



{\displaystyle \mathbb {T} }

 and outputting 



(

a

,
⟨

a

,

s

⟩

/

q
+
e
)


{\displaystyle (\mathbf {a} ,\langle \mathbf {a} ,\mathbf {s} \rangle /q+e)}

 for some fixed vector 




s

∈


Z


q


n




{\displaystyle \mathbf {s} \in \mathbb {Z} _{q}^{n}}

. Here 




⟨

a

,

s

⟩
=

∑

i
=
1


n



a

i



s

i





{\displaystyle \textstyle \langle \mathbf {a} ,\mathbf {s} \rangle =\sum _{i=1}^{n}a_{i}s_{i}}

 is the standard inner product 





Z


q


n


×


Z


q


n


⟶


Z


q




{\displaystyle \mathbb {Z} _{q}^{n}\times \mathbb {Z} _{q}^{n}\longrightarrow \mathbb {Z} _{q}}

, the division is done in the field of reals (or more formally, this ""division by 



q


{\displaystyle q}

"" is notation for the group homomorphism 





Z


q


⟶

T



{\displaystyle \mathbb {Z} _{q}\longrightarrow \mathbb {T} }

 mapping 



1
∈


Z


q




{\displaystyle 1\in \mathbb {Z} _{q}}

 to 



1

/

q
+

Z

∈

T



{\displaystyle 1/q+\mathbb {Z} \in \mathbb {T} }

), and the final addition is in 




T



{\displaystyle \mathbb {T} }

.","The learning with errors problem 





L
W
E


q
,
ϕ




{\displaystyle \mathrm {LWE} _{q,\phi }}

 is to find 




s

∈


Z


q


n




{\displaystyle \mathbf {s} \in \mathbb {Z} _{q}^{n}}

, given access to polynomially many samples of choice from 




A


s

,
ϕ




{\displaystyle A_{\mathbf {s} ,\phi }}

.","For every 



α
>
0


{\displaystyle \alpha >0}

, denote by 




D

α




{\displaystyle D_{\alpha }}

 the one-dimensional Gaussian with density function 




D

α


(
x
)
=

ρ

α


(
x
)

/

α


{\displaystyle D_{\alpha }(x)=\rho _{\alpha }(x)/\alpha }

 where 




ρ

α


(
x
)
=

e

−
π
(

|

x

|


/

α

)

2






{\displaystyle \rho _{\alpha }(x)=e^{-\pi (|x|/\alpha )^{2}}}

, and let 




Ψ

α




{\displaystyle \Psi _{\alpha }}

 be the distribution on 




T



{\displaystyle \mathbb {T} }

 obtained by considering 




D

α




{\displaystyle D_{\alpha }}

 modulo one. The version of LWE considered in most of the results would be 





L
W
E


q
,

Ψ

α






{\displaystyle \mathrm {LWE} _{q,\Psi _{\alpha }}}

","The LWE problem described above is the search version of the problem. In the decision version (DLWE), the goal is to distinguish between noisy inner products and uniformly random samples from 





Z


q


n


×

T



{\displaystyle \mathbb {Z} _{q}^{n}\times \mathbb {T} }

 (practically, some discretized version of it). Regev[1] showed that the decision and search versions are equivalent when 



q


{\displaystyle q}

 is a prime bounded by some polynomial in 



n


{\displaystyle n}

.","Intuitively, if we have a procedure for the search problem, the decision version can be solved easily: just feed the input samples for the decision problem to the solver for the search problem. Denote the given samples by 



{
(


a

i



,


b

i



)
}
⊂


Z


q


n


×

T



{\displaystyle \{(\mathbf {a_{i}} ,\mathbf {b_{i}} )\}\subset \mathbb {Z} _{q}^{n}\times \mathbb {T} }

. If the solver returns a candidate 




s



{\displaystyle \mathbf {s} }

, for all 



i


{\displaystyle i}

, calculate 



{
⟨


a

i



,

s

⟩
−


b

i



}


{\displaystyle \{\langle \mathbf {a_{i}} ,\mathbf {s} \rangle -\mathbf {b_{i}} \}}

. If the samples are from an LWE distribution, then the results of this calculation will be distributed according 



χ


{\displaystyle \chi }

, but if the samples are uniformly random, these quantities will be distributed uniformly as well.","For the other direction, given a solver for the decision problem, the search version can be solved as follows: Recover 




s



{\displaystyle \mathbf {s} }

 one coordinate at a time. To obtain the first coordinate, 





s


1




{\displaystyle \mathbf {s} _{1}}

, make a guess 



k
∈

Z

q




{\displaystyle k\in Z_{q}}

, and do the following. Choose a number 



r
∈


Z


q




{\displaystyle r\in \mathbb {Z} _{q}}

 uniformly at random. Transform the given samples 



{
(


a

i



,


b

i



)
}
⊂


Z


q


n


×

T



{\displaystyle \{(\mathbf {a_{i}} ,\mathbf {b_{i}} )\}\subset \mathbb {Z} _{q}^{n}\times \mathbb {T} }

 as follows. Calculate 



{
(


a

i



+
(
r
,
0
,
…
,
0
)
,


b

i



+
(
r
k
)

/

q
)
}


{\displaystyle \{(\mathbf {a_{i}} +(r,0,\ldots ,0),\mathbf {b_{i}} +(rk)/q)\}}

. Send the transformed samples to the decision solver.","If the guess 



k


{\displaystyle k}

 was correct, the transformation takes the distribution 




A


s

,
χ




{\displaystyle A_{\mathbf {s} ,\chi }}

 to itself, and otherwise, since 



q


{\displaystyle q}

 is prime, it takes it to the uniform distribution. So, given a polynomial-time solver for the decision problem that errs with very small probability, since 



q


{\displaystyle q}

 is bounded by some polynomial in 



n


{\displaystyle n}

, it only takes polynomial time to guess every possible value for 



k


{\displaystyle k}

 and use the solver to see which one is correct.","After obtaining 





s


1




{\displaystyle \mathbf {s} _{1}}

, we follow an analogous procedure for each other coordinate 





s


j




{\displaystyle \mathbf {s} _{j}}

. Namely, we transform our 





b

i





{\displaystyle \mathbf {b_{i}} }

 samples the same way, and transform our 





a

i





{\displaystyle \mathbf {a_{i}} }

 samples by calculating 





a

i



+
(
0
,
…
,
r
,
…
,
0
)


{\displaystyle \mathbf {a_{i}} +(0,\ldots ,r,\ldots ,0)}

, where the 



r


{\displaystyle r}

 is in the 




j

t
h




{\displaystyle j^{th}}

 coordinate.[1]","Peikert[2] showed that this reduction, with a small modification, works for any 



q


{\displaystyle q}

 that is a product of distinct, small (polynomial in 



n


{\displaystyle n}

) primes. The main idea is if 



q
=

q

1



q

2


⋯

q

t




{\displaystyle q=q_{1}q_{2}\cdots q_{t}}

, for each 




q

ℓ




{\displaystyle q_{\ell }}

, guess and check to see if 





s


j




{\displaystyle \mathbf {s} _{j}}

 is congruent to 



0

mod



q

ℓ




{\displaystyle 0\mod q_{\ell }}

, and then use the Chinese remainder theorem to recover 





s


j




{\displaystyle \mathbf {s} _{j}}

.","Regev[1] showed the Random self-reducibility of the LWE and DLWE problems for arbitrary 



q


{\displaystyle q}

 and 



χ


{\displaystyle \chi }

. Given samples 



{
(


a

i



,


b

i



)
}


{\displaystyle \{(\mathbf {a_{i}} ,\mathbf {b_{i}} )\}}

 from 




A


s

,
χ




{\displaystyle A_{\mathbf {s} ,\chi }}

, it is easy to see that 



{
(


a

i



,


b

i



+
⟨


a

i



,

t

⟩
)

/

q
}


{\displaystyle \{(\mathbf {a_{i}} ,\mathbf {b_{i}} +\langle \mathbf {a_{i}} ,\mathbf {t} \rangle )/q\}}

 are samples from 




A


s

+

t

,
χ




{\displaystyle A_{\mathbf {s} +\mathbf {t} ,\chi }}

.","So, suppose there was some set 





S


⊂


Z


q


n




{\displaystyle {\mathcal {S}}\subset \mathbb {Z} _{q}^{n}}

 such that 




|



S



|


/


|



Z


q


n



|

=
1

/

p
o
l
y
(
n
)


{\displaystyle |{\mathcal {S}}|/|\mathbb {Z} _{q}^{n}|=1/poly(n)}

, and for distributions 




A



s
′


,
χ




{\displaystyle A_{\mathbf {s'} ,\chi }}

, with 





s
′


←


S




{\displaystyle \mathbf {s'} \leftarrow {\mathcal {S}}}

, DLWE was easy.","Then there would be some distinguisher 





A




{\displaystyle {\mathcal {A}}}

, who, given samples 



{
(


a

i



,


b

i



)
}


{\displaystyle \{(\mathbf {a_{i}} ,\mathbf {b_{i}} )\}}

, could tell whether they were uniformly random or from 




A



s
′


,
χ




{\displaystyle A_{\mathbf {s'} ,\chi }}

. If we need to distinguish uniformly random samples from 




A


s

,
χ




{\displaystyle A_{\mathbf {s} ,\chi }}

, where 




s



{\displaystyle \mathbf {s} }

 is chosen uniformly at random from 





Z


q


n




{\displaystyle \mathbb {Z} _{q}^{n}}

, we could simply try different values 




t



{\displaystyle \mathbf {t} }

 sampled uniformly at random from 





Z


q


n




{\displaystyle \mathbb {Z} _{q}^{n}}

, calculate 



{
(


a

i



,


b

i



+
⟨


a

i



,

t

⟩
)

/

q
}


{\displaystyle \{(\mathbf {a_{i}} ,\mathbf {b_{i}} +\langle \mathbf {a_{i}} ,\mathbf {t} \rangle )/q\}}

 and feed these samples to 





A




{\displaystyle {\mathcal {A}}}

. Since 





S




{\displaystyle {\mathcal {S}}}

 comprises a large fraction of 





Z


q


n




{\displaystyle \mathbb {Z} _{q}^{n}}

, with high probability, if we choose a polynomial number of values for 




t



{\displaystyle \mathbf {t} }

, we will find one such that 




s

+

t

∈


S




{\displaystyle \mathbf {s} +\mathbf {t} \in {\mathcal {S}}}

, and 





A




{\displaystyle {\mathcal {A}}}

 will successfully distinguish the samples.","Thus, no such 





S




{\displaystyle {\mathcal {S}}}

 can exist, meaning LWE and DLWE are (up to a polynomial factor) as hard in the average case as they are in the worst case.","For a n-dimensional lattice 



L


{\displaystyle L}

, let smoothing parameter 




η

ϵ


(
L
)


{\displaystyle \eta _{\epsilon }(L)}

 denote the smallest 



s


{\displaystyle s}

 such that 




ρ

1

/

s


(

L

∗


∖
{

0

}
)
≤
ϵ


{\displaystyle \rho _{1/s}(L^{*}\setminus \{\mathbf {0} \})\leq \epsilon }

 where 




L

∗




{\displaystyle L^{*}}

 is the dual of 



L


{\displaystyle L}

 and 




ρ

α


(
x
)
=

e

−
π
(

|

x

|


/

α

)

2






{\displaystyle \rho _{\alpha }(x)=e^{-\pi (|x|/\alpha )^{2}}}

 is extended to sets by summing over function values at each element in the set. Let 




D

L
,
r




{\displaystyle D_{L,r}}

 denote the discrete Gaussian distribution on 



L


{\displaystyle L}

 of width 



r


{\displaystyle r}

 for a lattice 



L


{\displaystyle L}

 and real 



r
>
0


{\displaystyle r>0}

. The probability of each 



x
∈
L


{\displaystyle x\in L}

 is proportional to 




ρ

r


(
x
)


{\displaystyle \rho _{r}(x)}

.","The discrete Gaussian sampling problem(DGS) is defined as follows: An instance of 



D
G

S

ϕ




{\displaystyle DGS_{\phi }}

 is given by an 



n


{\displaystyle n}

-dimensional lattice 



L


{\displaystyle L}

 and a number 



r
≥
ϕ
(
L
)


{\displaystyle r\geq \phi (L)}

. The goal is to output a sample from 




D

L
,
r




{\displaystyle D_{L,r}}

. Regev shows that there is a reduction from 



G
a
p
S
V

P

100


n


γ
(
n
)




{\displaystyle GapSVP_{100{\sqrt {n}}\gamma (n)}}

 to 



D
G

S



n


γ
(
n
)

/

λ
(

L

∗


)




{\displaystyle DGS_{{\sqrt {n}}\gamma (n)/\lambda (L^{*})}}

 for any function 



γ
(
n
)


{\displaystyle \gamma (n)}

.","Regev then shows that there exists an efficient quantum algorithm for 



D
G

S



2
n



η

ϵ


(
L
)

/

α




{\displaystyle DGS_{{\sqrt {2n}}\eta _{\epsilon }(L)/\alpha }}

 given access to an oracle for 



L
W

E

q
,

Ψ

α






{\displaystyle LWE_{q,\Psi _{\alpha }}}

 for integer 



q


{\displaystyle q}

 and 



α
∈
(
0
,
1
)


{\displaystyle \alpha \in (0,1)}

 such that 



α
q
>
2


n




{\displaystyle \alpha q>2{\sqrt {n}}}

. This implies the hardness for 



L
W
E


{\displaystyle LWE}

. Although the proof of this assertion works for any 



q


{\displaystyle q}

, for creating a cryptosystem, the 



q


{\displaystyle q}

 has to be polynomial in 



n


{\displaystyle n}

.","Peikert proves[2] that there is a probabilistic polynomial time reduction from the 



G
a
p
S
V

P

ζ
,
γ




{\displaystyle GapSVP_{\zeta ,\gamma }}

 problem in the worst case to solving 



L
W

E

q
,

Ψ

α






{\displaystyle LWE_{q,\Psi _{\alpha }}}

 using 



p
o
l
y
(
n
)


{\displaystyle poly(n)}

 samples for parameters 



α
∈
(
0
,
1
)


{\displaystyle \alpha \in (0,1)}

, 



γ
(
n
)
≥
n

/

(
α


log
⁡

n



)


{\displaystyle \gamma (n)\geq n/(\alpha {\sqrt {\log {n}}})}

, 



ζ
(
n
)
≥
γ
(
n
)


{\displaystyle \zeta (n)\geq \gamma (n)}

 and 



q
≥
(
ζ

/



n


)
ω


log
⁡

n



)


{\displaystyle q\geq (\zeta /{\sqrt {n}})\omega {\sqrt {\log {n}}})}

.","The LWE problem serves as a versatile problem used in construction of several[1][2][4][5] cryptosystems. In 2005, Regev[1] showed that the decision version of LWE is hard assuming quantum hardness of the lattice problems 



G
a
p
S
V

P

γ




{\displaystyle GapSVP_{\gamma }}

 (for 



γ


{\displaystyle \gamma }

 as above) and 



S
I
V

P

t




{\displaystyle SIVP_{t}}

 with t=Õ(n/



α


{\displaystyle \alpha }

). In 2009, Peikert[2] proved a similar result assuming only the classical hardness of the related problem 



G
a
p
S
V

P

ζ
,
γ




{\displaystyle GapSVP_{\zeta ,\gamma }}

. The disadvantage of Peikert's result is that it bases itself on a non-standard version of an easier (when compared to SIVP) problem GapSVP.","Regev[1] proposed a public-key cryptosystem based on the hardness of the LWE problem. The cryptosystem as well as the proof of security and correctness are completely classical. The system is characterized by 



m
,
q


{\displaystyle m,q}

 and a probability distribution 



χ


{\displaystyle \chi }

 on 




T



{\displaystyle \mathbb {T} }

. The setting of the parameters used in proofs of correctness and security is",The cryptosystem is then defined by:,"The proof of correctness follows from choice of parameters and some probability analysis. The proof of security is by reduction to the decision version of LWE: an algorithm for distinguishing between encryptions (with above parameters) of 



0


{\displaystyle 0}

 and 



1


{\displaystyle 1}

 can be used to distinguish between 




A

s
,
χ




{\displaystyle A_{s,\chi }}

 and the uniform distribution over 





Z


q


n


×


Z


q




{\displaystyle \mathbb {Z} _{q}^{n}\times \mathbb {Z} _{q}}

",Peikert[2] proposed a system that is secure even against any chosen-ciphertext attack.
"



∀
i
∈
{
1
,
.
.
.
,
m
}
,


P


S


{

sup

z
∈
Z



|

V
(

f

S


,

z

i


)
−
V
(

f


S


|

i




,

z

i


)

|

≤

β

C
V


}
≥
1
−

δ

C
V




{\displaystyle \forall i\in \{1,...,m\},\mathbb {P} _{S}\{\sup _{z\in Z}|V(f_{S},z_{i})-V(f_{S^{|i}},z_{i})|\leq \beta _{CV}\}\geq 1-\delta _{CV}}

","



∀
i
∈
{
1
,
.
.
.
,
m
}
,


P


S


{

|

I
[

f

S


]
−


1
m



∑

i
=
1


m


V
(

f


S


|

i




,

z

i


)

|

≤

β

E
L


m


}
≥
1
−

δ

E
L


m




{\displaystyle \forall i\in \{1,...,m\},\mathbb {P} _{S}\{|I[f_{S}]-{\frac {1}{m}}\sum _{i=1}^{m}V(f_{S^{|i}},z_{i})|\leq \beta _{EL}^{m}\}\geq 1-\delta _{EL}^{m}}

, with 




β

E
L


m




{\displaystyle \beta _{EL}^{m}}

and 




δ

E
L


m




{\displaystyle \delta _{EL}^{m}}

 going to zero for 



n
→
inf


{\displaystyle n\rightarrow \inf }

","X and Y ⊂ R being respectively an input and an output space, we consider a training set","



S
=
{

z

1


=
(

x

1


,
 

y

1


)
 
,
.
.
,
 

z

m


=
(

x

m


,
 

y

m


)
}


{\displaystyle S=\{z_{1}=(x_{1},\ y_{1})\ ,..,\ z_{m}=(x_{m},\ y_{m})\}}

 of size m in 



Z
=
X
×
Y


{\displaystyle Z=X\times Y}

 drawn i.i.d. from an unknown distribution D. A learning algorithm is a function 



f


{\displaystyle f}

 from 




Z

m




{\displaystyle Z_{m}}

 into 



F
⊂
Y
X


{\displaystyle F\subset YX}

which maps a learning set S onto a function 




f

S




{\displaystyle f_{S}}

 from X to Y. To avoid complex notation, we consider only deterministic algorithms. It is also assumed that the algorithm 



f


{\displaystyle f}

 is symmetric with respect to S, i.e. it does not depend on the order of the elements in the training set. Furthermore, we assume that all functions are measurable and all sets are countable which does not limit the interest of the results presented here.","The loss of an hypothesis f with respect to an example 



z
=
(
x
,
y
)


{\displaystyle z=(x,y)}

 is then defined as 



V
(
f
,
z
)
=
V
(
f
(
x
)
,
y
)


{\displaystyle V(f,z)=V(f(x),y)}

. The empirical error of f is 




I

S


[
f
]
=


1
n


∑
V
(
f
,

z

i


)


{\displaystyle I_{S}[f]={\frac {1}{n}}\sum V(f,z_{i})}

.","The true error of f is 



I
[
f
]
=


E


z


V
(
f
,
z
)


{\displaystyle I[f]=\mathbb {E} _{z}V(f,z)}

","Given a training set S of size m, we will build, for all i = 1....,m, modified training sets as follows:","




S


|

i


=
{

z

1


,
.
.
.
,
 

z

i
−
1


,
 

z

i
+
1


,
.
.
.
,
 

z

m


}


{\displaystyle S^{|i}=\{z_{1},...,\ z_{i-1},\ z_{i+1},...,\ z_{m}\}}

","




S

i


=
{

z

1


,
.
.
.
,
 

z

i
−
1


,
 

z

i

′

,
 

z

i
+
1


,
.
.
.
,
 

z

m


}


{\displaystyle S^{i}=\{z_{1},...,\ z_{i-1},\ z_{i}',\ z_{i+1},...,\ z_{m}\}}

"
"In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable. Functions of this sort are standard in linear regression, where the coefficients are termed regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis. In many of these models, the coefficients are referred to as ""weights"".",,,"The basic form of a linear predictor function 



f
(
i
)


{\displaystyle f(i)}

 for data point i (consisting of p explanatory variables), for i = 1, ..., n, is","where 




β

0


,
…
,

β

p




{\displaystyle \beta _{0},\ldots ,\beta _{p}}

 are the coefficients (regression coefficients, weights, etc.) indicating the relative effect of a particular explanatory variable on the outcome.",It is common to write the predictor function in a more compact form as follows:,This makes it possible to write the linear predictor function as follows:,using the notation for a dot product between two vectors.,An equivalent form using matrix notation is as follows:,"where 




β



{\displaystyle {\boldsymbol {\beta }}}

 and 





x


i




{\displaystyle \mathbf {x} _{i}}

 are assumed to be a p-by-1 column vectors (as is standard when representing vectors as matrices), 





β



T





{\displaystyle {\boldsymbol {\beta }}^{\mathrm {T} }}

 indicates the matrix transpose of 




β



{\displaystyle {\boldsymbol {\beta }}}

 (which turns it into a 1-by-p row vector), and 





β



T





x


i




{\displaystyle {\boldsymbol {\beta }}^{\mathrm {T} }\mathbf {x} _{i}}

 indicates matrix multiplication between the 1-by-p row vector and the p-by-1 column vector, producing a 1-by-1 matrix that is taken to be a scalar.","An example of the usage of such a linear predictor function is in linear regression, where each data point is associated with a continuous outcome yi, and the relationship written","where 




ε

i




{\displaystyle \varepsilon _{i}}

 is a disturbance term or error variable — an unobserved random variable that adds noise to the linear relationship between the dependent variable and predictor function.","In some models (standard linear regression in particular), the equations for each of the data points i = 1, ..., n are stacked together and written in vector form as",where,"The matrix X is known as the design matrix and encodes all known information about the independent variables. The variables 




ε

i




{\displaystyle \varepsilon _{i}}

 are random variables, which in standard linear regression are distributed according to a standard normal distribution; they express the influence of any unknown factors on the outcome.","This makes it possible to find optimal coefficients through the method of least squares using simple matrix operations. In particular, the optimal coefficients 






β
^





{\displaystyle {\boldsymbol {\hat {\beta }}}}

 as estimated by least squares can be written as follows:","The matrix 



(

X


T



X

)

−
1



X


T





{\displaystyle (X^{\mathrm {T} }X)^{-1}X^{\mathrm {T} }}

 is known as the Moore-Penrose pseudoinverse of X. Note that this formula assumes that X is of full rank, i.e. there is no multicollinearity among different explanatory variables (i.e. one variable can be perfectly, or almost perfectly, predicted from another). In such cases, the singular value decomposition can be used to compute the pseudoinverse.","Although the outcomes (dependent variables) to be predicted are assumed to be random variables, the explanatory variables themselves are usually not assumed to be random. Instead, they are assumed to be fixed values, and any random variables (e.g. the outcomes) are assumed to be conditional on them. As a result, the model user is free to transform the explanatory variables in arbitrary ways, including creating multiple copies of a given explanatory variable, each transformed using a different function. Other common techniques are to create new explanatory variables in the form of interaction variables by taking products of two (or sometimes more) existing explanatory variables.","When a fixed set of nonlinear functions are used to transform the value(s) of a data point, these functions are known as basis functions. An example is polynomial regression, which uses a linear predictor function to fit an arbitrary degree polynomial relationship (up to a given order) between two sets of data points (i.e. a single real-valued explanatory variable and a related real-valued dependent variable), by adding multiple explanatory variables corresponding to various powers of the existing explanatory variable. Mathematically, the form looks like this:","In this case, for each data point, a set of explanatory variables is created as follows:",and then standard linear regression is run. The basis functions in this example would be,This example shows that a linear predictor function can actually be much more powerful than it first appears: It only really needs to be linear in the coefficients. All sorts of non-linear functions of the explanatory variables can be fit by the model.,"There is no particular need for the inputs to basis functions to be univariate or single-dimensional (or their outputs, for that matter, although in such a case, a K-dimensional output value is likely to be treated as K separate scalar-output basis functions). An example of this is radial basis functions (RBF's), which compute some transformed version of the distance to some fixed point:","An example is the Gaussian RBF, which has the same functional form as the normal distribution:",which drops off rapidly as the distance from c increases.,"A possible usage of RBF's is to create one for every observed data point. This means that the result of an RBF applied to a new data point will be close to 0 unless the new point is near to the point around which the RBF was applied. That is, the application of the radial basis functions will pick out the nearest point, and its regression coefficient will dominate. The result will be a form of nearest neighbor interpolation, where predictions are made by simply using the prediction of the nearest observed data point, possibly interpolating between multiple nearby data points when they are all similar distances away. This type of nearest neighbor method for prediction is often considered diametrically opposed to the type of prediction used in standard linear regression: But in fact, the transformations that can be applied to the explanatory variables in a linear predictor function are so powerful that even the nearest neighbor method can be implemented as a type of linear regression.","It is even possible to fit some functions that appear non-linear in the coefficients by transforming the coefficients into new coefficients that do appear linear. For example, a function of the form 



a
+

b

2



x

i
1


+


c



x

i
2




{\displaystyle a+b^{2}x_{i1}+{\sqrt {c}}x_{i2}}

 for coefficients 



a
,
b
,
c


{\displaystyle a,b,c}

 could be transformed into the appropriate linear function by applying the substitutions 




b
′

=

b

2


,

c
′

=


c


,


{\displaystyle b'=b^{2},c'={\sqrt {c}},}

 leading to 



a
+

b
′


x

i
1


+

c
′


x

i
2


,


{\displaystyle a+b'x_{i1}+c'x_{i2},}

 which is linear. Linear regression and similar techniques could be applied and will often still find the optimal coefficients, but their error estimates and such will be wrong.","The explanatory variables may be of any type: real-valued, binary, categorical, etc. The main distinction is between continuous variables (e.g. income, age, blood pressure, etc.) and discrete variables (e.g. sex, race, political party, etc.). Discrete variables referring to more than two possible choices are typically coded using dummy variables (or indicator variables), i.e. separate explanatory variables taking the value 0 or 1 are created for each possible value of the discrete variable, with a 1 meaning ""variable does have the given value"" and a 0 meaning ""variable does not have the given value"". For example, a four-way discrete variable of blood type with the possible values ""A, B, AB, O"" would be converted to separate two-way dummy variables, ""is-A, is-B, is-AB, is-O"", where only one of them has the value 1 and all the rest have the value 0. This allows for separate regression coefficients to be matched for each possible value of the discrete variable.","Note that, for K categories, not all K dummy variables are independent of each other. For example, in the above blood type example, only three of the four dummy variables are independent, in the sense that once the values of three of the variables are known, the fourth is automatically determined. Thus, it's really only necessary to encode three of the four possibilities as dummy variables, and in fact if all four possibilities are encoded, the overall model becomes non-identifiable. This causes problems for a number of methods, such as the simple closed-form solution used in linear regression. The solution is either to avoid such cases by eliminating one of the dummy variables, and/or introduce a regularization constraint (which necessitates a more powerful, typically iterative, method for finding the optimal coefficients)."
"In Euclidean geometry, linear separability is a geometric property of a pair of sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if line is replaced by hyperplane.","The problem of determining if a pair of sets is linearly separable and finding a separating hyperplane if they are arises in several areas. In statistics and machine learning, classifying certain types of data is a problem for which good algorithms exist that are based on this concept.",,,"Let 




X

0




{\displaystyle X_{0}}

 and 




X

1




{\displaystyle X_{1}}

 be two sets of points in an n-dimensional Euclidean space. Then 




X

0




{\displaystyle X_{0}}

 and 




X

1




{\displaystyle X_{1}}

 are linearly separable if there exists n + 1 real numbers 




w

1


,

w

2


,
.
.
,

w

n


,
k


{\displaystyle w_{1},w_{2},..,w_{n},k}

, such that every point 



x
∈

X

0




{\displaystyle x\in X_{0}}

 satisfies 




∑

i
=
1


n



w

i



x

i


>
k


{\displaystyle \sum _{i=1}^{n}w_{i}x_{i}>k}

 and every point 



x
∈

X

1




{\displaystyle x\in X_{1}}

 satisfies 




∑

i
=
1


n



w

i



x

i


<
k


{\displaystyle \sum _{i=1}^{n}w_{i}x_{i}<k}

, where 




x

i




{\displaystyle x_{i}}

 is the 



i


{\displaystyle i}

-th component of 



x


{\displaystyle x}

.","Equivalently, two sets are linearly separable precisely when their respective convex hulls are disjoint (colloquially, do not overlap).","Three non-collinear points in two classes ('+' and '-') are always linearly separable in two dimensions. This is illustrated by the three examples in the following figure (the all '+' case is not shown, but is similar to the all '-' case):","However, not all sets of four points, no three collinear, are linearly separable in two dimensions. The following example would need two straight lines and thus is not linearly separable:","Notice that three points which are collinear and of the form ""+ ⋅⋅⋅ — ⋅⋅⋅ +"" are also not linearly separable.",A Boolean function in n variables can be thought of as an assignment of 0 or 1 to each vertex of a Boolean hypercube in n dimensions. This gives a natural division of the vertices into two sets. The Boolean function is said to be linearly separable provided these two sets of points are linearly separable.,"Classifying data is a common task in machine learning. Suppose some data points, each belonging to one of two sets, are given and we wish to create a model that will decide which set a new data point will be in. In the case of support vector machines, a data point is viewed as a p-dimensional vector (a list of p numbers), and we want to know whether we can separate such points with a (p − 1)-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify (separate) the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two sets. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier.","More formally, given some training data 





D




{\displaystyle {\mathcal {D}}}

, a set of n points of the form","where the yi is either 1 or −1, indicating the set to which the point 





x


i




{\displaystyle \mathbf {x} _{i}}

 belongs. Each 





x


i




{\displaystyle \mathbf {x} _{i}}

 is a p-dimensional real vector. We want to find the maximum-margin hyperplane that divides the points having 




y

i


=
1


{\displaystyle y_{i}=1}

 from those having 




y

i


=
−
1


{\displaystyle y_{i}=-1}

. Any hyperplane can be written as the set of points 




x



{\displaystyle \mathbf {x} }

 satisfying","where 



⋅


{\displaystyle \cdot }

 denotes the dot product and 





w




{\displaystyle {\mathbf {w} }}

 the (not necessarily normalized) normal vector to the hyperplane. The parameter 






b

∥

w

∥






{\displaystyle {\tfrac {b}{\|\mathbf {w} \|}}}

 determines the offset of the hyperplane from the origin along the normal vector 





w




{\displaystyle {\mathbf {w} }}

.","If the training data are linearly separable, we can select two hyperplanes in such a way that they separate the data and there are no points between them, and then try to maximize their distance."
"These datasets are used for machine learning research and have been cited in peer-reviewed academic journals and other publications. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets.[1] High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.[2][3][4][5] This list aggregates high-quality datasets that have been shown to be of value to the machine learning research community from multiple different data repositories to provide greater coverage of the topic than is otherwise available.",,,"Datasets consisting primarily of images or videos for tasks such as object detection, facial recognition, and multi-label classification.","In computer vision, facial images have been used extensively to develop facial recognition systems, face detection, and many other projects that use facial images.","Images, text","Images, text","Images, text","Datasets consisting primarily of text for tasks such as natural language processing, sentiment analysis, translation, and cluster analysis.",citation analysis,Datasets of sounds and sound features.,(WAV),Datasets containing electric signal information requiring some sort of Signal processing for further analysis.,Datasets from physical systems,Datasets from biological systems.,Datasets consisting of rows of observations and columns of attributes characterizing those observations. Typically used for regression analysis or classification but other types of algorithms can also be used. This section includes datasets that do not fit in the above categories.
"In machine learning, local case-control sampling [1] is an algorithm used to reduce the complexity of training a logistic regression classifier. The algorithm reduces the training complexity by selecting a small subsample of the original dataset for training. It assumes the availability of a (unreliable) pilot estimation of the parameters. It then performs a single pass over the entire dataset using the pilot estimation to identify the most ""surprising"" samples. In practice, the pilot may come from prior knowledge or training using a subsample of the dataset. The algorithm is most effective when the underlying dataset is imbalanced. It exploits the structures of conditional imbalanced datasets more efficiently than alternative methods, such as case control sampling and weighted case control sampling.",,,"In classification, a dataset is a set of N data points 



(

x

i


,

y

i



)

i
=
1


N




{\displaystyle (x_{i},y_{i})_{i=1}^{N}}

, where 




x

i


∈


R


d




{\displaystyle x_{i}\in \mathbb {R} ^{d}}

 is a feature vector, 




y

i


∈
{
0
,
1
}


{\displaystyle y_{i}\in \{0,1\}}

 is a label. Intuitively, a dataset is imbalanced when certain important statistical patterns are rare. The lack of observations of certain patterns does not always imply their irrelevance. For example, in medical studies of rare diseases, the small number of infected patients (cases) conveys the most valuable information for diagnosis and treatments.","Formally, an imbalanced dataset exhibits one or more of the following properties:","In logistic regression, given the model 



θ
=
(
α
,
β
)


{\displaystyle \theta =(\alpha ,\beta )}

, the prediction is made according to 




P

(
Y
=
1
∣
X
;
θ
)
=




p
~




θ


(
x
)
=



exp
⁡
(
α
+

β

T


x
)


1
+
exp
⁡
(
α
+

β

T


x
)





{\displaystyle \mathbb {P} (Y=1\mid X;\theta )={\tilde {p}}_{\theta }(x)={\frac {\exp(\alpha +\beta ^{T}x)}{1+\exp(\alpha +\beta ^{T}x)}}}

. The local-case control sampling algorithm assumes the availability of a pilot model 






θ
~



=
(



α
~



,



β
~



)


{\displaystyle {\tilde {\theta }}=({\tilde {\alpha }},{\tilde {\beta }})}

. Given the pilot model, the algorithm performs a single pass over the entire dataset to select the subset of samples to include in training the logistic regression model. For a sample 



(
x
,
y
)


{\displaystyle (x,y)}

, define the acceptance probability as 



a
(
x
,
y
)
=

|

y
−




p
~






θ
~




(
x
)

|



{\displaystyle a(x,y)=|y-{\tilde {p}}_{\tilde {\theta }}(x)|}

. The algorithm proceeds as follows:",The algorithm can be understood as selecting samples that surprises the pilot model. Intuitively these samples are closer to the decision boundary of the classifier and is thus more informative.,"In practice, for cases where a pilot model is naturally available, the algorithm can be applied directly to reduce the complexity of training. In cases where a natural pilot is nonexistent, an estimate using a subsample selected through another sampling technique can be used instead. In the original paper describing the algorithm, the authors propose to use weighted case-control sampling with half the assigned sampling budget. For example, if the objective is to use a subsample with size 



N
=
1000


{\displaystyle N=1000}

, first estimate a model 






θ
~





{\displaystyle {\tilde {\theta }}}

 using 




N

h


=
500


{\displaystyle N_{h}=500}

 samples from weighted case control sampling, then collect another 




N

h


=
500


{\displaystyle N_{h}=500}

 samples using local case-control sampling.","It is possible to control the sample size by multiplying the acceptance probability with a constant 



c


{\displaystyle c}

. For a larger sample size, pick 



c
>
1


{\displaystyle c>1}

 and adjust the acceptance probability to 



min
(
c
a
(

x

i


,

y

i


)
,
1
)


{\displaystyle \min(ca(x_{i},y_{i}),1)}

. For a smaller sample size, the same strategy applies. In cases where the number of samples desired is precise, a convenient alternative method is to uniformly downsample from a larger subsample selected by local case-control sampling.","The algorithm has the following properties. When the pilot is consistent, the estimates using the samples from local case-control sampling is consistent even under model misspecification. If the model is correct then the algorithm has exactly twice the asymptotic variance of logistic regression on the full data set. For a larger sample size with 



c
>
1


{\displaystyle c>1}

, the factor 2 is improved to 



1
+


1
c




{\displaystyle 1+{\frac {1}{c}}}

."
"In Machine Learning and Computer Vision, M-Theory is a learning framework inspired by feed-forward processing in the ventral stream of visual cortex and originally developed for recognition and classification of objects in visual scenes. M-Theory was later applied to other areas, such as speech recognition. On certain image recognition tasks, algorithms based on a specific instantiation of M-Theory, HMAX, achieved human-level performance.[1]","The core principle of M-Theory is extracting representations invariant to various transformations of images (translation, scale, 2D and 3D rotation and others). In contrast with other approaches using invariant representations, in M-Theory they are not hardcoded into the algorithms, but learned. M-Theory also shares some principles with Compressed Sensing. The theory proposes multilayered hierarchical learning architecture, similar to that of visual cortex.",,,"A great challenge in visual recognition tasks is that the same object can be seen in a variety of conditions. It can be seen from different distances, different viewpoints, under different lighting, partially occluded, etc. In addition, for particular classes objects, such as faces, highly complex specific transformations may be relevant, such as changing facial expressions. For learning to recognize images, it is greatly beneficial to factor out these variations. It results in much simpler classification problem and, consequently, in great reduction of sample complexity of the model.","A simple computational experiment illustrates this idea. Two instances of a classifier were trained to distinguish images of planes from those of cars. For training and testing of the first instance, images with arbitrary viewpoints were used. Another instance received only images seen from a particular viewpoint, which was equivalent to training and testing the system on invariant representation of the images. One can see that the second classifier performed quite well even after receiving a single example from each category, while performance of the first classifier was close to random guess even after seeing 20 examples.","Invariant representations has been incorporated into several learning architectures, such as neocognitrons. Most of these architectures, however, provided invariance through custom-designed features or properties of architecture itself. While it helps to take into account some sorts of transformations, such as translations, it is very nontrivial to accommodate for other sorts of transformations, such as 3D rotations and changing facial expressions. M-Theory provides a framework of how such transformations can be learned. In addition to higher flexibility, this theory also suggests how human brain may have similar capabilities.","Another core idea of M-Theory is close in spirit to ideas from the field of compressed sensing. An implication from Johnson–Lindenstrauss lemma says that a particular number of images can be embedded into a low-dimensional feature space with the same distances between images by using random projections. This result suggests that dot product between the observed image and some other image stored in memory, called template, can be used as a feature helping to distinguish the image from other images. The template need not to be anyhow related to the image, it could be chosen randomly.","The two ideas outlined in previous sections can be brought together to construct a framework for learning invariant representations. The key observation is how dot product between image 



I


{\displaystyle I}

 and a template 



t


{\displaystyle t}

 behaves when image is transformed (by such transformations as translations, rotations, scales, etc.). If transformation 



g


{\displaystyle g}

 is a member of a unitary group of transformations, then the following holds:","



⟨
g
I
,
t
⟩
=
⟨
I
,

g

−
1


t
⟩
(
1
)


{\displaystyle \langle gI,t\rangle =\langle I,g^{-1}t\rangle (1)}

","In other words, the dot product of transformed image and a template is equal to the dot product of original image and inversely transformed template. For instance, for image rotated by 90 degrees, the inversely transformed template would be rotated by -90 degrees.","Consider the set of dot products of an image 



I


{\displaystyle I}

 to all possible transformations of template: 



{
⟨
I
,

g

′


t
⟩

|


g

′


∈
G
}


{\displaystyle \lbrace \langle I,g^{\prime }t\rangle |g^{\prime }\in G\rbrace }

. If one applies a transformation 



g


{\displaystyle g}

 to 



I


{\displaystyle I}

, the set would become 



{
⟨
g
I
,

g

′


t
⟩

|


g

′


∈
G
}


{\displaystyle \lbrace \langle gI,g^{\prime }t\rangle |g^{\prime }\in G\rbrace }

. But because of the property (1), this is equal to 



{
⟨
I
,

g

−
1



g

′


t
⟩

|


g

′


∈
G
}


{\displaystyle \lbrace \langle I,g^{-1}g^{\prime }t\rangle |g^{\prime }\in G\rbrace }

. The set 



{

g

−
1



g

′



|


g

′


∈
G
}


{\displaystyle \lbrace g^{-1}g^{\prime }|g^{\prime }\in G\rbrace }

 is equal to just the set of all elements in 



G


{\displaystyle G}

. To see this, note that every 




g

−
1



g

′




{\displaystyle g^{-1}g^{\prime }}

 is in 



G


{\displaystyle G}

 due to the closure property of groups, and for every 




g

′
′




{\displaystyle g^{\prime \prime }}

 in G there exist its prototype 




g

′




{\displaystyle g^{\prime }}

 such as 




g

′
′


=

g

−
1



g

′




{\displaystyle g^{\prime \prime }=g^{-1}g^{\prime }}

 (namely, 




g

′


=
g

g

′
′




{\displaystyle g^{\prime }=gg^{\prime \prime }}

). Thus, 



{
⟨
I
,

g

−
1



g

′


t
⟩

|


g

′


∈
G
}
=
{
⟨
I
,

g

′
′


t
⟩

|


g

′
′


∈
G
}


{\displaystyle \lbrace \langle I,g^{-1}g^{\prime }t\rangle |g^{\prime }\in G\rbrace =\lbrace \langle I,g^{\prime \prime }t\rangle |g^{\prime \prime }\in G\rbrace }

. One can see that the set of dot products remains the same despite that a transformation was applied to the image! This set by itself may serve as a (very cumbersome) invariant representation of an image. More practical representations can be derived from it.","In the introductory section, it was claimed that M-Theory allows to learn invariant representations. This is because templates and their transformed versions can be learned from visual experience - by exposing the system to sequences of transformations of objects. It is plausible that similar visual experiences occur in early period of human life, for instance when infants twiddle toys in their hands. Because templates may be totally unrelated to images that the system later will try to classify, memories of these visual experiences may serve as a basis for recognizing many different kinds of objects in later life. However, as it is shown later, for some kinds of transformations, specific templates are needed.","To implement the ideas described in previous sections, one need to know how to derive a computationally efficient invariant representation of an image. Such unique representation for each image can be characterized as it appears by a set of one-dimensional probability distributions (empirical distributions of the dot-products between image and a set of templates stored during unsupervised learning). These probability distributions in their turn can be described by either histograms or a set of statistical moments of it, as it will be shown below.","Orbit 




O

I




{\displaystyle O_{I}}

 is a set of images 



g
I


{\displaystyle gI}

 generated from a single image 



I


{\displaystyle I}

 under the action of the group 



G
,
∀
g
∈
G


{\displaystyle G,\forall g\in G}

.","In other words, images of an object and of its transformations correspond to a orbit 




O

I




{\displaystyle O_{I}}

. If two orbits have a point in common they are identical everywhere,[2] i.e. an orbit is an invariant and unique representation of an image. So, two images are called equivalent when they belong to the same orbit: 



I
∼

I

′




{\displaystyle I\sim I^{\prime }}

 if 



∃
g
∈
G


{\displaystyle \exists g\in G}

 such that 




I

′


=
g
I


{\displaystyle I^{\prime }=gI}

. Conversely, two orbits are different if none of the images in one orbit coincide with any image in the other.[3]","A natural question arises: how can one compare two orbits? There are several possible approaches. One of them employs the fact that intuitively two empirical orbits are the same irrespective of the ordering of their points. Thus, one can consider a probability distribution 




P

I




{\displaystyle P_{I}}

 induced by the group’s action on images 



I


{\displaystyle I}

 (



g
I


{\displaystyle gI}

 can be seen as a realization of a random variable).","This probability distribution 




P

I




{\displaystyle P_{I}}

 can be almost uniquely characterized by 



K


{\displaystyle K}

 one-dimensional probability distributions 




P

⟨
I
,

t

k


⟩




{\displaystyle P_{\langle I,t^{k}\rangle }}

 induced by the (one-dimensional) results of projections 



⟨
I
,

t

k


⟩


{\displaystyle \langle I,t^{k}\rangle }

, where 




t

k


,
k
=
1
,
.
.
.
,
K


{\displaystyle t^{k},k=1,...,K}

 are a set of templates (randomly chosen images) (based on the Cramer-Wold theorem [4] and concentration of measures).","Consider 



n


{\displaystyle n}

 images 




X

n


∈
X


{\displaystyle X_{n}\in X}

. Let 



K
≥


2

c

ϵ

2





log
⁡


n
δ




{\displaystyle K\geq {\frac {2}{c\epsilon ^{2}}}\log {\frac {n}{\delta }}}

 , where 



c


{\displaystyle c}

 is a universal constant. Then","




|

d
(

P

I


,

P

I


′


)
−
d
K
(

P

I


,

P

I


′


)

|

≤
ϵ
,


{\displaystyle |d(P_{I},P_{I}^{\prime })-dK(P_{I},P_{I}^{\prime })|\leq \epsilon ,}

","with probability 



1
−

δ

2




{\displaystyle 1-\delta ^{2}}

, for all 



I
,

I

′




{\displaystyle I,I^{\prime }}

 



∈


{\displaystyle \in }

 




X

n




{\displaystyle X_{n}}

.","This result (informally) says that an approximately invariant and unique representation of an image 



I


{\displaystyle I}

 can be obtained from the estimates of 



K


{\displaystyle K}

 1-D probability distributions 




P

⟨
I
,

t

k


⟩




{\displaystyle P_{\langle I,t^{k}\rangle }}

 for 



k
=
1
,
.
.
.
,
K


{\displaystyle k=1,...,K}

. The number 



K


{\displaystyle K}

 of projections needed to discriminate 



n


{\displaystyle n}

 orbits, induced by 



n


{\displaystyle n}

 images, up to precision 



ϵ


{\displaystyle \epsilon }

 (and with confidence 



1
−

δ

2




{\displaystyle 1-\delta ^{2}}

) is 



K
≥


2

c

ϵ

2





log
⁡


n
δ




{\displaystyle K\geq {\frac {2}{c\epsilon ^{2}}}\log {\frac {n}{\delta }}}

, where 



c


{\displaystyle c}

 is a universal constant.","To classify an image, the following “recipe” can be used:","Estimates of such one-dimensional probability density functions (PDFs) 




P

⟨
I
,

t

k


⟩




{\displaystyle P_{\langle I,t^{k}\rangle }}

 can be written in terms of histograms as 




μ

n


k


(
I
)
=
1

/


|
G
|


∑

i
=
1



|
G
|




η

n


(
⟨
I
,

g

i



t

k


⟩
)


{\displaystyle \mu _{n}^{k}(I)=1/\left|G\right|\sum _{i=1}^{\left|G\right|}\eta _{n}(\langle I,g_{i}t^{k}\rangle )}

, where 




η

n


,
n
=
1
,
.
.
.
,
N


{\displaystyle \eta _{n},n=1,...,N}

 is a set of nonlinear functions. These 1-D probability distributions can be characterized with N-bin histograms or set of statistical moments. For example, HMAX represents an architecture in which pooling is done with a max operation.","In the ""recipe"" for image classification, groups of transformations are approximated with finite number of transformations. Such approximation is possible only when the group is compact.","Such groups as all translations and all scalings of the image are not compact, as they allow arbitrarily big transformations. However, they are locally compact. For locally compact groups, invariance is achievable within certain range of transformations.[2]","Assume that 




G

0




{\displaystyle G_{0}}

 is a subset of transformations from 



G


{\displaystyle G}

 for which the transformed patterns exist in memory. For an image 



I


{\displaystyle I}

 and template 




t

k




{\displaystyle t_{k}}

, assume that 



⟨
I
,

g

−
1



t

k


⟩


{\displaystyle \langle I,g^{-1}t_{k}\rangle }

 is equal to zero everywhere except some subset of 




G

0




{\displaystyle G_{0}}

. This subset is called support of 



⟨
I
,

g

−
1



t

k


⟩


{\displaystyle \langle I,g^{-1}t_{k}\rangle }

 and denoted as 



s
u
p
p
(
⟨
I
,

g

−
1



t

k


⟩
)


{\displaystyle supp(\langle I,g^{-1}t_{k}\rangle )}

. It can be proven that if for a transformation 




g

′




{\displaystyle g^{\prime }}

, support set will also lie within 




g

′



G

0




{\displaystyle g^{\prime }G_{0}}

, then signature of 



I


{\displaystyle I}

 is invariant with respect to 




g

′




{\displaystyle g^{\prime }}

.[2] This theorem determines the range of transformations for which invariance is guaranteed to hold.","One can see that the smaller is 



s
u
p
p
(
⟨
I
,

g

−
1



t

k


⟩
)


{\displaystyle supp(\langle I,g^{-1}t_{k}\rangle )}

, the larger is the range of transformations for which invariance is guaranteed to hold. It means that for a group that is only locally compact, not all templates would work equally well anymore. Preferable templates are those with a reasonably small 



s
u
p
p
(
⟨
g
I
,

t

k


⟩
)


{\displaystyle supp(\langle gI,t_{k}\rangle )}

 for a generic image. This property is called localization: templates are sensitive only to images within a small range of transformations. Note that although minimizing 



s
u
p
p
(
⟨
g
I
,

t

k


⟩
)


{\displaystyle supp(\langle gI,t_{k}\rangle )}

 is not absolutely necessary for the system to work, it improves approximation of invariance. Requiring localization simultaneously for translation and scale yields a very specific kind of templates: Gabor functions.[2]","The desirability of custom templates for non-compact group is in conflict with the principle of learning invariant representations. However, for certain kinds of regularly encountered image transformations, templates might be the result of evolutionary adaptations. Neurobiological data suggests that there is Gabor-like tuning in the first layer of visual cortex.[5] The optimality of Gabor templates for translations and scales is a possible explanation of this phenomenon.","Many interesting transformations of images do not form groups. For instance, transformations of images associated with 3D rotation of corresponding 3D object do not form a group, because it is impossible to define an inverse transformation (two objects may looks the same from one angle but different from another angle). However, approximate invariance is still achievable even for non-group transformations, if localization condition for templates holds and transformation can be locally linearized.","As it was said in the previous section, for specific case of translations and scaling, localization condition can be satisfied by use of generic Gabor templates. However, for general case (non-group) transformation, localization condition can be satisfied only for specific class of objects.[2] More specifically, in order to satisfy the condition, templates must be similar to the objects one would like to recognize. For instance, if one would like to build a system to recognize 3D rotated faces, one need to use other 3D rotated faces as templates. This may explain the existence of such specialized modules in the brain as one responsible for face recognition.[2] Even with custom templates, a noise-like encoding of images and templates is necessary for localization. It can be naturally achieved if the non-group transformation is processed on any layer other than the first in hierarchical recognition architecture.","The previous section suggests one motivation for hierarchical image recognition architectures. However, they have other benefits as well.","Firstly, hierarchical architectures best accomplish the goal of ‘parsing’ a complex visual scene with many objects consisting of many parts, whose relative position may greatly vary. In this case, different elements of the system must react to different objects and parts. In hierarchical architectures, representations of parts at different levels of embedding hierarchy can be stored at different layers of hierarchy.","Secondly, hierarchical architectures which have invariant representations for parts of objects may facilitate learning of complex compositional concepts. This facilitation may happen through reusing of learned representations of parts that were constructed before in process of learning of other concepts. As a result, sample complexity of learning compositional concepts may be greatly reduced.","Finally, hierarchical architectures have better tolerance to clutter. Clutter problem arises when the target object is in front of a non-uniform background, which functions as a distractor for the visual task. Hierarchical architecture provides signatures for parts of target objects, which do not include parts of background and are not affected by background variations.[6]","In hierarchical architectures, one layer is not necessarily invariant to all transformations that are handled by the hierarchy as a whole. Some transformations may pass through that layer to upper layers, as in the case of non-group transformations described in the previous section. For other transformations, an element of the layer may produce invariant representations only within small range of transformations. For instance, elements of the lower layers in hierarchy have small visual field and thus can handle only a small range of translation. For such transformations, the layer should provide covariant rather than invariant, signatures. The property of covariance can be written as 



d
i
s
t
r
(
⟨

μ

l


(
g
I
)
,

μ

l


(
t
)
⟩
)
=
d
i
s
t
r
(
⟨

μ

l


(
I
)
,

μ

l


(

g

−
1


t
)
⟩
)


{\displaystyle distr(\langle \mu _{l}(gI),\mu _{l}(t)\rangle )=distr(\langle \mu _{l}(I),\mu _{l}(g^{-1}t)\rangle )}

, where 



l


{\displaystyle l}

 is a layer, 




μ

l


(
I
)


{\displaystyle \mu _{l}(I)}

 is the signature of image on that layer, and 



d
i
s
t
r


{\displaystyle distr}

 stands for ""distribution of values of the expression for all 



g
∈
G


{\displaystyle g\in G}

"".","M-theory is based on a quantitative theory of the ventral stream of visual cortex.[7][8] Understanding how visual cortex works in object recognition is still a challenging task for neuroscience. Humans and primates are able to memorize and recognize objects after seeing just couple of examples unlike any state-of-the art machine vision systems that usually require a lot of data in order to recognize objects. Prior to , the use of visual neuroscience in computer vision has been limited to early vision for deriving stereo algorithms (e.g.,[9]) and to justify the use of DoG (derivative-of-Gaussian) filters and more recently of Gabor filters.[10][11] No real attention has been given to biologically plausible features of higher complexity. While mainstream computer vision has always been inspired and challenged by human vision, it seems to have never advanced past the very first stages of processing in the simple cells in V1 and V2. Although some of the systems inspired - to various degrees - by neuroscience, have been tested on at least some natural images, neurobiological models of object recognition in cortex have not yet been extended to deal with real-world image databases.[12]","M-theory learning framework employs a novel hypothesis about the main computational function of the ventral stream: the representation of new objects/images in terms of a signature, which is invariant to transformations learned during visual experience. This allows recognition from very few labeled examples - in the limit, just one.","Neuroscience suggests that natural functionals for a neuron to compute is a high-dimensional dot product between an “image patch” and another image patch (called template) which is stored in terms of synaptic weights (synapses per neuron). The standard computational model of a neuron is based on a dot product and a threshold. Another important feature of the visual cortex is that it consists of simple and complex cells. This idea was originally proposed by Hubel and Wiesel.[9] M-theory employs this idea. Simple cells compute dot products of an image and transformations of templates 



⟨
I
,

g

i



t

k


⟩


{\displaystyle \langle I,g_{i}t^{k}\rangle }

 for 



i
=
1
,
.
.
.
,

|

G

|



{\displaystyle i=1,...,|G|}

 (




|

G

|



{\displaystyle |G|}

 is a number of simple cells). Complex cells are responsible for pooling and computing empirical histograms or statistical moments of it. The following formula for constructing histogram can be computed by neurons:","





1


|

G

|





∑

i
=
1



|

G

|



σ
(
⟨
I
,

g

i



t

k


⟩
+
n
Δ
)
,


{\displaystyle {\frac {1}{|G|}}\sum _{i=1}^{|G|}\sigma (\langle I,g_{i}t^{k}\rangle +n\Delta ),}

","where 



σ


{\displaystyle \sigma }

 is a smooth version of step function, 



Δ


{\displaystyle \Delta }

 is the width of a histogram bin, and 



n


{\displaystyle n}

 is the number of the bin.","In [clarification needed][13][14] authors applied M-theory to unconstrained face recognition in natural photographs. Unlike the DAR (detection, alignment, and recognition) method, which handles clutter by detecting objects and cropping closely around them so that very little background remains, this approach accomplishes detection and alignment implicitly by storing transformations of training images (templates) rather than explicitly detecting and aligning or cropping faces at test time. This system is built according to the principles of a recent theory of invariance in hierarchical networks and can evade the clutter problem generally problematic for feedforward systems. The resulting end-to-end system achieves a drastic improvement in the state of the art on this end-to-end task, reaching the same level of performance as the best systems operating on aligned, closely cropped images (no outside training data). It also performs well on two newer datasets, similar to LFW, but more difficult: significantly jittered (misaligned) version of LFW and SUFR-W (for example, the model’s accuracy in the LFW “unaligned & no outside data used” category is 87.55±1.41% compared to state-of-the-art APEM (adaptive probabilistic elastic matching): 81.70±1.78%).","The theory was also applied to a range of recognition tasks: from invariant single object recognition in clutter to multiclass categorization problems on publicly available data sets (CalTech5, CalTech101, MIT-CBCL) and complex (street) scene understanding tasks that requires the recognition of both shape-based as well as texture-based objects (on StreetScenes data set).[12] The approach performs really well: It has the capability of learning from only a few training examples and was shown to outperform several more complex state-of-the-art systems constellation models, the hierarchical SVM-based face- detection system). A key element in the approach is a new set of scale and position-tolerant feature detectors, which are biologically plausible and agree quantitatively with the tuning properties of cells along the ventral stream of visual cortex. These features are adaptive to the training set, though we also show that a universal feature set, learned from a set of natural images unrelated to any categorization task, likewise achieves good performance.","This theory can also be extended for the speech recognition domain. As an example, in[15] an extension of a theory for unsupervised learning of invariant visual representations to the auditory domain and empirically evaluated its validity for voiced speech sound classification was proposed. Authors empirically demonstrated that a single-layer, phone-level representation, extracted from base speech features, improves segment classification accuracy and decreases the number of training examples in comparison with standard spectral and cepstral features for an acoustic classification task on TIMIT dataset.[16]"
"Logic Learning Machine (LLM) is a machine learning method based on the generation of intelligible rules. LLM is an efficient implementation of the Switching Neural Network (SNN) paradigm,[1] developed by Marco Muselli, from the Italian National Research Council. Logic Learning Machine is implemented in the Rulex suite.","LLM has been employed in different fields, including orthopaedic patient classification,[2] DNA microarray analysis [3] and Clinical Decision Support System.[4]",,,"The Switching Neural Network approach was developed in the 1990s to overcome the drawbacks of the most commonly used machine learning methods. In particular, black box methods, such as multilayer perceptron and support vector machine, had good accuracy but could not provide deep insight into the studied phenomenon. On the other hand, decision trees were able to describe the phenomenon but often lacked accuracy. Switching Neural Networks made use of Boolean algebra to build sets of intelligible rules able to obtain very good performance. In 2014, an efficient version of Switching Neural Network was developed and implemented in the Rulex suite with the name Logic Learning Machine.[5] Also a LLM version devoted to regression problems was developed.","Like other machine learning methods, LLM uses data to build a model able to perform a good forecast about future behaviors. LLM starts from a table including a target variable (output) and some inputs and generates a set of rules that return the output value 



y


{\displaystyle y}

 corresponding to a given configuration of inputs. A rule is written in the form:","where consequence contains the output value whereas premise includes one or more conditions on the inputs. According to the input type, conditions can have different forms:",A possible rule is therefore in the form,"According to the output type, different versions of Logic Learning Machine have been developed:"
"Machine Learning is a peer-reviewed scientific journal, published since 1986.","In 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.[1]",
"In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth: data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.",,,"Manifold regularization is a type of regularization, a family of techniques that reduces overfitting and ensures that a problem is well-posed by penalizing complex solutions. In particular, manifold regularization extends the technique of Tikhonov regularization as applied to Reproducing kernel Hilbert spaces (RKHSs). Under standard Tikhonov regularization on RKHSs, a learning algorithm attempts to learn a function 



f


{\displaystyle f}

 from among a hypothesis space of functions 





H




{\displaystyle {\mathcal {H}}}

. The hypothesis space is an RKHS, meaning that it is associated with a kernel 



K


{\displaystyle K}

, and so every candidate function 



f


{\displaystyle f}

 has a norm 





∥
f
∥


K




{\displaystyle \left\|f\right\|_{K}}

, which represents the complexity of the candidate function in the hypothesis space. When the algorithm considers a candidate function, it takes its norm into account in order to penalize complex functions.","Formally, given a set of labeled training data 



(

x

1


,

y

1


)
,
…
,
(

x

ℓ


,

y

ℓ


)


{\displaystyle (x_{1},y_{1}),\ldots ,(x_{\ell },y_{\ell })}

 with 




x

i


∈
X
,

y

i


∈
Y


{\displaystyle x_{i}\in X,y_{i}\in Y}

 and a loss function 



V


{\displaystyle V}

, a learning algorithm using Tikhonov regularization will attempt to solve the expression","where 



γ


{\displaystyle \gamma }

 is a hyperparameter that controls how much the algorithm will prefer simpler functions to functions that fit the data better.","Manifold regularization adds a second regularization term, the intrinsic regularizer, to the ambient regularizer used in standard Tikhonov regularization. Under the manifold assumption in machine learning, the data in question do not come from the entire input space 



X


{\displaystyle X}

, but instead from a nonlinear manifold 



M
⊂
X


{\displaystyle M\subset X}

. The geometry of this manifold, the intrinsic space, is used to determine the regularization norm.[1]","There are many possible choices for 





∥
f
∥


I




{\displaystyle \left\|f\right\|_{I}}

. Many natural choices involve the gradient on the manifold 




∇

M




{\displaystyle \nabla _{M}}

, which can provide a measure of how smooth a target function is. A smooth function should change slowly where the input data are dense; that is, the gradient 




∇

M


f
(
x
)


{\displaystyle \nabla _{M}f(x)}

 should be small where the marginal probability density 






P



X


(
x
)


{\displaystyle {\mathcal {P}}_{X}(x)}

, the probability density of a randomly drawn data point appearing at 



x


{\displaystyle x}

, is large. This gives one appropriate choice for the intrinsic regularizer:","In practice, this norm cannot be computed directly because the marginal distribution 






P



X




{\displaystyle {\mathcal {P}}_{X}}

 is unknown, but it can be estimated from the provided data. In particular, if the distances between input points are interpreted as a graph, then the Laplacian matrix of the graph can help to estimate the marginal distribution. Suppose that the input data include 



ℓ


{\displaystyle \ell }

 labeled examples (pairs of an input 



x


{\displaystyle x}

 and a label 



y


{\displaystyle y}

) and 



u


{\displaystyle u}

 unlabeled examples (inputs without associated labels). Define 



W


{\displaystyle W}

 to be a matrix of edge weights for a graph, where 




W

i
j




{\displaystyle W_{ij}}

 is a distance measure between the data points 




x

i




{\displaystyle x_{i}}

 and 




x

j




{\displaystyle x_{j}}

. Define 



D


{\displaystyle D}

 to be a diagonal matrix with 




D

i
i


=

∑

j
=
1


ℓ
+
u



W

i
j




{\displaystyle D_{ii}=\sum _{j=1}^{\ell +u}W_{ij}}

 and 



L


{\displaystyle L}

 to be the Laplacian matrix 



D
−
W


{\displaystyle D-W}

. Then, as the number of data points 



ℓ
+
u


{\displaystyle \ell +u}

 increases, 



L


{\displaystyle L}

 converges to the Laplace-Beltrami operator 




Δ

M




{\displaystyle \Delta _{M}}

, which is the divergence of the gradient 




∇

M




{\displaystyle \nabla _{M}}

.[2][3] Then, if 




f



{\displaystyle \mathbf {f} }

 is a vector of the values of 



f


{\displaystyle f}

 at the data, 




f

=
[
f
(

x

1


)
,
…
,
f
(

x

l
+
u


)

]


T





{\displaystyle \mathbf {f} =[f(x_{1}),\ldots ,f(x_{l+u})]^{\mathrm {T} }}

, the intrinsic norm can be estimated:","As the number of data points 



ℓ
+
u


{\displaystyle \ell +u}

 increases, this empirical definition of 





∥
f
∥


I


2




{\displaystyle \left\|f\right\|_{I}^{2}}

 converges to the definition when 






P



X




{\displaystyle {\mathcal {P}}_{X}}

 is known.[1]","Using the weights 




γ

A




{\displaystyle \gamma _{A}}

 and 




γ

I




{\displaystyle \gamma _{I}}

 for the ambient and intrinsic regularizers, the final expression to be solved becomes:","As with other kernel methods, 





H




{\displaystyle {\mathcal {H}}}

 may be an infinite-dimensional space, so if the regularization expression cannot be solved explicitly, it is impossible to search the entire space for a solution. Instead, a representer theorem shows that under certain conditions on the choice of the norm 





∥
f
∥


I




{\displaystyle \left\|f\right\|_{I}}

, the optimal solution 




f

∗




{\displaystyle f^{*}}

 must be a linear combination of the kernel centered at each of the input points: for some weights 




α

i




{\displaystyle \alpha _{i}}

,","Using this result, it is possible to search for the optimal solution 




f

∗




{\displaystyle f^{*}}

 by searching the finite-dimensional space defined by the possible choices of 




α

i




{\displaystyle \alpha _{i}}

.[1]","Manifold regularization can extend a variety of algorithms that can be expressed using Tikhonov regularization, by choosing an appropriate loss function 



V


{\displaystyle V}

 and hypothesis space 





H




{\displaystyle {\mathcal {H}}}

. Two commonly used examples are the families of support vector machines and regularized least squares algorithms. (Regularized least squares includes the ridge regression algorithm; the related algorithms of LASSO and elastic net regularization can be expressed as support vector machines.[4][5]) The extended versions of these algorithms are called Laplacian Regularized Least Squares (abbreviated LapRLS) and Laplacian Support Vector Machines (LapSVM), respectively.[1]","Regularized least squares (RLS) is a family of regression algorithms: algorithms that predict a value 



y
=
f
(
x
)


{\displaystyle y=f(x)}

 for its inputs 



x


{\displaystyle x}

, with the goal that the predicted values should be close to the true labels for the data. In particular, RLS is designed to minimize the mean squared error between the predicted values and the true labels, subject to regularization. Ridge regression is one form of RLS; in general, RLS is the same as ridge regression combined with the kernel method.[citation needed] The problem statement for RLS results from choosing the loss function 



V


{\displaystyle V}

 in Tikhonov regularization to be the mean squared error:","Thanks to the representer theorem, the solution can be written as a weighted sum of the kernel evaluated at the data points:","and solving for 




α

∗




{\displaystyle \alpha ^{*}}

 gives:","where 



K


{\displaystyle K}

 is defined to be the kernel matrix, with 




K

i
j


=
K
(

x

i


,

x

j


)


{\displaystyle K_{ij}=K(x_{i},x_{j})}

, and 



Y


{\displaystyle Y}

 is the vector of data labels.",Adding a Laplacian term for manifold regularization gives the Laplacian RLS statement:,The representer theorem for manifold regularization again gives,"and this yields an expression for the vector 




α

∗




{\displaystyle \alpha ^{*}}

. Letting 



K


{\displaystyle K}

 be the kernel matrix as above, 



Y


{\displaystyle Y}

 be the vector of data labels, and 



J


{\displaystyle J}

 be the 



(
ℓ
+
u
)
×
(
ℓ
+
u
)


{\displaystyle (\ell +u)\times (\ell +u)}

 block matrix 





[




I

ℓ




0




0



0

u





]




{\displaystyle {\begin{bmatrix}I_{\ell }&0\\0&0_{u}\end{bmatrix}}}

:",with a solution of,"LapRLS has been applied to problems including sensor networks,[6] medical imaging,[7][8] object detection,[9] spectroscopy,[10] document classification,[11] drug-protein interactions,[12] and compressing images and videos.[13]","Support vector machines (SVMs) are a family of algorithms often used for classifying data into two or more groups, or classes. Intuitively, an SVM draws a boundary between classes so that the closest labeled examples to the boundary are as far away as possible. This can be directly expressed as a linear program, but it is also equivalent to Tikhonov regularization with the hinge loss function, 



V
(
f
(
x
)
,
y
)
=
max
(
0
,
1
−
y
f
(
x
)
)


{\displaystyle V(f(x),y)=\max(0,1-yf(x))}

:",Adding the intrinsic regularization term to this expression gives the LapSVM problem statement:,"Again, the representer theorem allows the solution to be expressed in terms of the kernel evaluated at the data points:","



α


{\displaystyle \alpha }

 can be found by writing the problem as a linear program and solving the dual problem. Again letting 



K


{\displaystyle K}

 be the kernel matrix and 



J


{\displaystyle J}

 be the block matrix 





[




I

ℓ




0




0



0

u





]




{\displaystyle {\begin{bmatrix}I_{\ell }&0\\0&0_{u}\end{bmatrix}}}

, the solution can be shown to be","where 




β

∗




{\displaystyle \beta ^{*}}

 is the solution to the dual problem","and 



Q


{\displaystyle Q}

 is defined by","LapSVM has been applied to problems including geographical imaging,[16][17][18] medical imaging,[19][20][21] face recognition,[22] machine maintenance,[23] and brain-computer interfaces.[24]"
"The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote it in order to generate interest from people outside the field. Towards the end of the book, while reviewing his invention of the Markov logic network [1] he pictures a ""master algorithm"" allowing technology to allow machine learning algorithms to asymptotically grow to a perfect understanding of how the world and people in it work.[2]",
"In the field of statistical learning theory, matrix regularization generalizes notions of vector regularization to cases where the object to be learned is a matrix. The purpose of regularization is to enforce conditions, for example sparsity or smoothness, that can produce stable predictive functions. For example, in the more common vector framework, Tikhonov regularization optimizes over","to find a vector, 



x


{\displaystyle x}

, that is a stable solution to the regression problem. When the system is described by a matrix rather than a vector, this problem can be written as","where the vector norm enforcing a regularization penalty on 



x


{\displaystyle x}

 has been extended to a matrix norm on 



X


{\displaystyle X}

.","Matrix Regularization has applications in matrix completion, multivariate regression, and multi-task learning. Ideas of feature and group selection can also be extended to matrices, and these can be generalized to the nonparametric case of multiple kernel learning.",,,"Consider a matrix 



W


{\displaystyle W}

 to be learned from a set of examples, 



S
=
(

X

i


t


,

y

i


t


)


{\displaystyle S=(X_{i}^{t},y_{i}^{t})}

, where 



i


{\displaystyle i}

 goes from 



1


{\displaystyle 1}

 to 



n


{\displaystyle n}

, and 



t


{\displaystyle t}

 goes from 



1


{\displaystyle 1}

 to 



T


{\displaystyle T}

. Let each input matrix 




X

i




{\displaystyle X_{i}}

 be 



∈


R


D
T




{\displaystyle \in \mathbb {R} ^{DT}}

, and let 



W


{\displaystyle W}

 be of size 



D
×
T


{\displaystyle D\times T}

. A general model for the output 



y


{\displaystyle y}

 can be posed as","where the inner product is the Frobenius inner product. For different applications the matrices 




X

i




{\displaystyle X_{i}}

 will have different forms,[1] but for each of these the optimization problem to infer 



W


{\displaystyle W}

 can be written as","where 



E


{\displaystyle E}

 defines the empirical error for a given 



W


{\displaystyle W}

, and 



R
(
W
)


{\displaystyle R(W)}

 is a matrix regularization penalty. The function 



R
(
W
)


{\displaystyle R(W)}

 is typically chosen to be convex, and is often selected to enforce sparsity (using 




ℓ

1




{\displaystyle \ell ^{1}}

-norms) and/or smoothness (using 




ℓ

2




{\displaystyle \ell ^{2}}

-norms). Finally, 



W


{\displaystyle W}

 is in the space of matrices, 





H




{\displaystyle {\mathcal {H}}}

, with Forbenius inner product,.","In the problem of matrix completion, the matrix 




X

i


t




{\displaystyle X_{i}^{t}}

 takes the form","where 



(

e

t



)

t




{\displaystyle (e_{t})_{t}}

 and 



(

e

i

′


)

i




{\displaystyle (e_{i}')_{i}}

 are the canonical basis in 





R


T




{\displaystyle \mathbb {R} ^{T}}

 and 





R


D




{\displaystyle \mathbb {R} ^{D}}

. In this case the role of the Frobenius inner product is to select individual elements, 




w

i


t




{\displaystyle w_{i}^{t}}

, from the matrix 



W


{\displaystyle W}

. Thus, the output, 



y


{\displaystyle y}

, is a sampling of entries from the matrix 



W


{\displaystyle W}

.","The problem of reconstructing 



W


{\displaystyle W}

 from a small set of sampled entries is possible only under certain restrictions on the matrix, and these restrictions can be enforced by a regularization function. For example, it might be assumed that 



W


{\displaystyle W}

 is low-rank, in which case the regularization penalty can take the form of a nuclear norm.[2]","where 




σ

i




{\displaystyle \sigma _{i}}

, with 



i


{\displaystyle i}

 from 



1


{\displaystyle 1}

 to 



min
D
,
T


{\displaystyle \min D,T}

, are the singular values of 



W


{\displaystyle W}

.","Models used in multivariate regression are parameterized by a matrix of coefficients. In the Frobenius inner product above, each matrix 



X


{\displaystyle X}

 is",such that the output of the inner product is the dot product of one row of the input with one column of the coefficient matrix. The familiar form of such models is,"Many of the vector norms used in single variable regression can be extended to the multivariate case. One example is the squared Frobenius norm, which can be viewed as an 




ℓ

2




{\displaystyle \ell ^{2}}

-norm acting either entrywise, or on the singular values of the matrix:","In the multivariate case the effect of regularizing with the Frobenius norm is the same as the vector case; very complex models will have larger norms, and, thus, will be penalized more.","The setup for multi-task learning is almost the same as the setup for multivariate regression. The primary difference is that the input variables are also indexed by task (columns of 



Y


{\displaystyle Y}

). The representation with the Frobenius inner product is then","The role of matrix regularization in this setting can be the same as in multivariate regression, but matrix norms can also be used to couple learning problems across tasks. In particular, note that for the optimization problem","the solutions corresponding to each column of 



Y


{\displaystyle Y}

 are decoupled. That is, the same solution can be found by solving the joint problem, or by solving an isolated regression problem for each column. The problems can be coupled by adding an additional regulatization penalty on the covariance of solutions","where 



Ω


{\displaystyle \Omega }

 models the relationship between tasks. This scheme can be used to both enforce similarity of solutions across tasks, and to learn the specific structure of task similarity by alternating between optimizations of 



W


{\displaystyle W}

 and 



Ω


{\displaystyle \Omega }

.[3] When the relationship between tasks is known to lie on a graph, the Laplacian matrix of the graph can be used to couple the learning problems.","Regularization by spectral filtering has been used to find stable solutions to problems such as those discussed above by addressing ill-posed matrix inversions (see for example Filter function for Tikhonov regularization). In many cases the regularization function acts on the input (or kernel) to ensure a bounded inverse by eliminating small singular values, but it can also be useful to have spectral norms that act on the matrix that is to be learned.","There are a number of matrix norms that act on the singular values of the matrix. Frequently used examples include the Schatten p-norms, with p = 1 or 2. For example, matrix regularization with a Schatten 1-norm, also called the nuclear norm, can be used to enforce sparsity in the spectrum of a matrix. This has been used in the context of matrix completion when the matrix in question is believed to have a restricted rank.[2] In this case the optimization problem becomes:","Spectral Regularization is also used to enforce a reduced rank coefficient matrix in multivariate regression.[4] In this setting, a reduced rank coefficient matrix can be found by keeping just the top 



n


{\displaystyle n}

 singular values, but this can be extended to keep any reduced set of singular values and vectors.","Sparse optimization has become the focus of much research interest as a way to find solutions that depend on a small number of variables (see e.g. the Lasso method). In principle, entry-wise sparsity can be enforced by penalizing the entry-wise 




ℓ

0




{\displaystyle \ell ^{0}}

-norm of the matrix, but the 




ℓ

0




{\displaystyle \ell ^{0}}

-norm is not convex. In practice this can be implemented by convex relaxation to the 




ℓ

1




{\displaystyle \ell ^{1}}

-norm. While entry-wise regularization with an 




ℓ

1




{\displaystyle \ell ^{1}}

-norm will find solutions with a small number of nonzero elements, applying an 




ℓ

1




{\displaystyle \ell ^{1}}

-norm to different groups of variables can enforce structure in the sparsity of solutions.[5]","The most straightforward example of structured sparsity uses the 




ℓ

p
,
q




{\displaystyle \ell _{p,q}}

 norm with 



p
=
2


{\displaystyle p=2}

 and 



q
=
1


{\displaystyle q=1}

:","For example, the 




ℓ

2
,
1




{\displaystyle \ell _{2,1}}

 norm is used in multi-task learning to group features across tasks, such that all the elements in a given row of the coefficient matrix can be forced to zero as a group.[6] The grouping effect is achieved by taking the 




ℓ

2




{\displaystyle \ell ^{2}}

-norm of each row, and then taking the total penalty to be the sum of these row-wise norms. This regularization results in rows that will tend to be all zeros, or dense. The same type of regularization can be used to enforce sparsity column-wise by taking the 




ℓ

2




{\displaystyle \ell ^{2}}

-norms of each column.","More generally, the 




ℓ

2
,
1




{\displaystyle \ell _{2,1}}

 norm can be applied to arbitrary groups of variables:","where the index 



g


{\displaystyle g}

 is across groups of variables, and 




|


G

g



|



{\displaystyle |G_{g}|}

 indicates the cardinality of group 



g


{\displaystyle g}

.","Algorithms for solving these group sparsity problems extend the more well-known Lasso and group Lasso methods by allowing overlapping groups, for example, and have been implemented via matching pursuit:[7] and proximal gradient methods.[8] By writing the proximal gradient with respect to a given coefficient, 




w

g


i




{\displaystyle w_{g}^{i}}

, it can be seen that this norm enforces a group-wise soft threshold[1]","where 





1


∥

w

g



∥

g


≥
λ




{\displaystyle \mathbf {1} _{\|w_{g}\|_{g}\geq \lambda }}

 is the indicator function for group norms 



≥
λ


{\displaystyle \geq \lambda }

.","Thus, using 




ℓ

2
,
1




{\displaystyle \ell _{2,1}}

 norms it is straightforward to enforce structure in the sparsity of a matrix either row-wise, column-wise, or in arbitrary blocks. By enforcing group norms on blocks in multivariate or multi-task regression, for example, it is possible to find groups of input and output variables, such that defined subsets of output variables (columns in the matrix 



Y


{\displaystyle Y}

) will depend on the same sparse set of input variables.","The ideas of structured sparsity and feature selection can be extended to the nonparametric case of multiple kernel learning.[9] This can be useful when there are multiple types of input data (color and texture, for example) with different appropriate kernels for each, or when the appropriate kernel is unknown. If there are two kernels, for example, with feature maps 



A


{\displaystyle A}

 and 



B


{\displaystyle B}

 that lie in corresponding reproducing kernel Hilbert spaces 






H

A




,



H

B






{\displaystyle {\mathcal {H_{A}}},{\mathcal {H_{B}}}}

, then a larger space, 






H

D






{\displaystyle {\mathcal {H_{D}}}}

, can be created as the sum of two spaces:","assuming linear independence in 



A


{\displaystyle A}

 and 



B


{\displaystyle B}

. In this case the 




ℓ

2
,
1




{\displaystyle \ell _{2,1}}

-norm is again the sum of norms:","Thus, by choosing a matrix regularization function as this type of norm, it is possible to find a solution that is sparse in terms of which kernels are used, but dense in the coefficient of each used kernel. Multiple kernel learning can also be used as a form of nonlinear variable selection, or as a model aggregation technique (e.g. by taking the sum of squared norms and relaxing sparsity constraints). For example, each kernel can be taken to be the Gaussian kernel with a different width."
"The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975.[1] It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between −1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and −1 indicates total disagreement between prediction and observation. The statistic is also known as the phi coefficient. MCC is related to the chi-square statistic for a 2×2 contingency table",where n is the total number of observations.,"While there is no perfect way of describing the confusion matrix of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures.[2] Other measures, such as the proportion of correct predictions (also termed accuracy), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.",The MCC can be calculated directly from the confusion matrix using the formula:,"In this equation, TP is the number of true positives, TN the number of true negatives, FP the number of false positives and FN the number of false negatives. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.",The original formula as given by Matthews was:[1],"This is equal to the formula given above. As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are Markedness (Δp) and Youden's J statistic (Informedness or Δp').[2][3] Markedness and Informedness correspond to different directions of information flow and generalize Youden's J statistic, the deltap statistics and (as their geometric mean) the Matthews Correlation Coefficient to more than two classes.[2]",Sources: Fawcett (2006) and Powers (2011).[2][4],"Let us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2×2 contingency table or confusion matrix, as follows:",
"Meta learning is a subfield of Machine learning where automatic learning algorithms are applied on meta-data about machine learning experiments. Although different researchers hold different views as to what the term exactly means (see below), the main goal is to use such meta-data to understand how automatic learning can become flexible in solving different kinds of learning problems, hence to improve the performance of existing learning algorithms.","Flexibility is very important because each learning algorithm is based on a set of assumptions about the data, its inductive bias. This means that it will only learn well if the bias matches the data in the learning problem. A learning algorithm may perform very well on one learning problem, but very badly on the next. From a non-expert point of view, this poses strong restrictions on the use of machine learning or data mining techniques, since the relationship between the learning problem (often some kind of database) and the effectiveness of different learning algorithms is not yet understood.","By using different kinds of meta-data, like properties of the learning problem, algorithm properties (like performance measures), or patterns previously derived from the data, it is possible to select, alter or combine different learning algorithms to effectively solve a given learning problem. Critiques of meta learning approaches bear a strong resemblance to the critique of metaheuristic, which can be said to be a related problem.",,,A proposed definition[1] for what qualifies as a meta learning system considers three requirements:,"The term bias in the last point refers to the set of assumptions influencing the choice of hypotheses for explaining the data[2] and must not be confused with the notion of bias represented in the bias-variance dilemma. Meta learning is concerned with two aspects of learning bias; declarative bias specifies the representation of the space of hypotheses, and affects the size of the search space (i.e. represent hypotheses using linear functions only) while procedural bias imposes constraints on the ordering of the inductive hypotheses (i.e. preferring smaller hypotheses).","These are some of the views on (and approaches to) meta learning, please note that there exist many variations on these general approaches:"
"In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with ""mixture distributions"" relate to deriving the properties of the overall population from those of the sub-populations, ""mixture models"" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.","Some ways of implementing mixture models involve steps that attribute postulated sub-population-identities to individual observations (or weights towards such sub-populations), in which case these can be regarded as types of unsupervised learning or clustering procedures. However, not all inference procedures involve such steps.","Mixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size of the population has been normalized to 1.",,,A typical finite-dimensional mixture model is a hierarchical model consisting of the following components:,"In addition, in a Bayesian setting, the mixture weights and parameters will themselves be random variables, and prior distributions will be placed over the variables. In such a case, the weights are typically viewed as a K-dimensional random vector drawn from a Dirichlet distribution (the conjugate prior of the categorical distribution), and the parameters will be distributed according to their respective conjugate priors.","Mathematically, a basic parametric mixture model can be described as follows:","In a Bayesian setting, all parameters are associated with random variables, as follows:","This characterization uses F and H to describe arbitrary distributions over observations and parameters, respectively. Typically H will be the conjugate prior of F. The two most common choices of F are Gaussian aka ""normal"" (for real-valued observations) and categorical (for discrete observations). Other common possibilities for the distribution of the mixture components are:",A typical non-Bayesian Gaussian mixture model looks like this:,A Bayesian version of a Gaussian mixture model is as follows:,"A Bayesian Gaussian mixture model is commonly extended to fit a vector of unknown parameters (denoted in bold), or multivariate normal distributions. In a multivariate distribution (i.e. one modelling a vector 




x



{\displaystyle {\boldsymbol {x}}}

 with N random variables) one may model a vector of parameters (such as several observations of a signal or patches within an image) using a Gaussian mixture model prior distribution on the vector of estimates given by","where the ith vector component is characterized by normal distributions with weights 




ϕ

i




{\displaystyle \phi _{i}}

, means 





μ

i





{\displaystyle {\boldsymbol {\mu _{i}}}}

 and covariance matrices 





Σ

i





{\displaystyle {\boldsymbol {\Sigma _{i}}}}

. To incorporate this prior into a Bayesian estimation, the prior is multiplied with the known distribution 



p
(

x

|

θ

)


{\displaystyle p({\boldsymbol {x|\theta }})}

 of the data 




x



{\displaystyle {\boldsymbol {x}}}

 conditioned on the parameters 




θ



{\displaystyle {\boldsymbol {\theta }}}

 to be estimated. With this formulation, the posterior distribution 



p
(

θ

|

x

)


{\displaystyle p({\boldsymbol {\theta |x}})}

 is also a Gaussian mixture model of the form","with new parameters 







ϕ

i


~



,




μ

i


~





{\displaystyle {\tilde {\phi _{i}}},{\boldsymbol {\tilde {\mu _{i}}}}}

 and 







Σ

i


~





{\displaystyle {\boldsymbol {\tilde {\Sigma _{i}}}}}

 that are updated using the EM algorithm. [1] Although EM-based parameter updates are well-established, providing the initial estimates for these parameters is currently an area of active research. Note that this formulation yields a closed-form solution to the complete posterior distribution. Estimations of the random variable 




θ



{\displaystyle {\boldsymbol {\theta }}}

 may be obtained via one of several estimators, such as the mean or maximum of the posterior distribution.","Such distributions are useful for assuming patch-wise shapes of images and clusters, for example. In the case of image representation, each Gaussian may be tilted, expanded, and warped according to the covariance matrices 





Σ

i





{\displaystyle {\boldsymbol {\Sigma _{i}}}}

. One Gaussian distribution of the set is fit to each patch (usually of size 8x8 pixels) in the image. Notably, any distribution of points around a cluster (see k-means) may be accurately given enough Gaussian components, but scarcely over K=20 components are needed to accurately model a given image distribution or cluster of data.",A typical non-Bayesian mixture model with categorical observations looks like this:,The random variables:,,A typical Bayesian mixture model with categorical observations looks like this:,The random variables:,,"Financial returns often behave differently in normal situations and during crisis times. A mixture model [2] for return data seems reasonable. Sometimes the model used is a jump-diffusion model, or as a mixture of two normal distributions. See Financial economics#Challenges and criticism for further context.","Assume that we observe the prices of N different houses. Different types of houses in different neighborhoods will have vastly different prices, but the price of a particular type of house in a particular neighborhood (e.g., three-bedroom house in moderately upscale neighborhood) will tend to cluster fairly closely around the mean. One possible model of such prices would be to assume that the prices are accurately described by a mixture model with K different components, each distributed as a normal distribution with unknown mean and variance, with each component specifying a particular combination of house type/neighborhood. Fitting this model to observed prices, e.g., using the expectation-maximization algorithm, would tend to cluster the prices according to house type/neighborhood and reveal the spread of prices in each type/neighborhood. (Note that for values such as prices or incomes that are guaranteed to be positive and which tend to grow exponentially, a log-normal distribution might actually be a better model than a normal distribution.)","Assume that a document is composed of N different words from a total vocabulary of size V, where each word corresponds to one of K possible topics. The distribution of such words could be modelled as a mixture of K different V-dimensional categorical distributions. A model of this sort is commonly termed a topic model. Note that expectation maximization applied to such a model will typically fail to produce realistic results, due (among other things) to the excessive number of parameters. Some sorts of additional assumptions are typically necessary to get good results. Typically two sorts of additional components are added to the model:","The following example is based on an example in Christopher M. Bishop, Pattern Recognition and Machine Learning.[3]","Imagine that we are given an N×N black-and-white image that is known to be a scan of a hand-written digit between 0 and 9, but we don't know which digit is written. We can create a mixture model with 



K
=
10


{\displaystyle K=10}

 different components, where each component is a vector of size 




N

2




{\displaystyle N^{2}}

 of Bernoulli distributions (one per pixel). Such a model can be trained with the expectation-maximization algorithm on an unlabeled set of hand-written digits, and will effectively cluster the images according to the digit being written. The same model could then be used to recognize the digit of another image simply by holding the parameters constant, computing the probability of the new image for each possible digit (a trivial calculation), and returning the digit that generated the highest probability.","Mixture models apply in the problem of directing multiple projectiles at a target (as in air, land, or sea defense applications), where the physical and/or statistical characteristics of the projectiles differ within the multiple projectiles. An example might be shots from multiple munitions types or shots from multiple locations directed at one target. The combination of projectile types may be characterized as a Gaussian mixture model.[4] Further, a well-known measure of accuracy for a group of projectiles is the circular error probable (CEP), which is the number R such that, on average, half of the group of projectiles falls within the circle of radius R about the target point. The mixture model can be used to determine (or estimate) the value R. The mixture model properly captures the different types of projectiles.","The financial example above is one direct application of the mixture model, a situation in which we assume an underlying mechanism so that each observation belongs to one of some number of different sources or categories. This underlying mechanism may or may not, however, be observable. In this form of mixture, each of the sources is described by a component probability density function, and its mixture weight is the probability that an observation comes from this component.","In an indirect application of the mixture model we do not assume such a mechanism. The mixture model is simply used for its mathematical flexibilities. For example, a mixture of two normal distributions with different means may result in a density with two modes, which is not modeled by standard parametric distributions. Another example is given by the possibility of mixture distributions to model fatter tails than the basic Gaussian ones, so as to be a candidate for modeling more extreme events. When combined with dynamical consistency, this approach has been applied to financial derivatives valuation in presence of the volatility smile in the context of local volatility models. This defines our application.","In image processing and computer vision, traditional image segmentation models often assign to one pixel only one exclusive pattern. In fuzzy or soft segmentation, any pattern can have certain ""ownership"" over any single pixel. If the patterns are Gaussian, fuzzy segmentation naturally results in Gaussian mixtures. Combined with other analytic or geometric tools (e.g., phase transitions over diffusive boundaries), such spatially regularized mixture models could lead to more realistic and computationally efficient segmentation methods.[5]",Identifiability refers to the existence of a unique characterization for any one of the models in the class (family) being considered. Estimation procedure may not be well-defined and asymptotic theory may not hold if a model is not identifiable.,Let J be the class of all binomial distributions with n = 2. Then a mixture of two members of J would have,"and p2 = 1 − p0 − p1. Clearly, given p0 and p1, it is not possible to determine the above mixture model uniquely, as there are three parameters (π, θ1, θ2) to be determined.",Consider a mixture of parametric distributions of the same class. Let,be the class of all component distributions. Then the convex hull K of J defines the class of all finite mixture of distributions in J:,"K is said to be identifiable if all its members are unique, that is, given two members p and p′ in K, being mixtures of k distributions and k′ distributions respectively in J, we have p = p′ if and only if, first of all, k = k′ and secondly we can reorder the summations such that ai = ai′ and ƒi = ƒi′ for all i.","Parametric mixture models are often used when we know the distribution Y and we can sample from X, but we would like to determine the ai and θi values. Such situations can arise in studies in which we sample from a population that is composed of several distinct subpopulations.","It is common to think of probability mixture modeling as a missing data problem. One way to understand this is to assume that the data points under consideration have ""membership"" in one of the distributions we are using to model the data. When we start, this membership is unknown, or missing. The job of estimation is to devise appropriate parameters for the model functions we choose, with the connection to the data points being represented as their membership in the individual model distributions.","A variety of approaches to the problem of mixture decomposition have been proposed, many of which focus on maximum likelihood methods such as expectation maximization (EM) or maximum a posteriori estimation (MAP). Generally these methods consider separately the question of parameter estimation and system identification, that is to say a distinction is made between the determination of the number and functional form of components within a mixture and the estimation of the corresponding parameter values. Some notable departures are the graphical methods as outlined in Tarter and Lock [6] and more recently minimum message length (MML) techniques such as Figueiredo and Jain [7] and to some extent the moment matching pattern analysis routines suggested by McWilliam and Loh (2009).[8]",Expectation maximization (EM) is seemingly the most popular technique used to determine the parameters of a mixture with an a priori given number of components. This is a particular way of implementing maximum likelihood estimation for this problem. EM is of particular appeal for finite normal mixtures where closed-form expressions are possible such as in the following iterative algorithm by Dempster et al. (1977)[9],with the posterior probabilities,"Thus on the basis of the current estimate for the parameters, the conditional probability for a given observation x(t) being generated from state s is determined for each t = 1, …, N ; N being the sample size. The parameters are then updated such that the new component weights correspond to the average conditional probability and each component mean and covariance is the component specific weighted average of the mean and covariance of the entire sample.","Dempster[9] also showed that each successive EM iteration will not decrease the likelihood, a property not shared by other gradient based maximization techniques. Moreover, EM naturally embeds within it constraints on the probability vector, and for sufficiently large sample sizes positive definiteness of the covariance iterates. This is a key advantage since explicitly constrained methods incur extra computational costs to check and maintain appropriate values. Theoretically EM is a first-order algorithm and as such converges slowly to a fixed-point solution. Redner and Walker (1984)[full citation needed] make this point arguing in favour of superlinear and second order Newton and quasi-Newton methods and reporting slow convergence in EM on the basis of their empirical tests. They do concede that convergence in likelihood was rapid even if convergence in the parameter values themselves was not. The relative merits of EM and other algorithms vis-à-vis convergence have been discussed in other literature.[10]","Other common objections to the use of EM are that it has a propensity to spuriously identify local maxima, as well as displaying sensitivity to initial values.[11][12] One may address these problems by evaluating EM at several initial points in the parameter space but this is computationally costly and other approaches, such as the annealing EM method of Udea and Nakano (1998) (in which the initial components are essentially forced to overlap, providing a less heterogeneous basis for initial guesses), may be preferable.","Figueiredo and Jain [7] note that convergence to 'meaningless' parameter values obtained at the boundary (where regularity conditions breakdown, e.g., Ghosh and Sen (1985)) is frequently observed when the number of model components exceeds the optimal/true one. On this basis they suggest a unified approach to estimation and identification in which the initial n is chosen to greatly exceed the expected optimal value. Their optimization routine is constructed via a minimum message length (MML) criterion that effectively eliminates a candidate component if there is insufficient information to support it. In this way it is possible to systematize reductions in n and consider estimation and identification jointly.",The Expectation-maximization algorithm can be used to compute the parameters of a parametric mixture model distribution (the ai and θi). It is an iterative algorithm with two steps: an expectation step and a maximization step. Practical examples of EM and Mixture Modeling are included in the SOCR demonstrations.,"With initial guesses for the parameters of our mixture model, ""partial membership"" of each data point in each constituent distribution is computed by calculating expectation values for the membership variables of each data point. That is, for each data point xj and distribution Yi, the membership value yi, j is:","With expectation values in hand for group membership, plug-in estimates are recomputed for the distribution parameters.",The mixing coefficients ai are the means of the membership values over the N data points.,"The component model parameters θi are also calculated by expectation maximization using data points xj that have been weighted using the membership values. For example, if θ is a mean μ","With new estimates for ai and the θi's, the expectation step is repeated to recompute new membership values. The entire procedure is repeated until model parameters converge.","As an alternative to the EM algorithm, the mixture model parameters can be deduced using posterior sampling as indicated by Bayes' theorem. This is still regarded as an incomplete data problem whereby membership of data points is the missing data. A two-step iterative procedure known as Gibbs sampling can be used.","The previous example of a mixture of two Gaussian distributions can demonstrate how the method works. As before, initial guesses of the parameters for the mixture model are made. Instead of computing partial memberships for each elemental distribution, a membership value for each data point is drawn from a Bernoulli distribution (that is, it will be assigned to either the first or the second Gaussian). The Bernoulli parameter θ is determined for each data point on the basis of one of the constituent distributions.[vague] Draws from the distribution generate membership associations for each data point. Plug-in estimators can then be used as in the M step of EM to generate a new set of mixture model parameters, and the binomial draw step repeated.","The method of moment matching is one of the oldest techniques for determining the mixture parameters dating back to Karl Pearson’s seminal work of 1894. In this approach the parameters of the mixture are determined such that the composite distribution has moments matching some given value. In many instances extraction of solutions to the moment equations may present non-trivial algebraic or computational problems. Moreover, numerical analysis by Day [13] has indicated that such methods may be inefficient compared to EM. Nonetheless there has been renewed interest in this method, e.g., Craigmile and Titterington (1998) and Wang.[14]",McWilliam and Loh (2009) consider the characterisation of a hyper-cuboid normal mixture copula in large dimensional systems for which EM would be computationally prohibitive. Here a pattern analysis routine is used to generate multivariate tail-dependencies consistent with a set of univariate and (in some sense) bivariate moments. The performance of this method is then evaluated using equity log-return data with Kolmogorov–Smirnov test statistics suggesting a good descriptive fit.,"Some problems in mixture model estimation can be solved using spectral methods. In particular it becomes useful if data points xi are points in high-dimensional real space, and the hidden distributions are known to be log-concave (such as Gaussian distribution or Exponential distribution).","Spectral methods of learning mixture models are based on the use of Singular Value Decomposition of a matrix which contains data points. The idea is to consider the top k singular vectors, where k is the number of distributions to be learned. The projection of each data point to a linear subspace spanned by those vectors groups points originating from the same distribution very close together, while points from different distributions stay far apart.","One distinctive feature of the spectral method is that it allows us to prove that if distributions satisfy certain separation condition (e.g., not too close), then the estimated mixture will be very close to the true one with high probability.",Tarter and Lock [6] describe a graphical approach to mixture identification in which a kernel function is applied to an empirical frequency plot so to reduce intra-component variance. In this way one may more readily identify components having differing means. While this λ-method does not require prior knowledge of the number or functional form of the components its success does rely on the choice of the kernel parameters which to some extent implicitly embeds assumptions about the component structure.,"Some of them can even probably learn mixtures of heavy-tailed distributions including those with infinite variance (see links to papers below). In this setting, EM based methods would not work, since the Expectation step would diverge due to presence of outliers.","To simulate a sample of size N that is from a mixture of distributions Fi, i=1 to n, with probabilities pi (sum= pi = 1):","In a Bayesian setting, additional levels can be added to the graphical model defining the mixture model. For example, in the common latent Dirichlet allocation topic model, the observations are sets of words drawn from D different documents and the K mixture components represent topics that are shared across documents. Each document has a different set of mixture weights, which specify the topics prevalent in that document. All sets of mixture weights share common hyperparameters.","A very common extension is to connect the latent variables defining the mixture component identities into a Markov chain, instead of assuming that they are independent identically distributed random variables. The resulting model is termed a hidden Markov model and is one of the most common sequential hierarchical models. Numerous extensions of hidden Markov models have been developed; see the resulting article for more information.","Mixture distributions and the problem of mixture decomposition, that is the identification of its constituent components and the parameters thereof, has been cited in the literature as far back as 1846 (Quetelet in McLachlan ,[11] 2000) although common reference is made to the work of Karl Pearson (1894)[citation needed] as the first author to explicitly address the decomposition problem in characterising non-normal attributes of forehead to body length ratios in female shore crab populations. The motivation for this work was provided by the zoologist Walter Frank Raphael Weldon who had speculated in 1893 (in Tarter and Lock[6]) that asymmetry in the histogram of these ratios could signal evolutionary divergence. Pearson’s approach was to fit a univariate mixture of two normals to the data by choosing the five parameters of the mixture such that the empirical moments matched that of the model.","While his work was successful in identifying two potentially distinct sub-populations and in demonstrating the flexibility of mixtures as a moment matching tool, the formulation required the solution of a 9th degree (nonic) polynomial which at the time posed a significant computational challenge.","Subsequent works focused on addressing these problems, but it was not until the advent of the modern computer and the popularisation of Maximum Likelihood (ML) parameterisation techniques that research really took off.[15] Since that time there has been a vast body of research on the subject spanning areas such as Fisheries research, Agriculture, Botany, Economics, Medicine, Genetics, Psychology, Palaeontology, Electrophoresis, Finance, Sedimentology/Geology and Zoology.[16]"
"Mountain Car, a standard testing domain in reinforcement learning, is a problem in which an under-powered car must drive up a steep hill. Since gravity is stronger than the car's engine, even at full throttle, the car cannot simply accelerate up the steep slope. The car is situated in a valley and must learn to leverage potential energy by driving up the opposite hill before the car is able to make it to the goal at the top of the rightmost hill. The domain has been used as a test bed in various reinforcement learning papers.",,,"The mountain car problem, although fairly simple, is commonly applied because it requires a reinforcement learning agent to learn on two continuous variables: position and velocity. For any given state (position and velocity) of the car, the agent is given the possibility of driving left, driving right, or not using the engine at all. In the standard version of the problem, the agent receives a negative reward at every time step when the goal is not reached; the agent has no information about the goal until an initial success.","The mountain car problem appeared first in Andrew Moore's PhD Thesis (1990).[1] It was later more strictly defined in Singh and Sutton's Reinforcement Leaning paper with eligibility traces.[2] The problem became more widely studied when Sutton and Barto added it to their book Reinforcement Learning: An Introduction (1998).[3] Throughout the years many versions of the problem have been used, such as those which modify the reward function, termination condition, and/or the start state.","Q-learning and similar techniques for mapping discrete states to discrete actions need to be extended to be able to deal with the continuous state space of the problem. Approaches often fall into one of two categories, state space discretization or function approximation.","In this approach, two continuous state variables are pushed into discrete states by bucketing each continuous variable into multiple discrete states. This approach works with properly tuned parameters but a disadvantage is information gathered from one state is not used to evaluate another state. Tile coding can be used to improve discretization and involves continuous variables mapping into sets of buckets offset from one another. Each step of training has a wider impact on the value function approximation because when the offset grids are summed, the information is diffused.[4]","Function approximation is another way to solve the mountain car. By choosing a set of basis functions beforehand, or by generating them as the car drives, the agent can approximate the value function at each state. Unlike the step-wise version of the value function created with discretization, function approximation can more cleanly estimate the true smooth function of the mountain car domain.[5]","An interesting aspect of the problem involves the delay of actual reward. The agent isn't able to learn about the goal until a successful completion. Given a naive approach without traces, for each trial the car can only backup the reward of the goal slightly. This is a problem for naive discretization because each discrete state will only be backup once, taking a larger number of episodes to learn the problem. To alleviate this problem, traces will automatically backup the reward given to states before dramatically increasing the speed of learning.",The mountain car problem has undergone many iterations. This section will focus on the standard well defined version from Sutton (2008).[6],Two dimensional continuous state space.,"



V
e
l
o
c
i
t
y
=
(
−
0.07
,
0.07
)


{\displaystyle Velocity=(-0.07,0.07)}

","



P
o
s
i
t
i
o
n
=
(
−
1.2
,
0.6
)


{\displaystyle Position=(-1.2,0.6)}

",One-dimensional discrete action space.,"



m
o
t
o
r
=
(
l
e
f
t
,
n
e
u
t
r
a
l
,
r
i
g
h
t
)


{\displaystyle motor=(left,neutral,right)}

","For every time step, with height 0 being the lowest point in the valley:","



r
e
w
a
r
d
=
−
1
+
h
e
i
g
h
t


{\displaystyle reward=-1+height}

",For every time step:,"



A
c
t
i
o
n
=
[
−
1
,
0
,
1
]


{\displaystyle Action=[-1,0,1]}

","



V
e
l
o
c
i
t
y
=
V
e
l
o
c
i
t
y
+
(
A
c
t
i
o
n
)
∗
0.001
+
cos
⁡
(
3
∗
P
o
s
i
t
i
o
n
)
∗
(
−
0.0025
)


{\displaystyle Velocity=Velocity+(Action)*0.001+\cos(3*Position)*(-0.0025)}

","



P
o
s
i
t
i
o
n
=
P
o
s
i
t
i
o
n
+
V
e
l
o
c
i
t
y


{\displaystyle Position=Position+Velocity}

","Optionally, many implementations include randomness in both parameters to show better generalized learning.","



P
o
s
i
t
i
o
n
=
−
0.5


{\displaystyle Position=-0.5}

","



V
e
l
o
c
i
t
y
=
0.0


{\displaystyle Velocity=0.0}

",End the simulation when:,"



P
o
s
i
t
i
o
n
≥
0.6


{\displaystyle Position\geq 0.6}

","There are many versions of the mountain car which deviate in different ways from the standard model. Variables that vary include but are not limited to changing the constants (gravity and steepness) of the problem so specific tuning for specific policies become irrelevant and altering the reward function to affect the agent's ability to learn in a different manner. An example is changing the reward to be equal to the distance from the goal, or changing the reward to zero everywhere and one at the goal. Additionally we can use a 3D mountain car with a 4D continuous state space.[7]"
"Movidius is a company based in San Mateo, California that designs specialised low-power processor chips for computer vision.",,,"Movidius was co-founded in in Dublin in 2005 by Dr. Sean Mitchell and David Moloney.[1][2] Between 2006 and 2016, it raised nearly $90 million in capital funding.[3]","In May, 2013 the company appointed Remi El-Ouazzane as CEO.[4]","In January, 2016 the company announced a partnership with Google.[5] Movidius has been active in the Google Project Tango project.[citation needed]","As of February 2016[update], Movidius's latest Myriad 2 chip is an always-on manycore Vision processing unit that can function on power constrained devices.[5] It is a heterogeneous architecture, combining several 128bit VLIW SIMD processors connected to a multiported Scratchpad memory, a pair of SPARC based processors for control, and a number of fixed function units to accelerate specific video processing tasks (such as small Convolutions and color conversion lookups). It includes camera interface hardware, bypassing the need for external memory buffers when handling realtime image inputs. In terms of software, a Visual programming language allows workflows to be devised, and there is support for OpenCL.","Fathom is a USB stick containing a Myriad 2 processor, allowing a vision accelerator to be easily added to devices using ARM processors including PCs, drones, robots, IoT devices and video surveillance for tasks such as identifying people or objects. It can run between 80 and 150 GFLOPS performance at below 1W of power. The company switched from a previous 65nm process to a 28nm one to increase its chip’s efficiency by 20-30x. The Fathom is expected to cost under $100 per unit.[6]"
"In probability theory, the multi-armed bandit problem (sometimes called the K-[1] or N-armed bandit problem[2]) is a problem in which a gambler at a row of slot machines (sometimes known as ""one-armed bandits"") has to decide which machines to play, how many times to play each machine and in which order to play them.[3] When played, each machine provides a random reward from a probability distribution specific to that machine. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls.[4][5]","Robbins in 1952, realizing the importance of the problem, constructed convergent population selection strategies in ""some aspects of the sequential design of experiments"".[6]","A theorem, the Gittins index, first published by John C. Gittins, gives an optimal policy for maximizing the expected discounted reward.[7]","In practice, multi-armed bandits have been used to model the problem of managing research projects in a large organization, like a science foundation or a pharmaceutical company. Given a fixed budget, the problem is to allocate resources among the competing projects, whose properties are only partially known at the time of allocation, but which may become better understood as time passes.[4][5]","In early versions of the multi-armed bandit problem, the gambler has no initial knowledge about the machines. The crucial tradeoff the gambler faces at each trial is between ""exploitation"" of the machine that has the highest expected payoff and ""exploration"" to get more information about the expected payoffs of the other machines. The trade-off between exploration and exploitation is also faced in reinforcement learning.",,,"The multi-armed bandit problem models an agent that simultaneously attempts to acquire new knowledge (called ""exploration"") and optimize his or her decisions based on existing knowledge (called ""exploitation""). The agent attempts to balance these competing tasks in order to maximize his or her total value over the period of time considered. There are many practical applications of the bandit model, for example:","In these practical examples, the problem requires balancing reward maximization based on the knowledge already acquired with attempting new actions to further increase knowledge. This is known as the exploitation vs. exploration tradeoff in reinforcement learning.","The model has also been used to control dynamic allocation of resources to different projects, answering the question of which project to work on, given uncertainty about the difficulty and payoff of each possibility.[12]","Originally considered by Allied scientists in World War II, it proved so intractable that, according to Peter Whittle, the problem was proposed to be dropped over Germany so that German scientists could also waste their time on it.[13]",The version of the problem now commonly analyzed was formulated by Herbert Robbins in 1952.,"The multi-armed bandit (short: bandit) can be seen as a set of real distributions 



B
=
{

R

1


,
…
,

R

K


}


{\displaystyle B=\{R_{1},\dots ,R_{K}\}}

, each distribution being associated with the rewards delivered by one of the 



K
∈


N


+




{\displaystyle K\in \mathbb {N} ^{+}}

 levers. Let 




μ

1


,
…
,

μ

K




{\displaystyle \mu _{1},\dots ,\mu _{K}}

 be the mean values associated with these reward distributions. The gambler iteratively plays one lever per round and observes the associated reward. The objective is to maximize the sum of the collected rewards. The horizon 



H


{\displaystyle H}

 is the number of rounds that remain to be played. The bandit problem is formally equivalent to a one-state Markov decision process. The regret 



ρ


{\displaystyle \rho }

 after 



T


{\displaystyle T}

 rounds is defined as the expected difference between the reward sum associated with an optimal strategy and the sum of the collected rewards: 



ρ
=
T

μ

∗


−

∑

t
=
1


T






r
^




t




{\displaystyle \rho =T\mu ^{*}-\sum _{t=1}^{T}{\widehat {r}}_{t}}

, where 




μ

∗




{\displaystyle \mu ^{*}}

 is the maximal reward mean, 




μ

∗


=

max

k


{

μ

k


}


{\displaystyle \mu ^{*}=\max _{k}\{\mu _{k}\}}

, and 







r
^




t




{\displaystyle {\widehat {r}}_{t}}

 is the reward at time t.","A zero-regret strategy is a strategy whose average regret per round 



ρ

/

T


{\displaystyle \rho /T}

 tends to zero with probability 1 when the number of played rounds tends to infinity.[14] Intuitively, zero-regret strategies are guaranteed to converge to a (not necessarily unique) optimal strategy if enough rounds are played.","A common formulation is the Binary multi-armed bandit or Bernoulli multi-armed bandit, which issues a reward of one with probability 



p


{\displaystyle p}

, and otherwise a reward of zero.","Another formulation of the multi-armed bandit has each arm representing an independent Markov machine. Each time a particular arm is played, the state of that machine advances to a new one, chosen according to the Markov state evolution probabilities. There is a reward depending on the current state of the machine. In a generalisation called the ""restless bandit problem"", the states of non-played arms can also evolve over time.[15] There has also been discussion of systems where the number of choices (about which arm to play) increases over time.[16]","Computer science researchers have studied multi-armed bandits under worst-case assumptions, obtaining algorithms to minimize regret in both finite and infinite (asymptotic) time horizons for both stochastic [1] and non-stochastic[17] arm payoffs.","A major breakthrough was the construction of optimal population selection strategies, or policies (that possess uniformly maximum convergence rate to the population with highest mean) in the work described below.","In the paper ""Asymptotically efficient adaptive allocation rules"", Lai and Robbins[18] (following papers of Robbins and his co-workers going back to Robbins in the year 1952) constructed convergent population selection policies that possess the fastest rate of convergence (to the population with highest mean) for the case that the population reward distributions are the one-parameter exponential family. Then, in Katehakis and Robbins [19] simplifications of the policy and the main proof were given for the case of normal populations with known variances. The next notable progress was obtained by Burnetas and Katehakis in the paper ""Optimal adaptive policies for sequential allocation problems"",[20] where index based policies with uniformly maximum convergence rate were constructed, under more general conditions that include the case in which the distributions of outcomes from each population depend on a vector of unknown parameters. Burnetas and Katehakis (1996) also provided an explicit solution for the important case in which the distributions of outcomes follow arbitrary (i.e., nonparametric) discrete, univariate distributions.","Later in ""Optimal adaptive policies for Markov decision processes""[21] Burnetas and Katehakis studied the much larger model of Markov Decision Processes under partial information, where the transition law and/or the expected one period rewards may depend on unknown parameter. In this work the explicit form for a class of adaptive policies that possess uniformly maximum convergence rate properties for the total expected finite horizon reward, were constructed under sufficient assumptions of finite state-action spaces and irreducibility of the transition law. A main feature of these policies is that the choice of actions, at each state and time period, is based on indices that are inflations of the right-hand side of the estimated average reward optimality equations. These inflations have recently been called the optimistic approach in the work of Tewari and Bartlett,[22] Ortner[23] Filippi, Cappé, and Garivier,[24] and Honda and Takemura.[25]","Many strategies exist which provide an approximate solution to the bandit problem, and can be put into the four broad categories detailed below.",Semi-uniform strategies were the earliest (and simplest) strategies discovered to approximately solve the bandit problem. All those strategies have in common a greedy behavior where the best lever (based on previous observations) is always pulled except when a (uniformly) random action is taken.,"Probability matching strategies reflect the idea that the number of pulls for a given lever should match its actual probability of being the optimal lever. Probability matching strategies are also known as Thompson sampling or Bayesian Bandits,[29] and surprisingly easy to implement if you can sample from the posterior for the mean value of each alternative.",Probability matching strategies also admit solutions to so-called contextual bandit problems.,"Pricing strategies establish a price for each lever. For example, as illustrated with the POKER algorithm,[14] the price can be the sum of the expected reward plus an estimation of extra future rewards that will gain through the additional knowledge. The lever of highest price is always pulled.","These strategies minimize the assignment of any patient to an inferior arm (""physician's duty""). In a typical case, they minimize expected successes lost (ESL), that is, the expected number of favorable outcomes that were missed because of assignment to an arm later proved to be inferior. Another version minimizes resources wasted on any inferior, more expensive, treatment.[8]","A particularly useful version of the multi-armed bandit is the contextual multi-armed bandit problem. In this problem, in each iteration an agent has to choose between arms. Before making the choice, the agent sees a d-dimensional feature vector (context vector), associated with the current iteration. The learner uses these context vectors along with the rewards of the arms played in the past to make the choice of the arm to play in the current iteration. Over time, the learner's aim is to collect enough information about how the context vectors and rewards relate to each other, so that it can predict the next best arm to play by looking at the feature vectors.[30]","Many strategies exist which provide an approximate solution to the Contextual bandit problem, and can be put into two broad categories detailed below.","In practice, there is usually a cost associated with the resource consumed by each action and the total cost is limited by a budget in many applications such as crowdsourcing and clinical trials. Constrained contextual bandit (CCB) is such a model that consider both the time and budget constraints in multi-armed bandit setting. A. Badanidiyuru et al.[37] first studies the contextual bandits with budget constraints, also referred to as Resourceful Contextual Bandits, and show that a 



O
(


T


)


{\displaystyle O({\sqrt {T}})}

 regret is achievable. However,[37] focuses on a finite set of policies and the algorithm is computationally inefficient.",A simple algorithm with logarithmic regret is proposed in:[38],"Another variant of the multi-armed bandit problem is called the adversarial bandit, first introduced by Auer and Cesa-Bianchi (1998). In this variant, at each iteration an agent chooses an arm and an adversary simultaneously chooses the payoff structure for each arm. This is one of the strongest generalizations of the bandit problem[39] as it removes all assumptions of the distribution and a solution to the adversarial bandit problem is a generalized solution to the more specific bandit problems.","In the original specification and in the above variants, the bandit problem is specified with a discrete and finite number of arms, often indicated by the variable 



K


{\displaystyle K}

. In the infinite armed case, introduced by Agarwal (1995), the ""arms"" are a continuous variable in 



K


{\displaystyle K}

 dimensions.","The Dueling Bandit variant was introduced by Yue et al. (2012) [40] to model the exploration-versus-exploitation tradeoff for relative feedback. In this variant the gambler is allowed to pull two levers at the same time, but she only gets a binary feedback telling which lever provided the best reward. The difficulty of this problem stems from the fact that the gambler has no way of directly observing the reward of her actions. The earliest algorithms for this problem are InterleaveFiltering,[40] Beat-The-Mean.[41] The relative feedback of dueling bandits can also lead to Voting paradoxes. A solution is to take the Condorcet winner as a reference.[42]","More recently, researchers have generalized algorithms from traditional MAB to dueling bandits: Relative Upper Confidence Bounds (RUCB),[43] Relative EXponential weighing (REX3),[44] Copeland Confidence Bounds (CCB),[45] Relative Minimum Empirical Divergence (RMED),[46] and Double Thompson Sampling (DTS).[47]","Garivier and Moulines derive some of the first results with respect to bandit problems where the underlying model can change during play. A number of algorithms were presented to deal with this case, including Discounted UCB [48] and Sliding-Window UCB.[49]","Djallel Bouneffouf and Raphael Feraud,[50] they consider a variant where the trend is a priori known, that is, the gambler knows the path of the changing reward function of each arm but not its distribution. By adapting the standard multi-armed bandit algorithm UCB1 to take advantage of this setting, they propose an algorithm named Adjusted Upper Confidence Bound (A-UCB) that assumes a stochastic model. They also provide upper bounds of the regret which compare favorably with the ones of UCB1.","Another work by Burtini et al. introduces a weighted least squares Thompson sampling approach, which proves beneficial in both the known and unknown non-stationary cases [51]","The Clustering of Bandits (i.e., CLUB) was introduced by Gentile and Li and Zappela (ICML 2014),[52] with a novel algorithmic approach to content recommender systems based on adaptive clustering of exploration-exploitation (""bandit"") strategies. They provide a sharp regret analysis of this algorithm in a standard stochastic noise setting, demonstrate its scalability properties, and prove its effectiveness on a number of artificial and real-world datasets. Their experiments show a significant increase in prediction performance over state-of-the-art methods for bandit problems.","The distributed clustering of bandits (i.e., DCCB) was introduced by Korda and Szorenyi and Li (ICML 2016),[53] they provide two distributed confidence ball algorithms for solving linear bandit problems in peer to peer networks with limited communication capabilities. For the first, they assume that all the peers are solving the same linear bandit problem, and prove that their algorithm achieves the optimal asymptotic regret rate of any centralised algorithm that can instantly communicate information between the peers. For the second, they assume that there are clusters of peers solving the same bandit problem within each cluster as in,[52] and they prove that their algorithm discovers these clusters, while achieving the optimal asymptotic regret rate within each one. Through experiments on several real-world datasets, they demonstrate the performance of proposed algorithms compared to the state-of-the-art.","The collaborative filtering bandits (i.e., COFIBA) was introduced by Li and Karatzoglou and Gentile (SIGIR 2016),[54] where the classical collaborative filtering, and content-based filtering methods try to learn a static recommendation model given training data. These approaches are far from ideal in highly dynamic recommendation domains such as news recommendation and computational advertisement, where the set of items and users is very fluid. In this work, they investigate an adaptive clustering technique for content recommendation based on exploration-exploitation strategies in contextual multi-armed bandit settings.[52] Their algorithm (COFIBA, pronounced as ""Coffee Bar"") takes into account the collaborative effects[54] that arise due to the interaction of the users with the items, by dynamically grouping users based on the items under consideration and, at the same time, grouping items based on the similarity of the clusterings induced over the users. The resulting algorithm thus takes advantage of preference patterns in the data in a way akin to collaborative filtering methods. They provide an empirical analysis on medium-size real-world datasets, showing scalability and increased prediction performance (as measured by click-through rate) over state-of-the-art methods for clustering bandits. They also provide a regret analysis within a standard linear stochastic noise setting."
"Multi-task learning (MTL) is an approach to machine learning that learns a problem together with other related problems at the same time, using a shared representation. This often leads to a better model for the main task, because it allows the learner to use the commonality among the tasks.[1][2][3] Therefore, multi-task learning is a kind of inductive transfer. This type of machine learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.[4]","The goal of MTL is to improve the performance of learning algorithms by learning classifiers for multiple tasks jointly. This works particularly well if these tasks have some commonality and are generally slightly under sampled. One example is a spam-filter. Everybody has a slightly different distribution over spam or not-spam emails (e.g. all emails in Russian are spam for me—but not so for my Russian colleagues), yet there is definitely a common aspect across users. Multi-task learning works, because encouraging a classifier (or a modification thereof) to also perform well on a slightly different task is a better regularization than uninformed regularizers (e.g. to enforce that all weights are small).[5]",,,,RoboEarth is one group involved in multi-task learning.[6],"In the paradigm of multi-task learning, multiple related prediction tasks are learned jointly, sharing information across the tasks. One can also use a framework for multi-task learning that enables one to selectively share the information across the tasks. Assume that each task parameter vector is a linear combination of a ﬁnite number of underlying basis tasks. The coeﬃcients of the linear combination are sparse in nature and the overlap in the sparsity patterns of two tasks controls the amount of sharing across these. This model is based on the assumption that task parameters within a group lie in a low-dimensional subspace but allows the tasks in diﬀerent groups to overlap with each other in one or more bases.[7] Tasks may also be grouped on the basis of feature (sub)space shared among them.[8]","One can attempt learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneﬁcial. The reason is that prior knowledge about which tasks are unrelated can lead to sparser and more informative representations for each task, essentially screening out idiosyncrasies of the data distribution. Novel methods which builds on a prior multitask methodology by favouring a shared low-dimensional representation within each group of tasks have been proposed. The programmer can impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. Experiments on synthetic and real data have indicated that incorporating unrelated tasks can improve signiﬁcantly over standard multi-task learning methods.[9]",The Multi-Task Learning via StructurAl Regularization (MALSAR) package [10] implements the following multi-task learning algorithms:,"Using the principles of MTL, techniques for collaborative spam filtering that facilitates personalization have been proposed. In large scale open membership email systems, most users do not label enough messages for an individual local classifier to be effective, while the data is too noisy to be used for a global filter across all users. A hybrid global/individual classifier can be effective at absorbing the influence of users who label emails very diligently from the general public. This can be accomplished while still providing sufficient quality to users with few labeled instances.[21]","Using boosted decision trees, one can enable implicit data sharing and regularization. This learning method can be used on web-search ranking data sets. One example is to use ranking data sets from several countries. Here, multitask learning is particularly helpful as data sets from different countries vary largely in size because of the cost of editorial judgments. It has been demonstrated that learning various tasks jointly can lead to signiﬁcant improvements in performance with surprising reliability.[22]"
"Multilinear Principal Component Analysis (MPCA) is a multilinear extension of principal component analysis (PCA). MPCA is employed in the analysis of n-way arrays, i.e. a cube or hyper-cube of numbers, also informally referred to as a ""data tensor"". N-way arrays may be decomposed, analyzed, or modeled by","The origin of MPCA can be traced back to the Tucker decomposition[1] and Peter Kroonenberg's ""M-mode PCA/3-mode PCA"" work.[2] In 2000, De Lathauwer etal. restated Tucker and Kroonenberg's work in clear and concise numerical computational terms in their SIAM paper entitled Multilinear Singular Value Decomposition,[3] (HOSVD) and in their paper ""On the Best Rank-1 and Rank-(R1, R2, ..., RN ) Approximation of Higher-order Tensors"".[4]","Circa 2001, Vasilescu reframed the data analysis, recognition and synthesis problems as multilinear tensor problems based on the insight that most observed data are the compositional consequence of several causal factors of data formation, and are well suited for multi-modal data tensor analysis. The power of the tensor framework was showcased by analyzing human motion joint angels, facial images or textures in terms of their causal factors of data formation in the following works: Human Motion Signatures [5] (CVPR 2001, ICPR 2002), face recognition - TensorFaces, [6] [7] (ECCV 2002, CVPR 2003, etc.) and computer graphics -- TensorTextures[8](Siggraph 2004).","Historically, MPCA has been referred to as ""M-mode PCA"", a terminology which was coined by Peter Kroonenberg in 1980.[2] In 2005, Vasilescu and Terzopoulos introduced the Multilinear PCA[9] terminology as a way to better differentiate between linear and multilinear tensor decomposition, as well as, to better differentiate between the work[5][6][7][8] that computed 2nd order statistics associated with each data tensor mode(axis), and subsequent work on Multilinear Independent Component Analysis[9] that computed higher order statistics associated with each tensor mode/axis.","Multilinear PCA may be applied to compute the causal factors of data formation,or as signal processing tool on data tensors whose individual observation have either been vectorized [5] [6] [7] ,[8] or whose observations are treated as matrix [10] and concatenated into a data tensor.","MPCA computes a set of orthonormal matrices associated with each mode of the data tensor which are analogous to the orthonormal row and column space of a matrix computed by the matrix SVD. This transformation aims to capture as high a variance as possible, accounting for as much of the variability in the data associated with each data tensor mode(axis).",,,"The MPCA solution follows the alternating least square (ALS) approach.[2] It is iterative in nature. As in PCA, MPCA works on centered data. Centering is a little more complicated for tensors, and it is problem dependent.",MPCA features: Supervised MPCA feature selection is used in object recognition[11] while unsupervised MPCA feature selection is employed in visualization task.[12],Various extensions of MPCA have been developed: [13]
"Multilinear subspace learning is an approach to dimensionality reduction. [1] [2] [3][4] [5] Dimensionality reduction can be performed on data tensor whose observations have been vectorized[1] and organized into a data tensor, or whose observations are matrices that are concatenated into a data tensor.[6][7] Here are some examples of data tensors whose observations are vectorized or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D).",The mapping from a high-dimensional vector space to a set of lower dimensional vector spaces vector space is a multilinear projection.[4][6],"Multilinear subspace learning#Algorithms are higher-order generalizations of linear subspace learning methods such as principal component analysis (PCA), independent component analysis (ICA), linear discriminant analysis (LDA) and canonical correlation analysis (CCA).",,,"With the advances in data acquisition and storage technology, big data (or massive data sets) are being generated on a daily basis in a wide range of emerging applications. Most of these big data are multidimensional. Moreover, they are usually very-high-dimensional, with a large amount of redundancy, and only occupying a part of the input space. Therefore, dimensionality reduction is frequently employed to map high-dimensional data to a low-dimensional space while retaining as much information as possible.","Linear subspace learning algorithms are traditional dimensionality reduction techniques that represent input data as vectors and solve for an optimal linear mapping to a lower-dimensional space. Unfortunately, they often become inadequate when dealing with massive multidimensional data. They result in very-high-dimensional vectors, lead to the estimation of a large number of parameters.[1][6][7][8][9]","Multilinear Subspace Learning employ different types of data tensor analysis tools for dimensionality reduction. Multilinear Subspace learning can be applied to observations whose measurements were vectorized and organized into a data tensor,[1] or whose measurements are treated as a matrix and concatenated into a tensor.[10]","Historically, Multilinear Principal Component Analysis has been referred to as ""M-mode PCA"", a terminology which was coined by Peter Kroonenberg.[11] In 2005, Vasilescu and Terzopoulos introduced the Multilinear PCA[12] terminology as a way to better differentiate between linear tensor decompositions and multilinear tensor decomposition, as well as, to better differentiate between analysis approaches that computed 2nd order statistics associated with each data tensor mode(axis)s,[1][2][3][8][13] and subsequent work on Multilinear Independent Component Analysis[12] that computed higher order statistics associated with each tensor mode/axis. MPCA is an extension of PCA.",Multilinear Independent Component Analysis[12] is an extension of ICA.,Multilinear extension of LDA,,,"There are N sets of parameters to be solved, one in each mode. The solution to one set often depends on the other sets (except when N=1, the linear case). Therefore, the suboptimal iterative procedure in [22] is followed.",This is originated from the alternating least square method for multi-way data analysis.[11],The advantages of MSL are:[6][7][8][9],The disadvantages of MSL are:[6][7][8][9]
"Depending on the type and variation in training data, machine learning can be roughly categorized into three frameworks: supervised learning, unsupervised learning, and reinforcement learning. Multiple instance learning (MIL) falls under the supervised learning framework, where every training instance has a label, either discrete or real valued. MIL deals with problems with incomplete knowledge of labels in training sets. More precisely, in multiple-instance learning, the training set consists of labeled “bags”, each of which is a collection of unlabeled instances. A bag is positively labeled if at least one instance in it is positive, and is negatively labeled if all instances in it are negative. The goal of the MIL is to predict the labels of new, unseen bags.","Convenient and simple example for MIL was given in.[1] Imagine several people, and each of them has a key chain that contains few keys. Some of these people are able to enter a certain room, and some aren’t. The task is then to predict whether a certain key or a certain key chain can get you into that room. To solve this problem we need to find the exact key that is common for all the “positive” key chains. If we can correctly identify this key, we can also correctly classify an entire key chain - positive if it contains the required key, or negative if it doesn’t.",,,"Keeler et al.,[2] in his work in early 1990s was the first one to explore the area of MIL. The actual term multi-instance learning was introduced in the middle of the 1990s, by Dietterich et al. while they were investigating the problem of drug activity prediction.[3] They tried to create a learning systems that could predict whether new molecule was qualified to make some drug, or not, through analyzing a collection of known molecules. Molecules can have many alternative low-energy states, but only one, or some of them, are qualified to make a drug. The problem arose because scientists could only determine if molecule is qualified, or not, but they couldn’t say exactly which of its low-energy shapes are responsible for that.","One of the proposed ways to solve this problem was to use supervised learning, and regard all the low-energy shapes of the qualified molecule as positive training instances, while all of the low-energy shapes of unqualified molecules as negative instances. Dietterich et al. showed that such method would have a high false positive noise, from all low-energy shapes that are mislabeled as positive, and thus wasn’t really useful.[3] Their approach was to regard each molecule as a labeled bag, and all the alternative low-energy shapes of that molecule as instances in the bag, without individual labels. Thus formulating multiple-instance learning.","Solution to the multiple instance learning problem that Dietterich et al. proposed is three axis-parallel rectangle (APR) algorithm.[3] It attempts to search for appropriate axis-parallel rectangles constructed by the conjunction of the features. They tested the algorithm on Musk dataset,[4] which is a concrete test data of drug activity prediction and the most popularly used benchmark in multiple-instance learning. APR algorithm achieved the best result, but it should be noted that APR was designed with Musk data in mind.","Problem of multi-instance learning is not unique to drug finding. In 1998, Maron and Ratan found another application of multiple instance learning to scene classification in machine vision, and devised Diverse Density framework.[5] Given an image, an instance is taken to be one or more fixed-size subimages, and the bag of instances is taken to be the entire image. An image is labeled positive if it contains the target scene - a waterfall, for example - and negative otherwise. Multiple instance learning can be used to learn the properties of the subimages which characterize the target scene. From there on, these frameworks have been applied to a wide spectrum of applications, ranging from image concept learning and text categorization, to stock market prediction.","If the space of instances is 





X




{\displaystyle {\mathcal {X}}}

, then the set of bags is the set of functions 





N



X



=
{
B
:


X


→

N

}


{\displaystyle \mathbb {N} ^{\mathcal {X}}=\{B:{\mathcal {X}}\rightarrow \mathbb {N} \}}

, which is isomorphic to the set of multi-subsets of 





X




{\displaystyle {\mathcal {X}}}

. For each bag 



B
∈


N



X





{\displaystyle B\in \mathbb {N} ^{\mathcal {X}}}

 and each instance 



x
∈


X




{\displaystyle x\in {\mathcal {X}}}

, 



B
(
x
)


{\displaystyle B(x)}

 is viewed as the number of times 



x


{\displaystyle x}

 occurs in 



B


{\displaystyle B}

.[6] Let 





Y




{\displaystyle {\mathcal {Y}}}

 be the space of labels, then a ""multiple instance concept"" is a map 



c
:


N



X



→


Y




{\displaystyle c:\mathbb {N} ^{\mathcal {X}}\rightarrow {\mathcal {Y}}}

. The goal of MIL is to learn such a concept. The remainder of the article will focus on binary classification, where 





Y


=
{
0
,
1
}


{\displaystyle {\mathcal {Y}}=\{0,1\}}

.","Most of the work on Multiple instance learning, including Dietterich et al. (1997) and Maron & Lozano-P´erez (1997) early papers,[3][7] make the assumption regarding the relationship between the instances within a bag and the class label of the bag. Because of its importance, that assumption is often called standard MI assumption.","The standard assumption takes each instance 



x
∈


X




{\displaystyle x\in {\mathcal {X}}}

 to have an associated label 



y
∈
{
0
,
1
}


{\displaystyle y\in \{0,1\}}

 which is hidden to the learner. The pair 



(
x
,
y
)


{\displaystyle (x,y)}

 is called an""instance-level concept"". A bag is now viewed as a multiset of instance-level concepts, and is labeled positive if at least one of its instances has a positive label, and negative if all of its instances have negative labels. Formally, let 



B
=
{
(

x

1


,

y

1


)
,
…
,
(

x

n


,

y

n


)
}


{\displaystyle B=\{(x_{1},y_{1}),\ldots ,(x_{n},y_{n})\}}

 be a bag. The label of 



B


{\displaystyle B}

 is then 



c
(
B
)
=
1
−

∏

i
=
1


n


(
1
−

y

i


)


{\displaystyle c(B)=1-\prod _{i=1}^{n}(1-y_{i})}

. Standard MI assumption is asymmetric, which means that if the positive and negative labels are reversed, the assumption has a different meaning. Because of that, when we use this assumption, we need to be clear which label should be the positive one.","Standard assumption might be viewed as too strict, and therefore in the recent years, researchers tried to relax that position, which gave rise to other more loose assumptions.[8] Reason for this is the belief that standard MI assumption is appropriate for the Musk dataset, but since MLI can be applied to numerous other problems, some different assumptions could probably be more appropriate. Guided by that idea, Weidmann [9] formulated a hierarchy of generalized instance-based assumptions for MIL. It consists of the standard MI assumption and three types of generalized MI assumptions, each more general than the last, 



s
t
a
n
d
a
r
d
⊂
p
r
e
s
e
n
c
e

-

b
a
s
e
d
⊂
t
h
r
e
s
h
o
l
d

-

b
a
s
e
d
⊂
c
o
u
n
t

-

b
a
s
e
d


{\displaystyle standard\subset presence{\text{-}}based\subset threshold{\text{-}}based\subset count{\text{-}}based}

, with the count-based assumption being the most general and the standard assumption being the least general. One would expect an algorithm which performs well under one of these assumptions to perform at least as well under the less general assumptions.","The presence-based assumption is a generalization of the standard assumption, wherein a bag must contain one or more instances that belong to a set of required instance-level concepts in order to be labeled positive. Formally, let 




C

R


⊆


X


×


Y




{\displaystyle C_{R}\subseteq {\mathcal {X}}\times {\mathcal {Y}}}

 be the set of required instance-level concepts, and let 



#
(
B
,

c

i


)


{\displaystyle \#(B,c_{i})}

 denote the number of times the instance-level concept 




c

i




{\displaystyle c_{i}}

 occurs in the bag 



B


{\displaystyle B}

. Then 



c
(
B
)
=
1
⇔
#
(
B
,

c

i


)
≥
1


{\displaystyle c(B)=1\Leftrightarrow \#(B,c_{i})\geq 1}

 for all 




c

i


∈

C

R




{\displaystyle c_{i}\in C_{R}}

. Note that, by taking 




C

R




{\displaystyle C_{R}}

 to contain only one instance-level concept, the presence-based assumption reduces to the standard assumption.","A further generalization comes with the threshold-based assumption, where each required instance-level concept must occur not only once in a bag, but some minimum (threshold) number of times in order for the bag to be labeled positive. With the notation above, to each required instance-level concept 




c

i


∈

C

R




{\displaystyle c_{i}\in C_{R}}

 is associated a threshold 




l

i


∈

N



{\displaystyle l_{i}\in \mathbb {N} }

. For a bag 



B


{\displaystyle B}

, 



c
(
B
)
=
1
⇔
#
(
B
,

c

i


)
≥

l

i




{\displaystyle c(B)=1\Leftrightarrow \#(B,c_{i})\geq l_{i}}

 for all 




c

i


∈

C

R




{\displaystyle c_{i}\in C_{R}}

.","The count-based assumption is a final generalization which enforces both lower and upper bounds for the number of times a required concept can occur in a positively labeled bag. Each required instance-level concept 




c

i


∈

C

R




{\displaystyle c_{i}\in C_{R}}

 has a lower threshold 




l

i


∈

N



{\displaystyle l_{i}\in \mathbb {N} }

 and upper threshold 




u

i


∈

N



{\displaystyle u_{i}\in \mathbb {N} }

 with 




l

i


≤

u

i




{\displaystyle l_{i}\leq u_{i}}

. A bag 



B


{\displaystyle B}

 is labeled according to 



c
(
B
)
=
1
⇔

l

i


≤
#
(
B
,

c

i


)
≤

u

i




{\displaystyle c(B)=1\Leftrightarrow l_{i}\leq \#(B,c_{i})\leq u_{i}}

 for all 




c

i


∈

C

R




{\displaystyle c_{i}\in C_{R}}

.","Scott, Zhang, and Brown (2005) [10] describe another generalization of the standard model, which they call ""generalized multiple instance learning"" (GMIL). The GMIL assumption specifies a set of required instances 



Q
⊆


X




{\displaystyle Q\subseteq {\mathcal {X}}}

. A bag 



X


{\displaystyle X}

 is labeled positive if it contains instances which are sufficiently close to at least 



r


{\displaystyle r}

 of the required instances 



Q


{\displaystyle Q}

.[10] Under only this condition, the GMIL assumption is equivalent the to the presence-based assumption.[6] However, Scott et. al. describe a further generalization in which there is a set of attraction points 



Q
⊆


X




{\displaystyle Q\subseteq {\mathcal {X}}}

 and a set of repulsion points 





Q
¯


⊆


X




{\displaystyle {\overline {Q}}\subseteq {\mathcal {X}}}

. A bag is labeled positive if and only if it contains instances which are sufficiently close to at least 



r


{\displaystyle r}

 of the attraction points and are sufficiently close to at most 



s


{\displaystyle s}

 of the repulsion points.[10] This condition is strictly more general than the presence-based, though it does not fall within the above hierarchy.","In contrast to the previous assumptions where the bags were viewed as fixed, the collective assumption views a bag 



B


{\displaystyle B}

 as a distribution 



p
(
x

|

B
)


{\displaystyle p(x|B)}

 over instances 





X




{\displaystyle {\mathcal {X}}}

, and similarly view labels as a distribution 



p
(
y

|

x
)


{\displaystyle p(y|x)}

 over instances. The goal of an algorithm operating under the collective assumption is then to model the distribution 



p
(
y

|

B
)
=

∫


X



p
(
y

|

x
)
p
(
x

|

B
)
d
x


{\displaystyle p(y|B)=\int _{\mathcal {X}}p(y|x)p(x|B)dx}

.","Since 



p
(
x

|

B
)


{\displaystyle p(x|B)}

 is typically considered fixed but unknown, algorithms instead focus on computing the empirical version: 






p
^



(
y

|

B
)
=


1

n

B





∑

i
=
1



n

B




p
(
y

|


x

i


)


{\displaystyle {\widehat {p}}(y|B)={\frac {1}{n_{B}}}\sum _{i=1}^{n_{B}}p(y|x_{i})}

, where 




n

B




{\displaystyle n_{B}}

 is the number of instances in bag 



B


{\displaystyle B}

. Since 



p
(
y

|

x
)


{\displaystyle p(y|x)}

 is also typically taken to be fixed but unknown, most collective-assumption based methods focus on learning this distribution, as in the single-instance version.[6][8]","While the collective assumption weights every instance with equal importance, Foulds extended the collective assumption to incorporate instance weights. The weighted collective assumption is then that 






p
^



(
y

|

B
)
=


1

w

B





∑

i
=
1



n

B




w
(

x

i


)
p
(
y

|


x

i


)


{\displaystyle {\widehat {p}}(y|B)={\frac {1}{w_{B}}}\sum _{i=1}^{n_{B}}w(x_{i})p(y|x_{i})}

, where 



w
:


X


→


R


+




{\displaystyle w:{\mathcal {X}}\rightarrow \mathbb {R} ^{+}}

 is a weight function over instances and 




w

B


=

∑

x
∈
B


w
(
x
)


{\displaystyle w_{B}=\sum _{x\in B}w(x)}

.[6]","There are two major flavors of algorithms for Multiple Instance Learning: instance-based and metadata-based algorithms. The term ""instance-based"" denotes that the algorithm attempts to find a set of representative instances based on an MI assumption and classify future bags from these representatives. By contrast, metadata-based algorithms make no assumptions about the relationship between instances and bag labels, and instead try to extract instance-independent information (or metadata) about the bags in order to learn the concept.[8] For a survey of some of the modern MI algorithms see Foulds and Frank [6]","The earliest proposed MI algorithms were a set of ""iterated-discrimination"" algorithms developed by Dietterich et. al, and Diverse Density developed by Maron and Lozano-Pérez.[3][7] Both of these algorithms operated under the standard assumption.","Broadly, all of the iterated-discrimination algorithms consist of two phases. The first phase is to grow an axis parallel rectangle (APR) which contains at least one instance from each positive bag and no instances from any negative bags. This is done iteratively: starting from a random instance 




x

1


∈

B

1




{\displaystyle x_{1}\in B_{1}}

 in a positive bag, the APR is expanded to the smallest APR covering any instance 




x

2




{\displaystyle x_{2}}

 in a new positive bag 




B

2




{\displaystyle B_{2}}

. This process is repeated until the APR covers at least one instance from each positive bag. Then, each instance 




x

i




{\displaystyle x_{i}}

 contained in the APR is given a ""relevance"", corresponding to how many negative points it excludes from the APR if removed. The algorithm then selects candidate representative instances in order of decreasing relevance, until no instance contained in a negative bag is also contained in the APR. The algorithm repeats these growth and representative selection steps until convergence, where APR size at each iteration is taken to be only along candidate representatives.","After the first phase, the APR is thought to tightly contain only the representative attributes. The second phase expands this tight APR as follows: a Gaussian distribution is centered at each attribute and a looser APR is drawn such that positive instances will fall outside the tight APR with fixed probability.[4] Though iterated discrimination techniques work well with the standard assumption, they do not generalize well to other MI assumptions.[6]","In its simplest form, Diverse Density (DD) assumes a single representative instance 




t

∗




{\displaystyle t^{*}}

 as the concept. This representative instance must be ""dense"" in that it is much closer to instances from positive bags than from negative bags, as well as ""diverse"" in that it is close to at least one instance from each positive bag.","Let 






B



+


=
{

B

i


+



}

1


m




{\displaystyle {\mathcal {B}}^{+}=\{B_{i}^{+}\}_{1}^{m}}

 be the set of positively labeled bags and let 






B



−


=
{

B

i


−



}

1


n




{\displaystyle {\mathcal {B}}^{-}=\{B_{i}^{-}\}_{1}^{n}}

 be the set of negatively labeled bags, then the best candidate for the representative instance is given by 






t
^



=
arg
⁡

max

t


D
D
(
t
)


{\displaystyle {\hat {t}}=\arg \max _{t}DD(t)}

, where the diverse density 



D
D
(
t
)
=
P
r

(
t

|




B



+


,



B



−


)

=
arg
⁡

max

t



∏

i
=
1


m


P
r

(
t

|


B

i


+


)


∏

i
=
1


n


P
r

(
t

|


B

i


−


)



{\displaystyle DD(t)=Pr\left(t|{\mathcal {B}}^{+},{\mathcal {B}}^{-}\right)=\arg \max _{t}\prod _{i=1}^{m}Pr\left(t|B_{i}^{+}\right)\prod _{i=1}^{n}Pr\left(t|B_{i}^{-}\right)}

 under the assumption that bags are independently distributed given the concept 




t

∗




{\displaystyle t^{*}}

. Letting 




B

i
j




{\displaystyle B_{ij}}

 denote the jth instance of bag i, the noisy-or model gives:","



P
(
t

|


B

i
j


)


{\displaystyle P(t|B_{ij})}

 is taken to be the scaled distance 



P
(
t

|


B

i
j


)
∝
exp
⁡

(
−

∑

k



s

k


2




(

x

k


−
(

B

i
j



)

k


)


2


)



{\displaystyle P(t|B_{ij})\propto \exp \left(-\sum _{k}s_{k}^{2}\left(x_{k}-(B_{ij})_{k}\right)^{2}\right)}

 where 



s
=
(

s

k


)


{\displaystyle s=(s_{k})}

 is the scaling vector. This way, if every positive bag has an instance close to 



t


{\displaystyle t}

, then 



P
r
(
t

|


B

i


+


)


{\displaystyle Pr(t|B_{i}^{+})}

 will be high for each 



i


{\displaystyle i}

, but if any negative bag 




B

i


−




{\displaystyle B_{i}^{-}}

 has an instance close to 



t


{\displaystyle t}

, 



P
r
(
t

|


B

i


−


)


{\displaystyle Pr(t|B_{i}^{-})}

 will be low. Hence, 



D
D
(
t
)


{\displaystyle DD(t)}

 is high only if every positive bag has an instance close to 



t


{\displaystyle t}

 and no negative bags have an instance close to 



t


{\displaystyle t}

. The candidate concept 






t
^





{\displaystyle {\hat {t}}}

 can be obtained through gradient methods. Classification of new bags can then be done by evaluating proximity to 






t
^





{\displaystyle {\hat {t}}}

.[7] Though Diverse Density was originally proposed by Maron et. al. in 1998, more recent MIL algorithms use the DD framework, such as EM-DD in 2001 [11] and DD-SVM in 2004,[12] and MILES in 2006 [6]","A number of single-instance algorithms have also been adapted to a multiple-instance context under the standard assumption, including","Post 2000, there was a movement away from the standard assumption and the development of algorithms designed to tackle the more general assumptions listed above.[8]","Because of the high dimensionality of the new feature space and the cost of explicitly enumerating all APRs of the original instance space, GMIL-1 is inefficient both in terms of computation and memory. GMIL-2 was developed as a refinement of GMIL-1 in an effort to improve efficiency. GMIL-2 pre-processes the instances to find a set of candidate representative instances. GMIL-2 then maps each bag to a Boolean vector, as in GMIL-1, but only considers APRs corresponding to unique subsets of the candidate representative instances. This significantly reduces the memory and computational requirements.[6]","By mapping each bag to a feature vector of metadata, metadata-based algorithms allow the flexibility of using an arbitrary single-instance algorithm to perform the actual classification task. Future bags are simply mapped into the feature space of metadata and labeled by the chosen classifier. Therefore, much of the focus for metadata-based algorithms is on what features lead to effective classification. Note that some of the previously mentioned algorithms, such as TLC and GMIL could be considered metadata-based.","They define two variations of kNN, Bayesian-kNN and citation-kNN, as adaptations of the traditional nearest-neighbor problem to the multiple-instance setting.","So far this article has considered multiple instance learning exclusively in the context of binary classifiers. However, the generalizations of single-instance binary classifiers can carry over to the multiple-instance case."
"In machine learning, multiple-instance learning (MIL) is a variation on supervised learning. Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled bags, each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative. On the other hand, a bag is labeled positive if there is at least one instance in it which is positive. From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept.","Take image classification for example in Amores (2013). Given an image, we want to know its target class based on its visual content. For instance, the target class might be ""beach"", where the image contains both ""sand"" and ""water"". In MIL terms, the image is described as a bag 



X
=
{

X

1


,
.
.
,

X

N


}


{\displaystyle X=\{X_{1},..,X_{N}\}}

, where each




X

i




{\displaystyle X_{i}}

 is the feature vector (called instance) extracted from the corresponding i-th region in the image and N is the total regions (instances) partitioning the image. The bag is labeled positive (""beach"") if it contains both ""sand"" region instances and ""water"" region instances.","Multiple-instance learning was originally proposed under this name by Dietterich, Lathrop & Lozano-Pérez (1997), but earlier examples of similar research exist, for instance in the work on handwritten digit recognition by Keeler, Rumelhart & Leow (1990). Recent reviews of the MIL literature include Amores (2013), which provides an extensive review and comparative study of the different paradigms, and Foulds & Frank (2010), which provides a thorough review of the different assumptions used by different paradigms in the literature.",Examples of where MIL is applied are:,"Numerous researchers have worked on adapting classical classification techniques, such as support vector machines or boosting, to work within the context of multiple-instance learning."
"In statistics, multivariate adaptive regression splines (MARS) is a form of regression analysis introduced by Jerome H. Friedman in 1991.[1] It is a non-parametric regression technique and can be seen as an extension of linear models that automatically models nonlinearities and interactions between variables.","The term ""MARS"" is trademarked and licensed to Salford Systems. In order to avoid trademark infringements, many open source implementations of MARS are called ""Earth"".[2][3]",,,"This section introduces MARS using a few examples. We start with a set of data: a matrix of input variables x, and a vector of the observed responses y, with a response for each row in x. For example, the data could be:","Here there is only one independent variable, so the x matrix is just a single column. Given these measurements, we would like to build a model which predicts the expected y for a given x.",A linear model for the above data is,"The hat on the 






y
^





{\displaystyle {\hat {y}}}

 indicates that 






y
^





{\displaystyle {\hat {y}}}

 is estimated from the data. The figure on the right shows a plot of this function: a line giving the predicted 






y
^





{\displaystyle {\hat {y}}}

 versus x, with the original values of y shown as red dots.",The data at the extremes of x indicates that the relationship between y and x may be non-linear (look at the red dots relative to the regression line at low and high values of x). We thus turn to MARS to automatically build a model taking into account non-linearities. MARS software constructs a model from the given x and y as follows,"The figure on the right shows a plot of this function: the predicted 






y
^





{\displaystyle {\hat {y}}}

 versus x, with the original values of y once again shown as red dots. The predicted response is now a better fit to the original y values.","MARS has automatically produced a kink in the predicted y to take into account non-linearity. The kink is produced by hinge functions. The hinge functions are the expressions starting with 



max


{\displaystyle \max }

 (where 



max
(
a
,
b
)


{\displaystyle \max(a,b)}

 is 



a


{\displaystyle a}

 if 



a
>
b


{\displaystyle a>b}

, else 



b


{\displaystyle b}

). Hinge functions are described in more detail below.","In this simple example, we can easily see from the plot that y has a non-linear relationship with x (and might perhaps guess that y varies with the square of x). However, in general there will be multiple independent variables, and the relationship between y and these variables will be unclear and not easily visible by plotting. We can use MARS to discover that non-linear relationship.",An example MARS expression with multiple variables is,"This expression models air pollution (the ozone level) as a function of the temperature and a few other variables. Note that the last term in the formula (on the last line) incorporates an interaction between 




w
i
n
d



{\displaystyle \mathrm {wind} }

 and 




v
i
s



{\displaystyle \mathrm {vis} }

.","The figure on the right plots the predicted 




o
z
o
n
e



{\displaystyle \mathrm {ozone} }

 as 




w
i
n
d



{\displaystyle \mathrm {wind} }

 and 




v
i
s



{\displaystyle \mathrm {vis} }

 vary, with the other variables fixed at their median values. The figure shows that wind does not affect the ozone level unless visibility is low. We see that MARS can build quite flexible regression surfaces by combining hinge functions.","To obtain the above expression, the MARS model building procedure automatically selects which variables to use (some variables are important, others not), the positions of the kinks in the hinge functions, and how the hinge functions are combined.",MARS builds models of the form,"The model is a weighted sum of basis functions 




B

i


(
x
)


{\displaystyle B_{i}(x)}

. Each 




c

i




{\displaystyle c_{i}}

 is a constant coefficient. For example, each line in the formula for ozone above is one basis function multiplied by its coefficient.","Each basis function 




B

i


(
x
)


{\displaystyle B_{i}(x)}

 takes one of the following three forms:","1) a constant 1. There is just one such term, the intercept. In the ozone formula above, the intercept term is 5.2.","2) a hinge function. A hinge function has the form 



max
(
0
,
x
−
c
o
n
s
t
)


{\displaystyle \max(0,x-const)}

 or 



max
(
0
,
c
o
n
s
t
−
x
)


{\displaystyle \max(0,const-x)}

. MARS automatically selects variables and values of those variables for knots of the hinge functions. Examples of such basis functions can be seen in the middle three lines of the ozone formula.",3) a product of two or more hinge functions. These basis functions can model interaction between two or more variables. An example is the last line of the ozone formula.,Hinge functions are a key part of MARS models. A hinge function takes the form,or,"where 



c


{\displaystyle c}

 is a constant, called the knot. The figure on the right shows a mirrored pair of hinge functions with a knot at 3.1.","A hinge function is zero for part of its range, so can be used to partition the data into disjoint regions, each of which can be treated independently. Thus for example a mirrored pair of hinge functions in the expression",creates the piecewise linear graph shown for the simple MARS model in the previous section.,"One might assume that only piecewise linear functions can be formed from hinge functions, but hinge functions can be multiplied together to form non-linear functions.","Hinge functions are also called hockey stick or rectifier functions. Instead of the 



max


{\displaystyle \max }

 notation used in this article, hinge functions are often represented by 



[
±
(

x

i


−
c
)

]

+




{\displaystyle [\pm (x_{i}-c)]_{+}}

 where 



[
⋅

]

+




{\displaystyle [\cdot ]_{+}}

 means take the positive part.",MARS builds a model in two phases: the forward and the backward pass. This two-stage approach is the same as that used by recursive partitioning trees.,MARS starts with a model which consists of just the intercept term (which is the mean of the response values).,"MARS then repeatedly adds basis function in pairs to the model. At each step it finds the pair of basis functions that gives the maximum reduction in sum-of-squares residual error (it is a greedy algorithm). The two basis functions in the pair are identical except that a different side of a mirrored hinge function is used for each function. Each new basis function consists of a term already in the model (which could perhaps be the intercept i.e. a constant 1) multiplied by a new hinge function. A hinge function is defined by a variable and a knot, so to add a new basis function, MARS must search over all combinations of the following:",1) existing terms (called parent terms in this context),2) all variables (to select one for the new basis function),3) all values of each variable (for the knot of the new hinge function).,To calculate the coefficient of each term MARS applies a linear regression over the terms.,This process of adding terms continues until the change in residual error is too small to continue or until the maximum number of terms is reached. The maximum number of terms is specified by the user before model building starts.,"The search at each step is done in a brute force fashion, but a key aspect of MARS is that because of the nature of hinge functions the search can be done relatively quickly using a fast least-squares update technique. Actually, the search is not quite brute force. The search can be sped up with a heuristic that reduces the number of parent terms to consider at each step (""Fast MARS"" [4]).","The forward pass usually builds an overfit model. (An overfit model has a good fit to the data used to build the model but will not generalize well to new data.) To build a model with better generalization ability, the backward pass prunes the model. It removes terms one by one, deleting the least effective term at each step until it finds the best submodel. Model subsets are compared using the GCV criterion described below.","The backward pass has an advantage over the forward pass: at any step it can choose any term to delete, whereas the forward pass at each step can only see the next pair of terms.","The forward pass adds terms in pairs, but the backward pass typically discards one side of the pair and so terms are often not seen in pairs in the final model. A paired hinge can be seen in the equation for 






y
^





{\displaystyle {\hat {y}}}

 in the first MARS example above; there are no complete pairs retained in the ozone example.",The backward pass uses GCV to compare the performance of model subsets in order to choose the best subset: lower values of GCV are better. The GCV is a form of regularization: it trades off goodness-of-fit against model complexity.,"(We want to estimate how well a model performs on new data, not on the training data. Such new data is usually not available at the time of model building, so instead we use GCV to estimate what performance would be on new data. The raw residual sum-of-squares (RSS) on the training data is inadequate for comparing models, because the RSS always increases as MARS terms are dropped. In other words, if the RSS were used to compare models, the backward pass would always choose the largest model—but the largest model typically does not have the best generalization performance.)",The formula for the GCV is,GCV = RSS / (N * (1 - EffectiveNumberOfParameters / N)^2),where RSS is the residual sum-of-squares measured on the training data and N is the number of observations (the number of rows in the x matrix).,The EffectiveNumberOfParameters is defined in the MARS context as,EffectiveNumberOfParameters = NumberOfMarsTerms + Penalty * (NumberOfMarsTerms - 1 ) / 2,where Penalty is about 2 or 3 (the MARS software allows the user to preset Penalty).,"Note that (NumberOfMarsTerms - 1 ) / 2 is the number of hinge-function knots, so the formula penalizes the addition of knots. Thus the GCV formula adjusts (i.e. increases) the training RSS to take into account the flexibility of the model. We penalize flexibility because models that are too flexible will model the specific realization of noise in the data instead of just the systematic structure of the data.",Generalized Cross Validation is so named because it uses a formula to approximate the error that would be determined by leave-one-out validation. It is just an approximation but works well in practice. GCVs were introduced by Craven and Wahba and extended by Friedman for MARS.,One constraint has already been mentioned: the user can specify the maximum number of terms in the forward pass.,"A further constraint can be placed on the forward pass by specifying a maximum allowable degree of interaction. Typically only one or two degrees of interaction are allowed, but higher degrees can be used when the data warrants it. The maximum degree of interaction in the first MARS example above is one (i.e. no interactions or an additive model); in the ozone example it is two.","Other constraints on the forward pass are possible. For example, the user can specify that interactions are allowed only for certain input variables. Such constraints could make sense because of knowledge of the process that generated the data.","No regression modeling technique is best for all situations. The guidelines below are intended to give an idea of the pros and cons of MARS, but there will be exceptions to the guidelines. It is useful to compare MARS to recursive partitioning and this is done below. (Recursive partitioning is also commonly called regression trees, decision trees, or CART; see the recursive partitioning article for details)."
"MysteryVibe is a British health & lifestyle brand that designs, develops and manufactures luxury pleasure products in the UK. While only 2 years old, they have witnessed a phenomenal rise with customers in 50+ countries and features in leading conferences and major publications including Glamour,[2] Elle, WSJ,[3] CNBC,[4] The Guardian,[5] FastCompany,[6] TechCrunch,[7] NBC,[8] VentureBeat,[9] Psychologies,[10] Stylist[11] and Evening Standard.[12]","MysteryVibe is headquartered in London, UK and the first company in their class to have been created as a collective of world leading partners - Design with Seymourpowell; Software with Fueled & Untitled Kingdom; and Sensory experiences with CondimentJunkie. They are also unique in their focus on collaboration with Tech giants and are building their company with close support from Google[13] Launchpad, Amazon AWS Activate, IBM Global Entrepreneur and Facebook FbStart.",,,"MysteryVibe was founded by a group of researchers, engineers and designers bringing together their experience from Google, Microsoft, Nokia, Sharp, Canon, Philips and Deloitte. Inspired by trends in smartphones like Nokia Morph, the founders came up with the idea of creating a pleasure product that would be truly personal by adapting to any body shape and vibrate to any pattern. They continued to research for a number of years to validate the idea before formally starting the company in May 2014, when they were selected to be incubated by top London-based industrial design firm, Seymourpowell.","Under the guidance of the experts[14] at Seymourpowell, the founders built out the wider objectives so they could transform their idea into not just a product / company but a revolution in pleasure which solved a real pain point of ""keeping bedrooms fun as people got older"". That's also when they realised that to build a luxury pleasure brand they would need to bring together real subject-matter experts and remove the taboos around the topic. Taking that relationship-centric approach, they set their vision to be:","Bring mainstream talent and innovation to create immersive experiences that make relationships more fun. Open up conversations and empower individuals to take control of their pleasures. Apply Big Data and IoT to pleasure, make the information openly available and support the medical & research communities to address health issues in a positive informed way. All with the hope that an enlightened and liberal society is a safer & happier one.","In early 2015 MysteryVibe partnered with New York-based app designers, Fueled and sensory design experts CondimentJunkie to bring the software element of their vision of life. Like Seymourpowell, Fueled and CondimentJunkie are considered world leaders in their fields[15] and count Glamour, Vanity Fair, MTV, Aston Martin, Bentley and Selfridges amongst their clients. MysteryVibe released the first version of their app on the Apple App Store in December, 2015 under the Health & Fitness category. The app was downloaded more than 100,000 times in its first 6 months alone.[16] In early 2016, MysteryVibe extended their collective to include leading European technologists, UntitledKingdom, and, manufacturing experts, RPD International, to scale manufacturing and commence their Android and Home Automation development.","MysteryVibe's flagship product, Crescendo,[17] became the first crowdfunding project[18] to offer its backers 2 versions of their product: Pilot and Retail. They ran what they called the #Pilot1000 programme for their first 1,000 users to get feedback on all areas that could make Crescendo more personal. The #Pilot1000 users spanned 48 countries and included both backers and experts. MysteryVibe took the unprecedented step of giving all 1,000 users full access to their founding CEO with direct email, phone and Skype. This enabled them to interact with every user directly, understand their feedback and act upon it to make the final Retail Crescendo[19] a genuinely user-designed product.","Due to the inherently lean model adopted by MysteryVibe with collectives and collaborations, they were able to build the company from a sketch to shipped products with less than £1m ($1.5m) in funding.[23] They were also unique in raising 100% of the money from Angel investors[24] without any recourse to Venture Capital, a normal staple in the startup world. MysteryVibe sits within its Angels' diverse portfolio of high-growth startups which include Techstars, Seedcamp, Funding Circle, Collider, F6S, DojoApp, Lystable and Playmob. As of Q2 2016, MysteryVibe has raised $2.4m in total funding.","In June 2016, MysteryVibe became the first pleasure product to be featured[20] by Virgin in their #VOOM competition.[21] They were showcased under the Export Awards category for exporting their UK-manufactured products to over 50 countries worldwide. Getting such a mainstream association gave them a big boost in bringing the conversations around pleasure to everyday lives. The same month, they got inducted into the prestigious Hardware Club and became the first pleasure product company to be nominated[25] for the highly coveted 'Best Hardware Startup' award at The Europas in London.","Ranked by top European investors as #9 of the 100 Slush start-ups in Cofounder Magazine,[26] MysteryVibe has also been named as one of the ""Top 100 Europe’s hottest early-stage Founders"" by PathFounders, Europas.[27] They have also been listed at #7 in the ""12 days of start-ups: Spectacular businesses set for stardom in 2016"".[28] Since their public launch at TechCrunch Disrupt New York in May 2015 MysteryVibe have been featured in 100+[29] leading newspapers and magazines across the world.[2][3][4][6][7][8][9][10][12] More recently at TechCrunch Disrupt London in December 2015, they were amongst Contagious' top 5 picks from the conference.[30]"
"Native-language identification (NLI) is the task of determining an author's native language (L1) based only on their writings in a second language (L2).[1] NLI works through identifying language-usage patterns that are common to specific L1 groups and then applying this knowledge to predict the native language of previously unseen texts. This is motivated in part by applications in second-language acquisition, language teaching and forensic linguistics, amongst others.",,,"NLI works under the assumption that an author's L1 will dispose them towards particular language production patterns in their L2, as influenced by their native language. This relates to cross-linguistic influence (CLI), a key topic in the field of second-language acquisition (SLA) that analyzes transfer effects from the L1 on later learned languages.","Using large-scale English data, NLI methods achieve over 80% accuracy in predicting the native language of texts written by authors from 11 different L1 backgrounds. This can be compared to a baseline of 9% for choosing randomly.","This identification of L1-specific features has been used to study language transfer effects in second-language acquisition.[2] This is useful for developing pedagogical material, teaching methods, L1-specific instructions and generating learner feedback that is tailored to their native language.","NLI methods can also be applied in forensic linguistics as a method of performing authorship profiling in order to infer the attributes of an author, including their linguistic background. This is particularly useful in situations where a text, e.g. an anonymous letter, is the key piece of evidence in an investigation and clues about the native language of a writer can help investigators in identifying the source. This has already attracted interest and funding from intelligence agencies.[3]","Natural language processing methods are used to extract and identify language usage patterns common to speakers of an L1-group. This is done using language learner data, usually from a learner corpus. Next, machine learning is applied to train classifiers, like support vector machines, for predicting the L1 of unseen texts.[4] A range of ensemble based systems have also been applied to the task and shown to improve performance over single classifier systems.[5]","Various linguistic feature types have been applied for this task. These include syntactic features such as constituent parses, grammatical dependencies and part-of-speech tags. Surface level lexical features such as character, word and lemma n-grams have also been found to be quite useful for this task.","The Building Educational Applications (BEA) workshop at NAACL 2013 hosted the inaugural NLI shared task.[6] The competition resulted in 29 entries from teams across the globe, 24 of which also published a paper describing their systems and approaches."
"Nearest neighbor search (NNS), also known as proximity search, similarity search or closest point search, is an optimization problem for finding closest (or most similar) points. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points.","Most commonly M is a metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example are asymmetric Bregman divergences, for which the triangle inequality does not hold.[1]",,,"The nearest neighbor search problem arises in numerous fields of application, including:",Various solutions to the NNS problem have been proposed. The quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. The informal observation usually referred to as the curse of dimensionality states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time.,"The simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the ""best so far"". This algorithm, sometimes referred to as the naive approach, has a running time of O(dN) where N is the cardinality of S and d is the dimensionality of M. There are no search data structures to maintain, so linear search has no space complexity beyond the storage of the database. Naive search can, on average, outperform space partitioning approaches on higher dimensional spaces.[3]","Since the 1970s, branch and bound methodology has been applied to the problem. In the case of Euclidean space this approach is known as spatial index or spatial access methods. Several space-partitioning methods have been developed for solving the NNS problem. Perhaps the simplest is the k-d tree, which iteratively bisects the search space into two regions containing half of the points of the parent region. Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split. Depending on the distance specified in the query, neighboring branches that might contain hits may also need to be evaluated. For constant dimension query time, average complexity is O(log N) [4] in the case of randomly distributed points, worst case complexity is O(kN^(1-1/k))[5] Alternatively the R-tree data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the R* tree.[6] R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances.",In case of general metric space branch and bound approach is known under the name of metric trees. Particular examples include vp-tree and BK-tree.,"Using a set of points taken from a 3-dimensional space and put into a BSP tree, and given a query point taken from the same space, a possible solution to the problem of finding the nearest point-cloud point to the query point is given in the following description of an algorithm. (Strictly speaking, no such point may exist, because it may not be unique. But in practice, usually we only care about finding any one of the subset of all point-cloud points that exist at the shortest distance to a given query point.) The idea is, for each branching of the tree, guess that the closest point in the cloud resides in the half-space containing the query point. This may not be the case, but it is a good heuristic. After having recursively gone through all the trouble of solving the problem for the guessed half-space, now compare the distance returned by this result with the shortest distance from the query point to the partitioning plane. This latter distance is that between the query point and the closest possible point that could exist in the half-space not searched. If this distance is greater than that returned in the earlier result, then clearly there is no need to search the other half-space. If there is such a need, then you must go through the trouble of solving the problem for the other half space, and then compare its result to the former result, and then return the proper result. The performance of this algorithm is nearer to logarithmic time than linear time when the query point is near the cloud, because as the distance between the query point and the closest point-cloud point nears zero, the algorithm needs only perform a look-up using the query point as a key to get the correct result.",Locality sensitive hashing (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability.[7],The cover tree has a theoretical bound that is based on the dataset's doubling constant. The bound on search time is O(c12 log n) where c is the expansion constant of the dataset.,"In the special case where the data is a dense 3D map of geometric points, the projection geometry of the sensing technique can be used to dramatically simplify the search problem. This approach requires that the 3D data is organized by a projection to a two dimensional grid and assumes that the data is spatially smooth across neighboring grid cells with the exception of object boundaries. These assumptions are valid when dealing with 3D sensor data in applications such as surveying, robotics and stereo vision but may not hold for unorganized data in general. In practice this technique has an average search time of O(1) or O(K) for the k-nearest neighbor problem when applied to real world stereo vision data. [2]","In high dimensional spaces, tree indexing structures become useless because an increasing percentage of the nodes need to be examined anyway. To speed up linear search, a compressed version of the feature vectors stored in RAM is used to prefilter the datasets in a first run. The final candidates are determined in a second stage using the uncompressed data from the disk for distance calculation.[8]","The VA-file approach is a special case of a compression based search, where each feature component is compressed uniformly and independently. The optimal compression technique in multidimensional spaces is Vector Quantization (VQ), implemented through clustering. The database is clustered and the most ""promising"" clusters are retrieved. Huge gains over VA-File, tree-based indexes and sequential scan have been observed.[9][10] Also note the parallels between clustering and LSH.","One possible way to solve NNS is to construct a graph 



G
(
V
,
E
)


{\displaystyle G(V,E)}

, where every point 




x

i


∈
S


{\displaystyle x_{i}\in S}

 is uniquely associated with vertex 




v

i


∈
V


{\displaystyle v_{i}\in V}

. The search of the point in the set S closest to the query q takes the form of the search of vertex in the graph 



G
(
V
,
E
)


{\displaystyle G(V,E)}

. One of the basic vertex search algorithms in graphs with metric objects is the greedy search algorithm. It starts from the random vertex 




v

i


∈
V


{\displaystyle v_{i}\in V}

. The algorithm computes a distance value from the query q to each vertex from the neighborhood 



{

v

j


:
(

v

i


,

v

j


)
∈
E
}


{\displaystyle \{v_{j}:(v_{i},v_{j})\in E\}}

 of the current vertex 




v

i




{\displaystyle v_{i}}

, and then selects a vertex with the minimal distance value. If the distance value between the query and the selected vertex is smaller than the one between the query and the current element, then the algorithm moves to the selected vertex, and it becomes new current vertex. The algorithm stops when it reaches a local minimum: a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself. This idea was exploited in VoroNet system [11] for the plane, in RayNet system [12] for the 





E


n




{\displaystyle \mathbb {E} ^{n}}

 and for the general metric space in Metrized Small World algorithm.[13] Java and C++ implementations of the algorithm together with sources are hosted on the GitHub (Non-Metric Space Library, Metrized Small World library).",There are numerous variants of the NNS problem and the two most well-known are the k-nearest neighbor search and the ε-approximate nearest neighbor search.,k-nearest neighbor search identifies the top k nearest neighbors to the query. This technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors. k-nearest neighbor graphs are graphs in which every point is connected to its k nearest neighbors.,"In some applications it may be acceptable to retrieve a ""good guess"" of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings. Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.","Algorithms that support the approximate nearest neighbor search include locality-sensitive hashing, best bin first and balanced box-decomposition tree based search.[14]","Nearest neighbor distance ratio do not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in CBIR to retrieve pictures through a ""query by example"" using the similarity between local features. More generally it is involved in several matching problems.",Fixed-radius near neighbors is the problem where one wants to efficiently find all points given in Euclidean space within a given fixed distance from a specified point. The data structure should work on a distance which is fixed however the query point is arbitrary.,"For some applications (e.g. entropy estimation), we may have N data-points and wish to know which is the nearest neighbor for every one of those N points. This could of course be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these N queries to produce a more efficient search. As a simple example: when we find the distance from point X to point Y, that also tells us the distance from point Y to point X, so the same calculation can be reused in two different queries.","Given a fixed dimension, a semi-definite positive norm (thereby including every Lp norm), and n points in this space, the nearest neighbour of every point can be found in O(n log n) time and the m nearest neighbours of every point can be found in O(mn log n) time.[15][16]"
"Neural modeling field (NMF) is a mathematical framework for machine learning which combines ideas from neural networks, fuzzy logic, and model based recognition. It has also been referred to as modeling fields, modeling fields theory (MFT), Maximum likelihood artificial neural networks (MLANS). [1] [2] [3] [4] [5] [6] This framework has been developed by Leonid Perlovsky at the AFRL. NMF is interpreted as a mathematical description of mind’s mechanisms, including concepts, emotions, instincts, imagination, thinking, and understanding. NMF is a multi-level, hetero-hierarchical system. At each level in NMF there are concept-models encapsulating the knowledge; they generate so-called top-down signals, interacting with input, bottom-up signals. These interactions are governed by dynamic equations, which drive concept-model learning, adaptation, and formation of new concept-models for better correspondence to the input, bottom-up signals.",,,"In the general case, NMF system consists of multiple processing levels. At each level, output signals are the concepts recognized in (or formed from) input, bottom-up signals. Input signals are associated with (or recognized, or grouped into) concepts according to the models and at this level. In the process of learning the concept-models are adapted for better representation of the input signals so that similarity between the concept-models and signals increases. This increase in similarity can be interpreted as satisfaction of an instinct for knowledge, and is felt as aesthetic emotions.","Each hierarchical level consists of N ""neurons"" enumerated by index n=1,2..N. These neurons receive input, bottom-up signals, X(n), from lower levels in the processing hierarchy. X(n) is a field of bottom-up neuronal synaptic activations, coming from neurons at a lower level. Each neuron has a number of synapses; for generality, each neuron activation is described as a set of numbers,",",where D is the number or dimensions necessary to describe individual neuron's activation.","Top-down, or priming signals to these neurons are sent by concept-models, Mm(Sm,n)",",where M is the number of models. Each model is characterized by its parameters, Sm; in the neuron structure of the brain they are encoded by strength of synaptic connections, mathematically, they are given by a set of numbers,",",where A is the number of dimensions necessary to describe invividual model.","Models represent signals in the following way. Suppose that signal X(n) is coming from sensory neurons n activated by object m, which is characterized by parameters Sm. These parameters may include position, orientation, or lighting of an object m. Model Mm(Sm,n) predicts a value X(n) of a signal at neuron n. For example, during visual perception, a neuron n in the visual cortex receives a signal X(n) from retina and a priming signal Mm(Sm,n) from an object-concept-model m. Neuron n is activated if both the bottom-up signal from lower-level-input and the top-down priming signal are strong. Various models compete for evidence in the bottom-up signals, while adapting their parameters for better match as described below. This is a simplified description of perception. The most benign everyday visual perception uses many levels from retina to object perception. The NMF premise is that the same laws describe the basic interaction dynamics at each level. Perception of minute features, or everyday objects, or cognition of complex abstract concepts is due to the same mechanism described below. Perception and cognition involve concept-models and learning. In perception, concept-models correspond to objects; in cognition models correspond to relationships and situations.","Learning is an essential part of perception and cognition, and in NMF theory it is driven by the dynamics that increase a similarity measure between the sets of models and signals, L({X},{M}). The similarity measure is a function of model parameters and associations between the input bottom-up signals and top-down, concept-model signals. In constructing a mathematical description of the similarity measure, it is important to acknowledge two principles:","Therefore, the similarity measure is constructed so that it accounts for all bottom-up signals, X(n),","This expression contains a product of partial similarities, l(X(n)), over all bottom-up signals; therefore it forces the NMF system to account for every signal (even if one term in the product is zero, the product is zero, the similarity is low and the knowledge instinct is not satisfied); this is a reflection of the first principle. Second, before perception occurs, the mind does not know which object gave rise to a signal from a particular retinal neuron. Therefore a partial similarity measure is constructed so that it treats each model as an alternative (a sum over concept-models) for each input neuron signal. Its constituent elements are conditional partial similarities between signal X(n) and model Mm, l(X(n)|m). This measure is “conditional” on object m being present, therefore, when combining these quantities into the overall similarity measure, L, they are multiplied by r(m), which represent a probabilistic measure of object m actually being present. Combining these elements with the two principles noted above, a similarity measure is constructed as follows:","The structure of the expression above follows standard principles of the probability theory: a summation is taken over alternatives, m, and various pieces of evidence, n, are multiplied. This expression is not necessarily a probability, but it has a probabilistic structure. If learning is successful, it approximates probabilistic description and leads to near-optimal Bayesian decisions. The name “conditional partial similarity” for l(X(n)|m) (or simply l(n|m)) follows the probabilistic terminology. If learning is successful, l(n|m) becomes a conditional probability density function, a probabilistic measure that signal in neuron n originated from object m. Then L is a total likelihood of observing signals {X(n)} coming from objects described by concept-model {Mm}. Coefficients r(m), called priors in probability theory, contain preliminary biases or expectations, expected objects m have relatively high r(m) values; their true values are usually unknown and should be learned, like other parameters Sm.","Note that in probability theory, a product of probabilities usually assumes that evidence is independent. Expression for L contains a product over n, but it does not assume independence among various signals X(n). There is a dependence among signals due to concept-models: each model Mm(Sm,n) predicts expected signal values in many neurons n.","During the learning process, concept-models are constantly modified. Usually, the functional forms of models, Mm(Sm,n), are all fixed and learning-adaptation involves only model parameters, Sm. From time to time a system forms a new concept, while retaining an old one as well; alternatively, old concepts are sometimes merged or eliminated. This requires a modification of the similarity measure L; The reason is that more models always result in a better fit between the models and data. This is a well known problem, it is addressed by reducing similarity L using a “skeptic penalty function,” (Penalty method) p(N,M) that grows with the number of models M, and this growth is steeper for a smaller amount of data N. For example, an asymptotically unbiased maximum likelihood estimation leads to multiplicative p(N,M) = exp(-Npar/2), where Npar is a total number of adaptive parameters in all models (this penalty function is known as Akaike information criterion, see (Perlovsky 2001) for further discussion and references).","The learning process consists of estimating model parameters S and associating signals with concepts by maximizing the similarity L. Note that all possible combinations of signals and models are accounted for in expression (2) for L. This can be seen by expanding a sum and multiplying all the terms resulting in MN items, a huge number. This is the number of combinations between all signals (N) and all models (M). This is the source of Combinatorial Complexity, which is solved in NMF by utilizing the idea of dynamic logic,.[7][8] An important aspect of dynamic logic is matching vagueness or fuzziness of similarity measures to the uncertainty of models. Initially, parameter values are not known, and uncertainty of models is high; so is the fuzziness of the similarity measures. In the process of learning, models become more accurate, and the similarity measure more crisp, the value of the similarity increases.","The maximization of similarity L is done as follows. First, the unknown parameters {Sm} are randomly initialized. Then the association variables f(m|n) are computed,","Equation for f(m|n) looks like the Bayes formula for a posteriori probabilities; if l(n|m) in the result of learning become conditional likelihoods, f(m|n) become Bayesian probabilities for signal n originating from object m. The dynamic logic of the NMF is defined as follows:",The following theorem has been proved (Perlovsky 2001):,"Theorem. Equations (3), (4), and (5) define a convergent dynamic NMF system with stationary states defined by max{Sm}L.","It follows that the stationary states of an MF system are the maximum similarity states. When partial similarities are specified as probability density functions (pdf), or likelihoods, the stationary values of parameters {Sm} are asymptotically unbiased and efficient estimates of these parameters.[9] The computational complexity of dynamic logic is linear in N.","Practically, when solving the equations through successive iterations, f(m|n) can be recomputed at every iteration using (3), as opposed to incremental formula (5).","The proof of the above theorem contains a proof that similarity L increases at each iteration. This has a psychological interpretation that the instinct for increasing knowledge is satisfied at each step, resulting in the positive emotions: NMF-dynamic logic system emotionally enjoys learning.","Finding patterns below noise can be an exceedingly complex problem. If an exact pattern shape is not known and depends on unknown parameters, these parameters should be found by fitting the pattern model to the data. However, when the locations and orientations of patterns are not known, it is not clear which subset of the data points should be selected for fitting. A standard approach for solving this kind of problem is multiple hypothesis testing (Singer et al. 1974). Since all combinations of subsets and models are exhaustively searched, this method faces the problem of combinatorial complexity. In the current example, noisy ‘smile’ and ‘frown’ patterns are sought. They are shown in Fig.1a without noise, and in Fig.1b with the noise, as actually measured. The true number of patterns is 3, which is not known. Therefore, at least 4 patterns should be fit to the data, to decide that 3 patterns fit best. The image size in this example is 100x100 = 10,000 points. If one attempts to fit 4 models to all subsets of 10,000 data points, computation of complexity, MN ~ 106000. An alternative computation by searching through the parameter space, yields lower complexity: each pattern is characterized by a 3-parameter parabolic shape. Fitting 4x3=12 parameters to 100x100 grid by a brute-force testing would take about 1032 to 1040 operations, still a prohibitive computational complexity. To apply NMF and dynamic logic to this problem one needs to develop parametric adaptive models of expected patterns. The models and conditional partial similarities for this case are described in details in:[10] a uniform model for noise, Gaussian blobs for highly-fuzzy, poorly resolved patterns, and parabolic models for ‘smiles’ and ‘frowns’. The number of computer operations in this example was about 1010. Thus, a problem that was not solvable due to combinatorial complexity becomes solvable using dynamic logic.","During an adaptation process, initially fuzzy and uncertain models are associated with structures in the input signals, and fuzzy models become more definite and crisp with successive iterations. The type, shape, and number, of models are selected so that the internal representation within the system is similar to input signals: the NMF concept-models represent structure-objects in the signals. The figure below illustrates operations of dynamic logic. In Fig. 1(a) true ‘smile’ and ‘frown’ patterns are shown without noise; (b) actual image available for recognition (signal is below noise, signal-to-noise ratio is between –2dB and –0.7dB); (c) an initial fuzzy model, a large fuzziness corresponds to uncertainty of knowledge; (d) through (m) show improved models at various iteration stages (total of 22 iterations). Every five iterations the algorithm tried to increase or decrease the number of models. Between iterations (d) and (e) the algorithm decided, that it needs three Gaussian models for the ‘best’ fit.","There are several types of models: one uniform model describing noise (it is not shown) and a variable number of blob models and parabolic models; their number, location, and curvature are estimated from the data. Until about stage (g) the algorithm used simple blob models, at (g) and beyond, the algorithm decided that it needs more complex parabolic models to describe the data. Iterations stopped at (h), when similarity stopped increasing.","Above, a single processing level in a hierarchical NMF system was described. At each level of hierarchy there are input signals from lower levels, models, similarity measures (L), emotions, which are defined as changes in similarity, and actions; actions include adaptation, behavior satisfying the knowledge instinct – maximization of similarity. An input to each level is a set of signals X(n), or in neural terminology, an input field of neuronal activations. The result of signal processing at a given level are activated models, or concepts m recognized in the input signals n; these models along with the corresponding instinctual signals and emotions may activate behavioral models and generate behavior at this level.","The activated models initiate other actions. They serve as input signals to the next processing level, where more general concept-models are recognized or created. Output signals from a given level, serving as input to the next level, are the model activation signals, am, defined as",am = ∑n=1..N f(m|n).,"The hierarchical NMF system is illustrated in Fig. 2. Within the hierarchy of the mind, each concept-model finds its “mental” meaning and purpose at a higher level (in addition to other purposes). For example, consider a concept-model “chair.” It has a “behavioral” purpose of initiating sitting behavior (if sitting is required by the body), this is the “bodily” purpose at the same hierarchical level. In addition, it has a “purely mental” purpose at a higher level in the hierarchy, a purpose of helping to recognize a more general concept, say of a “concert hall,” a model of which contains rows of chairs.","From time to time a system forms a new concept or eliminates an old one. At every level, the NMF system always keeps a reserve of vague (fuzzy) inactive concept-models. They are inactive in that their parameters are not adapted to the data; therefore their similarities to signals are low. Yet, because of a large vagueness (covariance) the similarities are not exactly zero. When a new signal does not fit well into any of the active models, its similarities to inactive models automatically increase (because first, every piece of data is accounted for, and second, inactive models are vague-fuzzy and potentially can “grab” every signal that does not fit into more specific, less fuzzy, active models. When the activation signal am for an inactive model, m, exceeds a certain threshold, the model is activated. Similarly, when an activation signal for a particular model falls below a threshold, the model is deactivated. Thresholds for activation and deactivation are set usually based on information existing at a higher hierarchical level (prior information, system resources, numbers of activated models of various types, etc.). Activation signals for active models at a particular level { am } form a “neuronal field,” which serve as input signals to the next level, where more abstract and more general concepts are formed."
"In computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.","Occam learnability implies PAC learning, and for a wide variety of concept classes, the converse is also true: PAC learnability implies Occam learnability.",,,"Occam Learning is named after Occam's razor, which is a principle stating that, given all other things being equal, a shorter explanation for observed data should be favored over a lengthier explanation. The theory of Occam learning is a formal and mathematical justification for this principle. It was first shown by Blumer, et al.[1] that Occam learning implies PAC learning, which is the standard model of learning in computational learning theory. In other words, parsimony (of the output hypothesis) implies predictive power.","The succinctness of a concept 



c


{\displaystyle c}

 in concept class 





C




{\displaystyle {\mathcal {C}}}

 can be expressed by the length 



s
i
z
e
(
c
)


{\displaystyle size(c)}

 of the shortest bit string that can represent 



c


{\displaystyle c}

 in 





C




{\displaystyle {\mathcal {C}}}

. Occam learning connects the succinctness of a learning algorithm's output to its predictive power on unseen data.","Let 





C




{\displaystyle {\mathcal {C}}}

 and 





H




{\displaystyle {\mathcal {H}}}

 be concept classes containing target concepts and hypotheses respectively. Then, for constants 



α
≥
0


{\displaystyle \alpha \geq 0}

 and 



0
≤
β
<
1


{\displaystyle 0\leq \beta <1}

, a learning algorithm 



L


{\displaystyle L}

 is an 



(
α
,
β
)


{\displaystyle (\alpha ,\beta )}

-Occam algorithm for 





C




{\displaystyle {\mathcal {C}}}

 using 





H




{\displaystyle {\mathcal {H}}}

 if, given a set 



S


{\displaystyle S}

 of 



m


{\displaystyle m}

 samples labeled according to a concept 



c
∈


C




{\displaystyle c\in {\mathcal {C}}}

, 



L


{\displaystyle L}

 outputs a hypothesis 



h
∈


H




{\displaystyle h\in {\mathcal {H}}}

 such that","where 



n


{\displaystyle n}

 is the maximum length of any sample 



x
∈
S


{\displaystyle x\in S}

. An Occam algorithm is called efficient if it runs in time polynomial in 



n


{\displaystyle n}

, 



m


{\displaystyle m}

, and 



s
i
z
e
(
c
)


{\displaystyle size(c)}

. We say a concept class 





C




{\displaystyle {\mathcal {C}}}

 is Occam learnable with respect to a hypothesis class 





H




{\displaystyle {\mathcal {H}}}

 if there exists an efficient Occam algorithm for 





C




{\displaystyle {\mathcal {C}}}

 using 





H




{\displaystyle {\mathcal {H}}}

.","Occam learnability implies PAC learnability, as the following theorem of Blumer, et al.[2] shows:","Let 



L


{\displaystyle L}

 be an efficient 



(
α
,
β
)


{\displaystyle (\alpha ,\beta )}

-Occam algorithm for 





C




{\displaystyle {\mathcal {C}}}

 using 





H




{\displaystyle {\mathcal {H}}}

. Then there exists a constant 



a
>
0


{\displaystyle a>0}

 such that for any 



0
<
ϵ
,
δ
<
1


{\displaystyle 0<\epsilon ,\delta <1}

, for any distribution 





D




{\displaystyle {\mathcal {D}}}

, given 



m
≥
a

(


1
ϵ


log
⁡


1
δ


+


(



(
n
⋅
s
i
z
e
(
c
)

)

α


)

ϵ


)



1

1
−
β




)



{\displaystyle m\geq a\left({\frac {1}{\epsilon }}\log {\frac {1}{\delta }}+\left({\frac {(n\cdot size(c))^{\alpha })}{\epsilon }}\right)^{\frac {1}{1-\beta }}\right)}

 samples drawn from 





D




{\displaystyle {\mathcal {D}}}

 and labelled according to a concept 



c
∈


C




{\displaystyle c\in {\mathcal {C}}}

 of length 



n


{\displaystyle n}

 bits each, the algorithm 



L


{\displaystyle L}

 will output a hypothesis 



h
∈


H




{\displaystyle h\in {\mathcal {H}}}

 such that 



e
r
r
o
r
(
h
)
≤
ϵ


{\displaystyle error(h)\leq \epsilon }

 with probability at least 



1
−
δ


{\displaystyle 1-\delta }

 .","Here, 



e
r
r
o
r
(
h
)


{\displaystyle error(h)}

 is with respect to the concept 



c


{\displaystyle c}

 and distribution 





D




{\displaystyle {\mathcal {D}}}

. This implies that the algorithm 



L


{\displaystyle L}

 is also a PAC learner for the concept class 





C




{\displaystyle {\mathcal {C}}}

 using hypothesis class 





H




{\displaystyle {\mathcal {H}}}

. A slightly more general formulation is as follows:","Let 



0
<
ϵ
,
δ
<
1


{\displaystyle 0<\epsilon ,\delta <1}

. Let 



L


{\displaystyle L}

 be an algorithm such that, given 



m


{\displaystyle m}

 samples drawn from a fixed but unknown distribution 





D




{\displaystyle {\mathcal {D}}}

 and labeled according to a concept 



c
∈


C




{\displaystyle c\in {\mathcal {C}}}

 of length 



n


{\displaystyle n}

 bits each, outputs a hypothesis 



h
∈



H



n
,
m




{\displaystyle h\in {\mathcal {H}}_{n,m}}

 that is consistent with the labeled samples. Then, there exists a constant 



b


{\displaystyle b}

 such that if 



log
⁡

|




H



n
,
m



|

≤
b
ϵ
m
−
log
⁡


1
δ




{\displaystyle \log |{\mathcal {H}}_{n,m}|\leq b\epsilon m-\log {\frac {1}{\delta }}}

, then 



L


{\displaystyle L}

 is guaranteed to output a hypothesis 



h
∈



H



n
,
m




{\displaystyle h\in {\mathcal {H}}_{n,m}}

 such that 



e
r
r
o
r
(
h
)
≤
ϵ


{\displaystyle error(h)\leq \epsilon }

 with probability at least 



1
−
δ


{\displaystyle 1-\delta }

.","While the above theorems show that Occam learning is sufficient for PAC learning, it doesn't say anything about necessity. Board and Pitt show that, for a wide variety of concept classes, Occam learning is in fact necessary for PAC learning.[3] They proved that for any concept class that is polynomially closed under exception lists, PAC learnability implies the existence of an Occam algorithm for that concept class. Concept classes that are polynomially closed under exception lists include Boolean formulas, circuits, deterministic finite automata, decision-lists, decision-trees, and other geometrically-defined concept classes.","A concept class 





C




{\displaystyle {\mathcal {C}}}

 is polynomially closed under exception lists if there exists a polynomial-time algorithm 



A


{\displaystyle A}

 such that, when given the representation of a concept 



c
∈


C




{\displaystyle c\in {\mathcal {C}}}

 and a finite list 



E


{\displaystyle E}

 of exceptions, outputs a representation of a concept 




c
′

∈


C




{\displaystyle c'\in {\mathcal {C}}}

 such that the concepts 



c


{\displaystyle c}

 and 




c
′



{\displaystyle c'}

 agree except on the set 



E


{\displaystyle E}

.","We first prove the Cardinality version. Call a hypothesis 



h
∈


H




{\displaystyle h\in {\mathcal {H}}}

 bad if 



e
r
r
o
r
(
h
)
≥
ϵ


{\displaystyle error(h)\geq \epsilon }

, where again 



e
r
r
o
r
(
h
)


{\displaystyle error(h)}

 is with respect to the true concept 



c


{\displaystyle c}

 and the underlying distribution 





D




{\displaystyle {\mathcal {D}}}

. The probability that a set of samples 



S


{\displaystyle S}

 is consistent with 



h


{\displaystyle h}

 is at most 



(
1
−
ϵ

)

m




{\displaystyle (1-\epsilon )^{m}}

, by the independence of the samples. By the union bound, the probability that there exists a bad hypothesis in 






H



n
,
m




{\displaystyle {\mathcal {H}}_{n,m}}

 is at most 




|




H



n
,
m



|

(
1
−
ϵ

)

m




{\displaystyle |{\mathcal {H}}_{n,m}|(1-\epsilon )^{m}}

, which is less than 



δ


{\displaystyle \delta }

 if 



log
⁡

|




H



n
,
m



|

≤
O
(
ϵ
m
)
−
log
⁡


1
δ




{\displaystyle \log |{\mathcal {H}}_{n,m}|\leq O(\epsilon m)-\log {\frac {1}{\delta }}}

. This concludes the proof of the second theorem above.","Using the second theorem, we can prove the first theorem. Since we have a 



(
α
,
β
)


{\displaystyle (\alpha ,\beta )}

-Occam algorithm, this means that any hypothesis output by 



L


{\displaystyle L}

 can be represented by at most 



(
n
⋅
s
i
z
e
(
c
)

)

α



m

β




{\displaystyle (n\cdot size(c))^{\alpha }m^{\beta }}

 bits, and thus 



log
⁡

|




H



n
,
m



|

≤
(
n
⋅
s
i
z
e
(
c
)

)

α



m

β




{\displaystyle \log |{\mathcal {H}}_{n,m}|\leq (n\cdot size(c))^{\alpha }m^{\beta }}

. This is less than 



O
(
ϵ
m
)
−
log
⁡


1
δ




{\displaystyle O(\epsilon m)-\log {\frac {1}{\delta }}}

 if we set 



m
≥
a

(


1
ϵ


log
⁡


1
δ


+


(



(
n
⋅
s
i
z
e
(
c
)

)

α


)

ϵ


)



1

1
−
β




)



{\displaystyle m\geq a\left({\frac {1}{\epsilon }}\log {\frac {1}{\delta }}+\left({\frac {(n\cdot size(c))^{\alpha })}{\epsilon }}\right)^{\frac {1}{1-\beta }}\right)}

 for some constant 



a
>
0


{\displaystyle a>0}

. Thus, by the Cardinality version Theorem, 



L


{\displaystyle L}

 will output a consistent hypothesis 



h


{\displaystyle h}

 with probability at least 



1
−
δ


{\displaystyle 1-\delta }

. This concludes the proof of the first theorem above.","Though Occam and PAC learnability are equivalent, the Occam framework can be used to produce tighter bounds on the sample complexity of classical problems including conjunctions,[2] conjunctions with few relevant variables,[4] and decision lists.[5]","Occam algorithms have also been shown to be successful for PAC learning in the presence of errors,[6][7] probabilistic concepts,[8] function learning[9] and Markovian non-independent examples.[10]"
"In machine learning, systems which employ offline learning do not change their approximation of the target function when the initial training phase has been completed. These systems are also typically examples of eager learning.",
"OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks,[1] a main area of deep learning research. The library is open source, licensed under the GNU Lesser General Public License.",,,"The software implements any number of layers of non-linear processing units for supervised learning. This deep architecture allows the design of neural networks with universal approximation properties. Additionally, it allows multiprocessing programming by means of OpenMP, in order to increase computer performance.","OpenNN contains data mining algorithms as a bundle of functions. These can be embedded in other software tools, using an application programming interface, for the integration of the predictive analytics tasks. In this regard, a graphical user interface is missing but some functions can be supported by specific visualization tools.[2]","The development started in 2003 at the International Center for Numerical Methods in Engineering (CIMNE), within the research project funded by the European Union called RAMFLOOD.[3] Then it continued as part of similar projects. At present, OpenNN is being developed by the startup company Artelnics.[4]","In 2014, Big Data Analytics Today rated OpenNN as the #1 brain inspired artificial intelligence project.[5] Also, during the same year, ToppersWorld selected OpenNN among the top 5 open source data mining tools.[6]","OpenNN is a general purpose artificial intelligence software package.[7] It uses machine learning techniques for solving data mining and predictive analytics tasks in different fields. For instance, the library has been applied in the engineering,[8] energy,[9] or chemistry[10] sectors."
"Orange is a free software machine learning and data mining software (written in Python). It has a visual programming front-end for explorative data analysis and visualization, and can also be used as a Python library. The program is maintained and developed by the Bioinformatics Laboratory of the Faculty of Computer and Information Science at University of Ljubljana.",,,"Orange is a component-based visual programming software for data mining, machine learning and data analysis.","Components are called widgets and they range from simple data visualization, subset selection and preprocessing, to empirical evaluation of learning algorithms and predictive modeling.","Visual programming is implemented through an interface in which workflows are created by linking predefined or user-designed widgets, while advanced users can use Orange as a Python library for data manipulation and widget alteration.[1]","Orange is an open-source software released under GPL and available for use on github. Versions up to 3.0 include core components in C++ with wrappers in Python. From version 3.0 onwards, Orange uses common Python open-source libraries for scientific computing, such as numpy, scipy and scikit-learn, while its graphical user interface operates within the cross-platform Qt framework.","The default installation includes a number of machine learning, preprocessing and data visualization algorithms in 6 widget sets (data, visualize, classify, regression, evaluate and unsupervised). Additional functionalities are available as add-ons (bioinformatics, data fusion and text-mining).","Orange is supported on OS X, Windows and Linux and can also be installed from the Python Package Index repository (pip install Orange). As of 2016 the stable version is 3.3 and runs with Python 3, while the legacy version 2.7 that runs with Python 2.7 is still available.","Orange consists of a canvas interface onto which the user places widgets and creates a data analysis workflow. Widgets offer basic functionalities such as reading the data, showing a data table, selecting features, training predictors, comparing learning algorithms, visualizing data elements, etc. The user can interactively explore visualizations or feed the selected subset into other widgets.","The program provides a platform for experiment selection, recommendation systems and predictive modeling and is used in biomedicine, bioinformatics, genomic research, and teaching. In science, it is used as a platform for testing new machine learning algorithms and for implementing new techniques in genetics and bioinformatics. In education, it was used for teaching machine learning and data mining methods to students of biology, biomedicine and informatics."
"In statistics and machine learning, one of the most common tasks is to fit a ""model"" to a set of training data, so as to be able to make reliable predictions on general untrained data. In overfitting, a statistical model describes random error or noise instead of the underlying relationship. Overfitting occurs when a model is excessively complex, such as having too many parameters relative to the number of observations. A model that has been overfit has poor predictive performance, as it overreacts to minor fluctuations in the training data.","The possibility of overfitting exists because the criterion used for training the model is not the same as the criterion used to judge the efficacy of a model. In particular, a model is typically trained by maximizing its performance on some set of training data. However, its efficacy is determined not by its performance on the training data but by its ability to perform well on unseen data. Overfitting occurs when a model begins to ""memorize"" training data rather than ""learning"" to generalize from trend. As an extreme example, if the number of parameters is the same as or greater than the number of observations, a simple model or learning process can perfectly predict the training data simply by memorizing the training data in its entirety, but such a model will typically fail drastically when making predictions about new or unseen data, since the simple model has not learned to generalize at all.","The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data.","Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting.[1] In particular, the value of the coefficient of determination will shrink relative to the original training data.","In order to avoid overfitting, it is necessary to use additional techniques (e.g. cross-validation, regularization, early stopping, pruning, Bayesian priors on parameters or model comparison), that can indicate when further training is not resulting in better generalization. The basis of some techniques is either (1) to explicitly penalize overly complex models, or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.",,,"Usually a learning algorithm is trained using some set of ""training data"": exemplary situations for which the desired output is known. The goal is that the algorithm will also perform well on predicting the output when fed ""validation data"" that was not encountered during its training.","Overfitting is the use of models or procedures that violate Occam's razor, for example by including more adjustable parameters than are ultimately optimal, or by using a more complicated approach than is ultimately optimal. For an example where there are too many adjustable parameters, consider a dataset where training data for y can be adequately predicted by a linear function of two dependent variables. Such a function requires only three parameters (the intercept and two slopes). Replacing this simple function with a new, more complex quadratic function, or with a new, more complex linear function on more than two dependent variables, carries a risk: Occam's razor implies that any given complex function is a priori less probable than any given simple function. If the new, more complicated function is selected instead of the simple function, and if there was not a large enough gain in training-data fit to offset the complexity increase, then the new complex function ""overfits"" the data, and the complex overfitted function will likely perform worse than the simpler function on validation data outside the training dataset, even though the complex function performed as well, or perhaps even better, on the training dataset.[2]","When comparing different types of models, complexity cannot be measured solely by counting how many parameters exist in each model; the expressivity of each parameter must be considered as well. For example, it is nontrivial to directly compare the complexity of a neural net (which can track curvilinear relationships) with m parameters to a regression model with n parameters.[2]","Overfitting is especially likely in cases where learning was performed too long or where training examples are rare, causing the learner to adjust to very specific random features of the training data, that have no causal relation to the target function. In this process of overfitting, the performance on the training examples still increases while the performance on unseen data becomes worse.","As a simple example, consider a database of retail purchases that includes the item bought, the purchaser, and the date and time of purchase. It's easy to construct a model that will fit the training set perfectly by using the date and time of purchase to predict the other attributes; but this model will not generalize at all to new data, because those past times will never occur again.","Generally, a learning algorithm is said to overfit relative to a simpler one if it is more accurate in fitting known data (hindsight) but less accurate in predicting new data (foresight). One can intuitively understand overfitting from the fact that information from all past experience can be divided into two groups: information that is relevant for the future and irrelevant information (""noise""). Everything else being equal, the more difficult a criterion is to predict (i.e., the higher its uncertainty), the more noise exists in past information that needs to be ignored. The problem is determining which part to ignore. A learning algorithm that can reduce the chance of fitting noise is called robust.",The most obvious consequence of overfitting is poor performance on the validation dataset. Other negative consequences include:[2],"Outside machine learning, overfitting is also a problem in the broad study of regression, including regression done ""by hand"". In the extreme case, if there are p variables in a linear regression with p data points, the fitted line will go exactly through every point. [3] There is a variety of rules of thumb for the number of observations needed per independent variable, including 10 [4] and 10-15.[5]"
"Parity learning is a problem in machine learning. An algorithm that solves this problem must guess the function ƒ, given some samples (x, ƒ(x)) and the assurance that ƒ computes the parity of bits at some fixed locations. The samples are generated using some distribution over the input. The problem is easy to solve using Gaussian elimination provided that a sufficient number of samples (from a distribution which is not too skewed) are provided to the algorithm.","In this version, the samples may contain some error. Instead of samples (x, ƒ(x)), the algorithm is provided with (x, y), where y = 1 − ƒ(x) with some small probability. The noisy version of the parity learning problem is conjectured to be hard."
"In theoretical computer science, a pattern language is a formal language that can be defined as the set of all particular instances of a string of constants and variables. Pattern Languages were introduced by Dana Angluin in the context of machine learning.[1]",,,"Given a finite set Σ of constant symbols and a countable set X of variable symbols disjoint from Σ, a pattern is a finite non-empty string of symbols from Σ∪X. The length of a pattern p, denoted by |p|, is just the number of its symbols. The set of all patterns containing exactly n distinct variables (each of which may occur several times) is denoted by Pn, the set of all patterns at all by P*. A substitution is a mapping f: P* → P* such that[note 1]","If p = f(q) for some patterns p, q ∈ P* and some substitution f, then p is said to be less general than q, written p≤q; in that case, necessarily |p| ≥ |q| holds. For a pattern p, its language is defined as the set of all less general patterns that are built from constants only, formally: L(p) = { s ∈ Σ+ : s ≤ p }, where Σ+ denotes the set of all finite non-empty strings of symbols from Σ.","For example, using the constants Σ = { 0, 1 } and the variables X = { x, y, z, ... }, the pattern 0x10xx1 ∈P1 and xxy ∈P2 has length 7 and 3, respectively. An instance of the former pattern is 00z100z0z1 and 01z101z1z1, it is obtained by the substitution that maps x to 0z and to 1z, respectively, and each other symbol to itself. Both 00z100z0z1 and 01z101z1z1 are also instances of xxy. In fact, L(0x10xx1) is a subset of L(xxy). The language of the pattern x0 and x1 is the set of all bit strings which denote an even and odd number, respectively. The language of xx is the set of all strings obtainable by concatenating a bit string with itself, e.g. 00, 11, 0101, 1010, 11101110 ∈ L(xx).","The problem of deciding whether s ∈ L(p) for an arbitrary string s ∈ Σ+ and pattern p is NP-complete (see picture), and so is hence the problem of deciding p ≤ q for arbitrary patterns p, q.[2]",The class of pattern languages is not closed under ...,The class of pattern languages is closed under ...,"If p, q ∈ P1 are patterns containing exactly one variable, then p ≤ q if and only if L(p) ⊆ L(q); the same equivalence holds for patterns of equal length.[4] For patterns of different length, the above example p = 0x10xx1 and q = xxy shows that L(p) ⊆ L(q) may hold without implying p ≤ q. However, any two patterns p and q, of arbitrary lengths, generate the same language if and only if they are equal up to consistent variable renaming.[5] Each pattern p is a common generalization of all strings in its generated language L(p), modulo associativity of (⋅).","In a refined Chomsky hierarchy, the class of pattern languages is a proper superclass and subclass of the singleton[note 2] and the indexed languages, but incomparable to the language classes in between; due to the latter, the pattern language class is not explicitly shown in the table below.","The class of pattern languages is incomparable with the class of finite languages, with the class of regular languages, and with the class of context-free languages:","Each singleton language is trivially a pattern language, generated by a pattern without variables.","Each pattern language can be produced by an indexed grammar: For example, using Σ = { a, b, c } and X = { x, y }, the pattern a x b y c x a y b is generated by a grammar with nonterminal symbols N = { Sx, Sy, S } ∪ X, terminal symbols T = Σ, index symbols F = { ax, bx, cx, ay, by, cy }, start symbol Sx, and the following production rules:",An example derivation is:,Sx[]   ⇒   Sx[bx]   ⇒   Sx[ax bx]   ⇒   Sy[ax bx]   ⇒   Sy[cy ax bx]   ⇒   S[cy ax bx]   ⇒   a x[cy ax bx] b y[cy ax bx] c x[cy ax bx] a y[cy ax bx] b   ⇒   a x[ax bx] b y[cy ax bx] c x[cy ax bx] a y[cy ax bx] b   ⇒   a a x[bx] b y[cy ax bx] c x[cy ax bx] a y[cy ax bx] b   ⇒   a ab x[] b y[cy ax bx] c x[cy ax bx] a y[cy ax bx] b   ⇒   a ab b y[cy ax bx] c x[cy ax bx] a y[cy ax bx] b   ⇒ ... ⇒   a ab b c y[] c x[cy ax bx] a y[cy ax bx] b   ⇒   a ab b c c x[cy ax bx] a y[cy ax bx] b   ⇒ ... ⇒   a ab b c c ab x[] a y[cy ax bx] b   ⇒   a ab b c c ab a y[cy ax bx] b   ⇒ ... ⇒   a ab b c c ab a c y[] b   ⇒   a ab b c c ab a c b,"In a similar way, an index grammar can be constructed from any pattern.","Given a sample set S of strings, a pattern p is called descriptive of S if S ⊆ L(p), but not S ⊆ L(q) ⊂ L(p) for any other pattern q.","Given any sample set S, a descriptive pattern for S can be computed by","Based on this algorithm, the class of pattern languages can be identified in the limit from positive examples.[7]"
"Pattern recognition is a branch of machine learning that focuses on the recognition of patterns and regularities in data, although it is in some cases considered to be nearly synonymous with machine learning.[1] Pattern recognition systems are in many cases trained from labeled ""training"" data (supervised learning), but when no labeled data are available other algorithms can be used to discover previously unknown patterns (unsupervised learning).","The terms pattern recognition, machine learning, data mining and knowledge discovery in databases (KDD) are hard to separate, as they largely overlap in their scope. Machine learning is the common term for supervised learning methods[dubious – discuss] and originates from artificial intelligence, whereas KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition has its origins in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In pattern recognition, there may be a higher interest to formalize, explain and visualize the pattern, while machine learning traditionally focuses on maximizing the recognition rates. Yet, all of these domains have evolved substantially from their roots in artificial intelligence, engineering and statistics, and they've become increasingly similar by integrating developments and ideas from each other.","In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is ""spam"" or ""non-spam""). However, pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.[citation needed]","Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform ""most likely"" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors. In contrast to pattern recognition, pattern matching is generally not considered a type of machine learning, although pattern-matching algorithms (especially with fairly general, carefully tailored patterns) can sometimes succeed in providing similar-quality output of the sort provided by pattern-recognition algorithms.",,,"Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. Supervised learning assumes that a set of training data (the training set) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a model that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of ""simple"", in accordance with Occam's Razor, discussed below). Unsupervised learning, on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances.[2] A combination of the two that has recently been explored is semi-supervised learning, which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). Note that in cases of unsupervised learning, there may be no training data at all to speak of; in other words, the data to be labeled is the training data.","Note that sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. For example, the unsupervised equivalent of classification is normally known as clustering, based on the common perception of the task as involving no training data to speak of, and of grouping the input data into clusters based on some inherent similarity measure (e.g. the distance between instances, considered as vectors in a multi-dimensional vector space), rather than assigning each input instance into one of a set of pre-defined classes. Note also that in some fields, the terminology is different: For example, in community ecology, the term ""classification"" is used to refer to what is commonly known as ""clustering"".","The piece of input data for which an output value is generated is formally termed an instance. The instance is formally described by a vector of features, which together constitute a description of all known characteristics of the instance. (These feature vectors can be seen as defining points in an appropriate multidimensional space, and methods for manipulating vectors in vector spaces can be correspondingly applied to them, such as computing the dot product or the angle between two vectors.) Typically, features are either categorical (also known as nominal, i.e., consisting of one of a set of unordered items, such as a gender of ""male"" or ""female"", or a blood type of ""A"", ""B"", ""AB"" or ""O""), ordinal (consisting of one of a set of ordered items, e.g., ""large"", ""medium"" or ""small""), integer-valued (e.g., a count of the number of occurrences of a particular word in an email) or real-valued (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together; likewise for integer-valued and real-valued data. Furthermore, many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be discretized into groups (e.g., less than 5, between 5 and 10, or greater than 10).","Many common pattern recognition algorithms are probabilistic in nature, in that they use statistical inference to find the best label for a given instance. Unlike other algorithms, which simply output a ""best"" label, often probabilistic algorithms also output a probability of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the N-best labels with associated probabilities, for some value of N, instead of simply a single best label. When the number of possible labels is fairly small (e.g., in the case of classification), N may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms:","Feature selection algorithms attempt to directly prune out redundant or irrelevant features. A general introduction to feature selection which summarizes approaches and challenges, has been given.[3] The complexity of feature-selection is, because of its non-monotonous character, an optimization problem where given a total of 



n


{\displaystyle n}

 features the powerset consisting of all 




2

n


−
1


{\displaystyle 2^{n}-1}

 subsets of features need to be explored. The Branch-and-Bound algorithm[4] does reduce this complexity but is intractable for medium to large values of the number of available features 



n


{\displaystyle n}

. For a large-scale comparison of feature-selection algorithms see .[5]","Techniques to transform the raw feature vectors (feature extraction) are sometimes used prior to application of the pattern-matching algorithm. For example, feature extraction algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as principal components analysis (PCA). The distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features.","Formally, the problem of supervised pattern recognition can be stated as follows: Given an unknown function 



g
:


X


→


Y




{\displaystyle g:{\mathcal {X}}\rightarrow {\mathcal {Y}}}

 (the ground truth) that maps input instances 




x

∈


X




{\displaystyle {\boldsymbol {x}}\in {\mathcal {X}}}

 to output labels 



y
∈


Y




{\displaystyle y\in {\mathcal {Y}}}

, along with training data 




D

=
{
(


x


1


,

y

1


)
,
…
,
(


x


n


,

y

n


)
}


{\displaystyle \mathbf {D} =\{({\boldsymbol {x}}_{1},y_{1}),\dots ,({\boldsymbol {x}}_{n},y_{n})\}}

 assumed to represent accurate examples of the mapping, produce a function 



h
:


X


→


Y




{\displaystyle h:{\mathcal {X}}\rightarrow {\mathcal {Y}}}

 that approximates as closely as possible the correct mapping 



g


{\displaystyle g}

. (For example, if the problem is filtering spam, then 





x


i




{\displaystyle {\boldsymbol {x}}_{i}}

 is some representation of an email and 



y


{\displaystyle y}

 is either ""spam"" or ""non-spam""). In order for this to be a well-defined problem, ""approximates as closely as possible"" needs to be defined rigorously. In decision theory, this is defined by specifying a loss function that assigns a specific value to ""loss"" resulting from producing an incorrect label. The goal then is to minimize the expected loss, with the expectation taken over the probability distribution of 





X




{\displaystyle {\mathcal {X}}}

. In practice, neither the distribution of 





X




{\displaystyle {\mathcal {X}}}

 nor the ground truth function 



g
:


X


→


Y




{\displaystyle g:{\mathcal {X}}\rightarrow {\mathcal {Y}}}

 are known exactly, but can be computed only empirically by collecting a large number of samples of 





X




{\displaystyle {\mathcal {X}}}

 and hand-labeling them using the correct value of 





Y




{\displaystyle {\mathcal {Y}}}

 (a time-consuming process, which is typically the limiting factor in the amount of data of this sort that can be collected). The particular loss function depends on the type of label being predicted. For example, in the case of classification, the simple zero-one loss function is often sufficient. This corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data (i.e. counting up the fraction of instances that the learned function 



h
:


X


→


Y




{\displaystyle h:{\mathcal {X}}\rightarrow {\mathcal {Y}}}

 labels wrongly, which is equivalent to maximizing the number of correctly classified instances). The goal of the learning procedure is then to minimize the error rate (maximize the correctness) on a ""typical"" test set.","For a probabilistic pattern recognizer, the problem is instead to estimate the probability of each possible output label given a particular input instance, i.e., to estimate a function of the form","where the feature vector input is 




x



{\displaystyle {\boldsymbol {x}}}

, and the function f is typically parameterized by some parameters 




θ



{\displaystyle {\boldsymbol {\theta }}}

.[6] In a discriminative approach to the problem, f is estimated directly. In a generative approach, however, the inverse probability 



p
(


x


|



l
a
b
e
l



)


{\displaystyle p({{\boldsymbol {x}}|{\rm {label}}})}

 is instead estimated and combined with the prior probability 



p
(


l
a
b
e
l



|


θ

)


{\displaystyle p({\rm {label}}|{\boldsymbol {\theta }})}

 using Bayes' rule, as follows:","When the labels are continuously distributed (e.g., in regression analysis), the denominator involves integration rather than summation:","The value of 




θ



{\displaystyle {\boldsymbol {\theta }}}

 is typically learned using maximum a posteriori (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects: To perform as well as possible on the training data (smallest error-rate) and to find the simplest possible model. Essentially, this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models. In a Bayesian context, the regularization procedure can be viewed as placing a prior probability 



p
(

θ

)


{\displaystyle p({\boldsymbol {\theta }})}

 on different values of 




θ



{\displaystyle {\boldsymbol {\theta }}}

. Mathematically:","where 





θ


∗




{\displaystyle {\boldsymbol {\theta }}^{*}}

 is the value used for 




θ



{\displaystyle {\boldsymbol {\theta }}}

 in the subsequent evaluation procedure, and 



p
(

θ


|


D

)


{\displaystyle p({\boldsymbol {\theta }}|\mathbf {D} )}

, the posterior probability of 




θ



{\displaystyle {\boldsymbol {\theta }}}

, is given by","In the Bayesian approach to this problem, instead of choosing a single parameter vector 





θ


∗




{\displaystyle {\boldsymbol {\theta }}^{*}}

, the probability of a given label for a new instance 




x



{\displaystyle {\boldsymbol {x}}}

 is computed by integrating over all possible values of 




θ



{\displaystyle {\boldsymbol {\theta }}}

, weighted according to the posterior probability:","The first pattern classifier – the linear discriminant presented by Fisher – was developed in the Frequentist tradition. The frequentist approach entails that the model parameters are considered unknown, but objective. The parameters are then computed (estimated) from the collected data. For the linear discriminant, these parameters are precisely the mean vectors and the Covariance matrix. Also the probability of each class 



p
(


l
a
b
e
l



|


θ

)


{\displaystyle p({\rm {label}}|{\boldsymbol {\theta }})}

 is estimated from the collected dataset. Note that the usage of 'Bayes rule' in a pattern classifier does not make the classification approach Bayesian.","Bayesian statistics has its origin in Greek philosophy where a distinction was already made between the 'a priori' and the 'a posteriori' knowledge. Later Kant defined his distinction between what is a priori known – before observation – and the empirical knowledge gained from observations. In a Bayesian pattern classifier, the class probabilities 



p
(


l
a
b
e
l



|


θ

)


{\displaystyle p({\rm {label}}|{\boldsymbol {\theta }})}

 can be chosen by the user, which are then a priori. Moreover, experience quantified as a priori parameter values can be weighted with empirical observations – using e.g., the Beta- (conjugate prior) and Dirichlet-distributions. The Bayesian approach facilitates a seamless intermixing between expert knowledge in the form of subjective probabilities, and objective observations.",Probabilistic pattern classifiers can be used according to a frequentist or a Bayesian approach.,"Within medical science, pattern recognition is the basis for computer-aided diagnosis (CAD) systems. CAD describes a procedure that supports the doctor's interpretations and findings.","Other typical applications of pattern recognition techniques are automatic speech recognition, classification of text into several categories (e.g., spam/non-spam email messages), the automatic recognition of handwritten postal codes on postal envelopes, automatic recognition of images of human faces, or handwriting image extraction from medical forms.[7] The last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems.[8][9]","Optical character recognition is a classic example of the application of a pattern classifier, see OCR-example. The method of signing one's name was captured with stylus and overlay starting in 1990.[citation needed] The strokes, speed, relative min, relative max, acceleration and pressure is used to uniquely identify and confirm identity. Banks were first offered this technology, but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers..[citation needed]","Artificial neural networks (neural net classifiers) and Deep Learning have many real-world applications in image processing, a few examples:","For a discussion of the aforementioned applications of neural networks in image processing, see e.g.[13]","In psychology, pattern recognition (making sense of and identifying objects) is closely related to perception, which explains how the sensory inputs humans receive are made meaningful. Pattern recognition can be thought of in two different ways: the first being template matching and the second being feature detection. A template is a pattern used to produce items of the same proportions. The template-matching hypothesis suggests that incoming stimuli are compared with templates in the long term memory. If there is a match, the stimulus is identified. Feature detection models, such as the Pandemonium system for classifying letters (Selfridge, 1959), suggest that the stimuli are broken down into their component parts for identification. For example, a capital E has three horizontal lines and one vertical line.[14]","Algorithms for pattern recognition depend on the type of label output, on whether learning is supervised or unsupervised, and on whether the algorithm is statistical or non-statistical in nature. Statistical algorithms can further be categorized as generative or discriminative.",Parametric:[15],Nonparametric:[16],Unsupervised:,Supervised (?):,Supervised:,Unsupervised:,Supervised:,Unsupervised:,"This article is based on material taken from the Free On-line Dictionary of Computing prior to 1 November 2008 and incorporated under the ""relicensing"" terms of the GFDL, version 1.3 or later."
"Predictive learning is a technique of machine learning in which an agent tries to build a model of its environment by trying out different actions in various circumstances. It uses knowledge of the effects its actions appear to have, turning them into planning operators. These allow the agent to act purposefully in its world. Predictive learning is one attempt to learn with a minimum of pre-existing mental structure. It may have been inspired by Piaget's account of how children construct knowledge of the world by interacting with it. Gary Drescher's book 'Made-up Minds' was seminal for the area.","Another more recent predictive learning theory is Jeff Hawkins' memory-prediction framework, which is laid out in his On Intelligence."
"In computer science, a predictive state representation (PSR) is a new way to model a state of controlled dynamical system from a history of actions taken and resulting observations. PSR captures the state of a system as a vector of predictions for future tests (experiments) that can be done on the system.[1] A test is a sequence of action-observation pairs and its prediction is the probability of the test’s observation- sequence happening if the test’s action-sequence were to be executed on the system. One of the advantage of using PSR is that the predictions are directly related to observable quantities. This is in contrast to other models of dynamical systems, such as partially observable Markov decision processes (POMDPs) where the state of the system is represented as a probability distribution over unobserved nominal states.[2]",
"Preference learning is a subfield in machine learning in which the goal is to learn a predictive preference model from observed preference information.[1] In the view of supervised learning, preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.","While the concept of preference learning has been emerged for some time in many fields such as economics,[2] it's a relatively new topic in Artificial Intelligence research. Several workshops have been discussing preference learning and related topics in the past decade.[3]",,,"The main task in preference learning concerns problems in ""learning to rank"". According to different types of preference information observed, the tasks are categorized as three main problems in the book Preference Learning:[4]","In label ranking, the model has an instance space 



X
=
{

x

i


}




{\displaystyle X=\{x_{i}\}\,\!}

 and a finite set of labels 



Y
=
{

y

i



|

i
=
1
,
2
,
⋯
,
k
}




{\displaystyle Y=\{y_{i}|i=1,2,\cdots ,k\}\,\!}

. The preference information is given in the form 




y

i



≻

x



y

j






{\displaystyle y_{i}\succ _{x}y_{j}\,\!}

 indicating instance 



x




{\displaystyle x\,\!}

 shows preference in 




y

i






{\displaystyle y_{i}\,\!}

 rather than 




y

j






{\displaystyle y_{j}\,\!}

. A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance.","It was observed some conventional classification problems can be generalized in the framework of label ranking problem:[5] if a training instance 



x




{\displaystyle x\,\!}

 is labeled as class 




y

i






{\displaystyle y_{i}\,\!}

, it implies that 



∀
j
≠
i
,

y

i



≻

x



y

j






{\displaystyle \forall j\neq i,y_{i}\succ _{x}y_{j}\,\!}

. In multi-label situation, 



x




{\displaystyle x\,\!}

 is associated with a set of labels 



L
⊆
Y




{\displaystyle L\subseteq Y\,\!}

 and thus the model can extract a set of preference information 



{

y

i



≻

x



y

j



|


y

i


∈
L
,

y

j


∈
Y
∖
L
}




{\displaystyle \{y_{i}\succ _{x}y_{j}|y_{i}\in L,y_{j}\in Y\backslash L\}\,\!}

. Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label.","Instance ranking also has the instance space 



X




{\displaystyle X\,\!}

 and label set 



Y




{\displaystyle Y\,\!}

. In this task, labels are defined to have a fixed order 




y

1


≻

y

2


≻
⋯
≻

y

k






{\displaystyle y_{1}\succ y_{2}\succ \cdots \succ y_{k}\,\!}

 and each instance 




x

l






{\displaystyle x_{l}\,\!}

 is associated with a label 




y

l






{\displaystyle y_{l}\,\!}

. Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.","Object ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form 




x

i


≻

x

j






{\displaystyle x_{i}\succ x_{j}\,\!}

 and the model should find out a ranking order among instances.","There are two practical representations of the preference information 



A
≻
B




{\displaystyle A\succ B\,\!}

. One is assigning 



A




{\displaystyle A\,\!}

 and 



B




{\displaystyle B\,\!}

 with two real numbers 



a




{\displaystyle a\,\!}

 and 



b




{\displaystyle b\,\!}

 respectively such that 



a
>
b




{\displaystyle a>b\,\!}

. Another one is assigning a binary value 



V
(
A
,
B
)
∈
{
0
,
1
}




{\displaystyle V(A,B)\in \{0,1\}\,\!}

 for all pairs 



(
A
,
B
)




{\displaystyle (A,B)\,\!}

 denoting whether 



A
≻
B




{\displaystyle A\succ B\,\!}

 or 



B
≻
A




{\displaystyle B\succ A\,\!}

. Corresponding to these two different representations, there are two different techniques applied to the learning process.","If we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called utility function. For label ranking the mapping is a function 



f
:
X
×
Y
→

R





{\displaystyle f:X\times Y\rightarrow \mathbb {R} \,\!}

 such that 




y

i



≻

x



y

j


⇒
f
(
x
,

y

i


)
>
f
(
x
,

y

j


)




{\displaystyle y_{i}\succ _{x}y_{j}\Rightarrow f(x,y_{i})>f(x,y_{j})\,\!}

. For instance ranking and object ranking, the mapping is a function 



f
:
X
→

R





{\displaystyle f:X\rightarrow \mathbb {R} \,\!}

.",Finding the utility function is a regression learning problem which is well developed in machine learning.,"The binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervising learning approach. Fürnkranz, Johannes and Hüllermeier proposed this approach in label ranking problem.[6] For object ranking, there is an early approach by Cohen et al.[7]","Using preference relations to predict the ranking will not be so intuitive. Since preference relation is not transitive, it implies that the solution of ranking satisfying those relations would sometimes be unreachable, or there could be more than one solution. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification.[6]","Preference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Liu's survey paper.[8]",Another application of preference learning is recommender systems.[9] Online store may analyze customer's purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of user's ratings to provide more user preferred contents.
"Pattern recognition is a very active field of research intimately bound to machine learning. Also known as classification or statistical classification, pattern recognition aims at building a classifier that can determine the class of an input pattern. This procedure, known as training, corresponds to learning an unknown decision function based only on a set of input-output pairs 



(


x


i


,

y

i


)


{\displaystyle ({\boldsymbol {x}}_{i},y_{i})}

 that form the training data (or training set). Nonetheless, in real world applications such as character recognition, a certain amount of information on the problem is usually known beforehand. The incorporation of this prior knowledge into the training is the key element that will allow an increase of performance in many applications.",,,"Prior knowledge[1] refers to all information about the problem available in addition to the training data. However, in this most general form, determining a model from a finite set of samples without prior knowledge is an ill-posed problem, in the sense that a unique model may not exist. Many classifiers incorporate the general smoothness assumption that a test pattern similar to one of the training samples tends to be assigned to the same class.","The importance of prior knowledge in machine learning is suggested by its role in search and optimization. Loosely, the no free lunch theorem states that all search algorithms have the same average performance over all problems, and thus implies that to gain in performance on a certain application one must use a specialized algorithm that includes some prior knowledge about the problem.",The different types of prior knowledge encountered in pattern recognition are now regrouped under two main categories: class-invariance and knowledge on the data.,A very common type of prior knowledge in pattern recognition is the invariance of the class (or the output of the classifier) to a transformation of the input pattern. This type of knowledge is referred to as transformation-invariance. The mostly used transformations used in image recognition are:,"Incorporating the invariance to a transformation 




T

θ


:

x

↦

T

θ



x



{\displaystyle T_{\theta }:{\boldsymbol {x}}\mapsto T_{\theta }{\boldsymbol {x}}}

 parametrized in 



θ


{\displaystyle \theta }

 into a classifier of output 



f
(

x

)


{\displaystyle f({\boldsymbol {x}})}

 for an input pattern 




x



{\displaystyle {\boldsymbol {x}}}

 corresponds to enforcing the equality","Local invariance can also be considered for a transformation centered at 



θ
=
0


{\displaystyle \theta =0}

, so that 




T

0



x

=

x



{\displaystyle T_{0}{\boldsymbol {x}}={\boldsymbol {x}}}

, by using the constraint","The function 



f


{\displaystyle f}

 in these equations can be either the decision function of the classifier or its real-valued output.","Another approach is to consider class-invariance with respect to a ""domain of the input space"" instead of a transformation. In this case, the problem becomes finding 



f


{\displaystyle f}

 so that","where 




y


P





{\displaystyle y_{\mathcal {P}}}

 is the membership class of the region 





P




{\displaystyle {\mathcal {P}}}

 of the input space.","A different type of class-invariance found in pattern recognition is permutation-invariance, i.e. invariance of the class to a permutation of elements in a structured input. A typical application of this type of prior knowledge is a classifier invariant to permutations of rows of the matrix inputs.",Other forms of prior knowledge than class-invariance concern the data more specifically and are thus of particular interest for real-world applications. The three particular cases that most often occur when gathering data are:,"Prior knowledge of these can enhance the quality of the recognition if included in the learning. Moreover, not taking into account the poor quality of some data or a large imbalance between the classes can mislead the decision of a classifier."
Proactive learning[1] is a generalization of active learning designed to relax unrealistic assumptions and thereby reach practical applications.,"""Active learning seeks to select the most informative unlabeled instances and ask an omniscient oracle for their labels, so as to retrain a learning algorithm maximizing accuracy. However, the oracle is assumed to be infallible (never wrong), indefatigable (always answers), individual (only one oracle), and insensitive to costs (always free or always charges the same).""[1]","""In real life, it is possible and more general to have multiple sources of information with differing reliabilities or areas of expertise. Active learning also assumes that the single oracle is perfect, always providing a correct answer when requested. In reality, though, an ""oracle"" (if we generalize the term to mean any source of expert information) may be incorrect (fallible) with a probability that should be a function of the difficulty of the question. Moreover, an oracle may be reluctant – it may refuse to answer if it is too uncertain or too busy. Finally, active learning presumes the oracle is either free or charges uniform cost in label elicitation. Such an assumption is naive since cost is likely to be regulated by difficulty (amount of work required to formulate an answer) or other factors.""[1]","Proactive learning relaxes all four of these assumptions, relying on a decision-theoretic approach to jointly select the optimal oracle and instance, by casting the problem as a utility optimization problem subject to a budget constraint."
"Probability matching is a decision strategy in which predictions of class membership are proportional to the class base rates. Thus, if in the training set positive examples are observed 60% of the time, and negative examples are observed 40% of the time, then the observer using a probability-matching strategy will predict (for unlabeled examples) a class label of ""positive"" on 60% of instances, and a class label of ""negative"" on 40% of instances.","The optimal Bayesian decision strategy (to maximize the number of correct predictions, see Duda, Hart & Stork (2001)) in such a case is to always predict ""positive"" (i.e., predict the majority category in the absence of other information), which has 60% chance of winning rather than matching which has 52% of winning (where p is the probability of positive realization, the result of matching would be 




p

2


+
(
1
−
p

)

2




{\displaystyle p^{2}+(1-p)^{2}}

, here 



.6
×
.6
+
.4
×
.4


{\displaystyle .6\times .6+.4\times .4}

). The probability-matching strategy is of psychological interest because it is frequently employed by human subjects in decision and classification studies (where it may be related to Thompson sampling).",
"Product of experts (PoE) is a machine learning technique. It models a probability distribution by combining the output from several simpler distributions. It was proposed by Geoff Hinton, along with an algorithm for training the parameters of such a system.","The core idea is to combine several probability distributions (""experts"") by multiplying their density functions—making the PoE classification similar to an ""and"" operation. This allows each expert to make decisions on the basis of a few dimensions without having to cover the full dimensionality of a problem.","This is related to (but quite different from) a mixture model, where several probability distributions are combined via an ""or"" operation, which is a weighted sum of their density functions.",
"Proximal gradient (forward backward splitting) methods for learning is an area of research in optimization and statistical learning theory which studies algorithms for a general class of convex regularization problems where the regularization penalty may not be differentiable. One such example is 




ℓ

1




{\displaystyle \ell _{1}}

 regularization (also known as Lasso) of the form","Proximal gradient methods offer a general framework for solving regularization problems from statistical learning theory with penalties that are tailored to a specific problem application.[1][2] Such customized penalties can help to induce certain structure in problem solutions, such as sparsity (in the case of lasso) or group structure (in the case of group lasso).",,,Proximal gradient methods are applicable in a wide variety of scenarios for solving convex optimization problems of the form,"where 



F


{\displaystyle F}

 is convex and differentiable with Lipschitz continuous gradient, 



R


{\displaystyle R}

 is a convex, lower semicontinuous function which is possibly nondifferentiable, and 





H




{\displaystyle {\mathcal {H}}}

 is some set, typically a Hilbert space. The usual criterion of 



x


{\displaystyle x}

 minimizes 



F
(
x
)
+
R
(
x
)


{\displaystyle F(x)+R(x)}

 if and only if 



∇
(
F
+
R
)
(
x
)
=
0


{\displaystyle \nabla (F+R)(x)=0}

 in the convex, differentiable setting is now replaced by","where 



∂
φ


{\displaystyle \partial \varphi }

 denotes the subdifferential of a real-valued, convex function 



φ


{\displaystyle \varphi }

.","Given a convex function 



φ
:


H


→

R



{\displaystyle \varphi :{\mathcal {H}}\to \mathbb {R} }

 an important operator to consider is its proximity operator 




prox

φ


:


H


→


H




{\displaystyle \operatorname {prox} _{\varphi }:{\mathcal {H}}\to {\mathcal {H}}}

 defined by","which is well-defined because of the strict convexity of the 




ℓ

2




{\displaystyle \ell _{2}}

 norm. The proximity operator can be seen as a generalization of a projection.[1][3][4] We see that the proximity operator is important because 




x

∗




{\displaystyle x^{*}}

 is a minimizer to the problem 




min

x
∈


H




F
(
x
)
+
R
(
x
)


{\displaystyle \min _{x\in {\mathcal {H}}}F(x)+R(x)}

 if and only if","One important technique related to proximal gradient methods is the Moreau decomposition, which decomposes the identity operator as the sum of two proximity operators.[1] Namely, let 



φ
:


X


→

R



{\displaystyle \varphi :{\mathcal {X}}\to \mathbb {R} }

 be a lower semicontinuous, convex function on a vector space 





X




{\displaystyle {\mathcal {X}}}

. We define its Fenchel conjugate 




φ

∗


:


X


→

R



{\displaystyle \varphi ^{*}:{\mathcal {X}}\to \mathbb {R} }

 to be the function","The general form of Moreau's decomposition states that for any 



x
∈


X




{\displaystyle x\in {\mathcal {X}}}

 and any 



γ
>
0


{\displaystyle \gamma >0}

 that","which for 



γ
=
1


{\displaystyle \gamma =1}

 implies that 



x
=

prox

φ


⁡
(
x
)
+

prox


φ

∗




⁡
(
x
)


{\displaystyle x=\operatorname {prox} _{\varphi }(x)+\operatorname {prox} _{\varphi ^{*}}(x)}

.[1][3] The Moreau decomposition can be seen to be a generalization of the usual orthogonal decomposition of a vector space, analogous with the fact that proximity operators are generalizations of projections.[1]","In certain situations it may be easier to compute the proximity operator for the conjugate 




φ

∗




{\displaystyle \varphi ^{*}}

 instead of the function 



φ


{\displaystyle \varphi }

, and therefore the Moreau decomposition can be applied. This is the case for group lasso.","Consider the regularized empirical risk minimization problem with square loss and with the 




ℓ

1




{\displaystyle \ell _{1}}

 norm as the regularization penalty:","where 




x

i


∈


R


d



 and 


y

i


∈

R

.


{\displaystyle x_{i}\in \mathbb {R} ^{d}{\text{ and }}y_{i}\in \mathbb {R} .}

 The 




ℓ

1




{\displaystyle \ell _{1}}

 regularization problem is sometimes referred to as lasso (least absolute shrinkage and selection operator).[5] Such 




ℓ

1




{\displaystyle \ell _{1}}

 regularization problems are interesting because they induce sparse solutions, that is, solutions 



w


{\displaystyle w}

 to the minimization problem have relatively few nonzero components. Lasso can be seen to be a convex relaxation of the non-convex problem","where 



∥
w

∥

0




{\displaystyle \|w\|_{0}}

 denotes the 




ℓ

0




{\displaystyle \ell _{0}}

 ""norm"", which is the number of nonzero entries of the vector 



w


{\displaystyle w}

. Sparse solutions are of particular interest in learning theory for interpretability of results: a sparse solution can identify a small number of important factors.[5]","For simplicity we restrict our attention to the problem where 



λ
=
1


{\displaystyle \lambda =1}

. To solve the problem","we consider our objective function in two parts: a convex, differentiable term 



F
(
w
)
=


1
n



∑

i
=
1


n


(

y

i


−
⟨
w
,

x

i


⟩

)

2




{\displaystyle F(w)={\frac {1}{n}}\sum _{i=1}^{n}(y_{i}-\langle w,x_{i}\rangle )^{2}}

 and a convex function 



R
(
w
)
=
∥
w

∥

1




{\displaystyle R(w)=\|w\|_{1}}

. Note that 



R


{\displaystyle R}

 is not strictly convex.","Let us compute the proximity operator for 



R
(
w
)


{\displaystyle R(w)}

. First we find an alternative characterization of the proximity operator 




prox

R


⁡
(
x
)


{\displaystyle \operatorname {prox} _{R}(x)}

 as follows:","







u
=

prox

R


⁡
(
x
)

⟺



0
∈
∂

(
R
(
u
)
+


1
2


∥
u
−
x

∥

2


2


)






⟺



0
∈
∂
R
(
u
)
+
u
−
x





⟺



x
−
u
∈
∂
R
(
u
)
.






{\displaystyle {\begin{aligned}u=\operatorname {prox} _{R}(x)\iff &0\in \partial \left(R(u)+{\frac {1}{2}}\|u-x\|_{2}^{2}\right)\\\iff &0\in \partial R(u)+u-x\\\iff &x-u\in \partial R(u).\end{aligned}}}

","For 



R
(
w
)
=
∥
w

∥

1




{\displaystyle R(w)=\|w\|_{1}}

 it is easy to compute 



∂
R
(
w
)


{\displaystyle \partial R(w)}

: the 



i


{\displaystyle i}

th entry of 



∂
R
(
w
)


{\displaystyle \partial R(w)}

 is precisely","Using the recharacterization of the proximity operator given above, for the choice of 



R
(
w
)
=
∥
w

∥

1




{\displaystyle R(w)=\|w\|_{1}}

 and 



γ
>
0


{\displaystyle \gamma >0}

 we have that 




prox

γ
R


⁡
(
x
)


{\displaystyle \operatorname {prox} _{\gamma R}(x)}

 is defined entrywise by","which is known as the soft thresholding operator 




S

γ


(
x
)
=

prox

γ
∥
⋅

∥

1




⁡
(
x
)


{\displaystyle S_{\gamma }(x)=\operatorname {prox} _{\gamma \|\cdot \|_{1}}(x)}

.[1][6]",To finally solve the lasso problem we consider the fixed point equation shown earlier:,"Given that we have computed the form of the proximity operator explicitly, then we can define a standard fixed point iteration procedure. Namely, fix some initial 




w

0


∈


R


d




{\displaystyle w^{0}\in \mathbb {R} ^{d}}

, and for 



k
=
1
,
2
,
…


{\displaystyle k=1,2,\ldots }

 define","Note here the effective trade-off between the empirical error term 



F
(
w
)


{\displaystyle F(w)}

 and the regularization penalty 



R
(
w
)


{\displaystyle R(w)}

. This fixed point method has decoupled the effect of the two different convex functions which comprise the objective function into a gradient descent step (




w

k


−
γ
∇
F

(

w

k


)



{\displaystyle w^{k}-\gamma \nabla F\left(w^{k}\right)}

) and a soft thresholding step (via 




S

γ




{\displaystyle S_{\gamma }}

).","Convergence of this fixed point scheme is well-studied in the literature[1][6] and is guaranteed under appropriate choice of step size 



γ


{\displaystyle \gamma }

 and loss function (such as the square loss taken here). Accelerated methods were introduced by Nesterov in 1983 which improve the rate of convergence under certain regularity assumptions on 



F


{\displaystyle F}

.[7] Such methods have been studied extensively in previous years.[8] For more general learning problems where the proximity operator cannot be computed explicitly for some regularization term 



R


{\displaystyle R}

, such fixed point schemes can still be carried out using approximations to both the gradient and the proximity operator.[4][9]",There have been numerous developments within the past decade in convex optimization techniques which have influenced the application of proximal gradient methods in statistical learning theory. Here we survey a few important topics which can greatly improve practical algorithmic performance of these methods.[2][10],In the fixed point iteration scheme,"one can allow variable step size 




γ

k




{\displaystyle \gamma _{k}}

 instead of a constant 



γ


{\displaystyle \gamma }

. Numerous adaptive step size schemes have been proposed throughout the literature.[1][4][11][12] Applications of these schemes[2][13] suggest that these can offer substantial improvement in number of iterations required for fixed point convergence.","Elastic net regularization offers an alternative to pure 




ℓ

1




{\displaystyle \ell _{1}}

 regularization. The problem of lasso (




ℓ

1




{\displaystyle \ell _{1}}

) regularization involves the penalty term 



R
(
w
)
=
∥
w

∥

1




{\displaystyle R(w)=\|w\|_{1}}

, which is not strictly convex. Hence, solutions to 




min

w


F
(
w
)
+
R
(
w
)
,


{\displaystyle \min _{w}F(w)+R(w),}

 where 



F


{\displaystyle F}

 is some empirical loss function, need not be unique. This is often avoided by the inclusion of an additional strictly convex term, such as an 




ℓ

2




{\displaystyle \ell _{2}}

 norm regularization penalty. For example, one can consider the problem","where 




x

i


∈


R


d



 and 


y

i


∈

R

.


{\displaystyle x_{i}\in \mathbb {R} ^{d}{\text{ and }}y_{i}\in \mathbb {R} .}

 For 



0
<
μ
≤
1


{\displaystyle 0<\mu \leq 1}

 the penalty term 



λ

(
(
1
−
μ
)
∥
w

∥

1


+
μ
∥
w

∥

2


2


)



{\displaystyle \lambda \left((1-\mu )\|w\|_{1}+\mu \|w\|_{2}^{2}\right)}

 is now strictly convex, and hence the minimization problem now admits a unique solution. It has been observed that for sufficiently small 



μ
>
0


{\displaystyle \mu >0}

, the additional penalty term 



μ
∥
w

∥

2


2




{\displaystyle \mu \|w\|_{2}^{2}}

 acts as a preconditioner and can substantially improve convergence while not adversely affecting the sparsity of solutions.[2][14]",Proximal gradient methods provide a general framework which is applicable to a wide variety of problems in statistical learning theory. Certain problems in learning can often involve data which has additional structure that is known a priori. In the past several years there have been new developments which incorporate information about group structure to provide methods which are tailored to different applications. Here we survey a few such methods.,"Group lasso is a generalization of the lasso method when features are grouped into disjoint blocks.[15] Suppose the features are grouped into blocks 



{

w

1


,
…
,

w

G


}


{\displaystyle \{w_{1},\ldots ,w_{G}\}}

. Here we take as a regularization penalty","which is the sum of the 




ℓ

2




{\displaystyle \ell _{2}}

 norm on corresponding feature vectors for the different groups. A similar proximity operator analysis as above can be used to compute the proximity operator for this penalty. Where the lasso penalty has a proximity operator which is soft thresholding on each individual component, the proximity operator for the group lasso is soft thresholding on each group. For the group 




w

g




{\displaystyle w_{g}}

 we have that proximity operator of 



λ
γ

(

∑

g
=
1


G


∥

w

g



∥

2


)



{\displaystyle \lambda \gamma \left(\sum _{g=1}^{G}\|w_{g}\|_{2}\right)}

 is given by","where 




w

g




{\displaystyle w_{g}}

 is the 



g


{\displaystyle g}

th group.","In contrast to lasso, the derivation of the proximity operator for group lasso relies on the Moreau decomposition. Here the proximity operator of the conjugate of the group lasso penalty becomes a projection onto the ball of a dual norm.[2]","In contrast to the group lasso problem, where features are grouped into disjoint blocks, it may be the case that grouped features are overlapping or have a nested structure. Such generalizations of group lasso have been considered in a variety of contexts.[16][17][18][19] For overlapping groups one common approach is known as latent group lasso which introduces latent variables to account for overlap.[20][21] Nested group structures are studied in hierarchical structure prediction and with directed acyclic graphs.[18]"
"Quantum machine learning is a newly emerging interdisciplinary research area between quantum physics and computer science that summarises efforts to combine quantum mechanics with methods of machine learning.[1][2] Quantum machine learning models or algorithms intend to use the advantages of quantum information in order to improve classical methods of machine learning, for example by developing efficient implementations of expensive classical algorithms on a quantum computer.[3][4][5] However, quantum machine learning also includes the vice versa approach, namely applying classical methods of machine learning to quantum information theory.","Although yet in its infancy, quantum machine learning is met with high expectations of providing a solution for big data analysis using the ‘parallel’ power of quantum computation.[6] This trend is underlined by recent investments of companies such as Google and Microsoft into quantum computing hardware and research. However, quantum machine learning is still in its infancy and requires more theoretical foundations as well as solid scientific results in order to mature to a full academic discipline.",,,A number of proposals suggest ideas of how to adapt classical methods of machine learning to quantum information processing.[7],"A support vector machine can be implemented on a quantum computer using a combination of known quantum algorithms.[8] In order to construct the hyperplane separating the dataset for classification tasks, the linear equation from the dual form or least squares formulation is solved using a quantum algorithm to solve linear equations [9] An important trick is thereby a routine to construct a density matrix whose entries correspond to those of the kernel matrix.","Extracting information from the final state can be done through quantum principal component analysis.[10] The classification of a new input is accomplished through a so-called swap test, in which the overlap between two quantum states is calculated. The quantum support vector machine can be implemented in time that depends logarithmically on the dimension of the feature space and the number of training vectors, while the classical solution requires a polynomial dependence.[11] First experiments on a quantum support vector machine have been realised.[12]","Machine learning algorithms such as k-means clustering or classification with k-nearest neighbours are based on calculating distances between feature vectors and selecting the closest one (either to identify the nearest cluster centroid or the nearest neighbours to a certain feature vector). Implementing such distance-based methods on a quantum computer means in the first place to find a way of calculating classical distances with quantum algorithms. A frequent idea is to employ the overlap of two carefully prepared wavefunctions 



⟨
ψ

|

φ
⟩


{\displaystyle \langle \psi |\varphi \rangle }

 as a distance measure between quantum states.",The minimum distance can be found based on an iterative Grover search.[13][14],"Distance-based machine learning algorithms such as unsupervised clustering can also be implemented through adiabatic quantum computing which improves the classical computation time of 



O
(
M
l
o
g
(
M
N
)
)


{\displaystyle O(Mlog(MN))}

 for Lloyd’s algorithm to 



O
(
k

l
o
g
(
M
N
)
)


{\displaystyle O(k\;log(MN))}

 (where M is the number of N-dimensional data vectors, and k is the given number of clusters).[15]","First experiment on distance-based quantum machine learning algorithms has been implemented on a photonic quantum computer up to eight dimensions, demonstrating supervised nearest-neighbor algorithm and unsupervised k-means algorithm.[16]","Quantum neural networks were initially discussed from a different perspective, namely the question of whether and how quantum effects could play a role in the brain’s biological neural networks.[17] However, the debate quickly shifted towards a purely computational focus on quantum versions of artificial neural networks, which play an important role in machine learning. A number of ideas for quantum neural network models have been published since.[18][19][20][21][22][23] An interesting approach for quantum machine learning is the quantum associative memory model based on Grover’s search algorithm.[24] However, finding a convincing method to train a quantum neural network is still an open task.[25]","The term quantum machine learning can also be used for approaches that apply classical methods of machine learning to problems of quantum information theory. For example, when experimentalists have to deal with incomplete information on a quantum system or source, Bayesian methods and concepts of algorithmic learning can be fruitfully applied. This includes machine learning approaches to quantum state classification,[26] Hamiltonian learning,[27] and learning an unknown unitary transformation.[28][29]","Not only academia but also leading IT companies show interest in the potential of quantum machine learning for future technological implementations. Google Research launched its Quantum Artificial Intelligence Lab in 2013.[30] which is run as a joint initiative together with NASA and the Universities Space Research Association. An important hardware asset is the controversially debated D-Wave quantum computer.[31] Also Microsoft seems to become interested in the topic, and Microsoft’s Head of Research Peter Lee announced to “dramatically” increase the companies’ activity in quantum computing.[32]"
A query level feature or QLF is a ranking feature utilized in a machine-learned ranking algorithm.,Example QLFs:,,
"In computational learning theory (machine learning and theory of computation), Rademacher complexity, named after Hans Rademacher, measures richness of a class of real-valued functions with respect to a probability distribution.","Given a training sample 



S
=
(

z

1


,

z

2


,
…
,

z

m


)
∈

Z

m




{\displaystyle S=(z_{1},z_{2},\dots ,z_{m})\in Z^{m}}

, and a class 





H




{\displaystyle {\mathcal {H}}}

 of real-valued functions defined on a domain space 



Z


{\displaystyle Z}

, the empirical Rademacher complexity of 





H




{\displaystyle {\mathcal {H}}}

 is defined as:","








R

^




S


(


H


)
=


2
m



E


[

sup

h
∈


H





|

∑

i
=
1


m



σ

i


h
(

z

i


)
|

 


|


 
S
]



{\displaystyle {\widehat {\mathcal {R}}}_{S}({\mathcal {H}})={\frac {2}{m}}\mathbb {E} \left[\sup _{h\in {\mathcal {H}}}\left|\sum _{i=1}^{m}\sigma _{i}h(z_{i})\right|\ {\bigg |}\ S\right]}

","where 




σ

1


,

σ

2


,
…
,

σ

m




{\displaystyle \sigma _{1},\sigma _{2},\dots ,\sigma _{m}}

 are independent random variables drawn from the Rademacher distribution i.e. 



Pr
(

σ

i


=
+
1
)
=
Pr
(

σ

i


=
−
1
)
=
1

/

2


{\displaystyle \Pr(\sigma _{i}=+1)=\Pr(\sigma _{i}=-1)=1/2}

 for 



i
=
1
,
2
,
…
,
m


{\displaystyle i=1,2,\dots ,m}

.","Let 



P


{\displaystyle P}

 be a probability distribution over 



Z


{\displaystyle Z}

. The Rademacher complexity of the function class 





H




{\displaystyle {\mathcal {H}}}

 with respect to 



P


{\displaystyle P}

 for sample size 



m


{\displaystyle m}

 is:","






R



m


(


H


)
=

E


[





R

^




S


(


H


)
]



{\displaystyle {\mathcal {R}}_{m}({\mathcal {H}})=\mathbb {E} \left[{\widehat {\mathcal {R}}}_{S}({\mathcal {H}})\right]}

","where the above expectation is taken over an identically independently distributed (i.i.d.) sample 



S
=
(

z

1


,

z

2


,
…
,

z

m


)


{\displaystyle S=(z_{1},z_{2},\dots ,z_{m})}

 generated according to 



P


{\displaystyle P}

.","One can show, for example, that there exists a constant 



C


{\displaystyle C}

, such that any class of 



{
0
,
1
}


{\displaystyle \{0,1\}}

-indicator functions with Vapnik-Chervonenkis dimension 



d


{\displaystyle d}

 has Rademacher complexity upper-bounded by 



C



d
m





{\displaystyle C{\sqrt {\frac {d}{m}}}}

.","Gaussian complexity is a similar complexity with similar physical meanings, and can be obtained from the previous complexity using the random variables 




g

i




{\displaystyle g_{i}}

 instead of 




σ

i




{\displaystyle \sigma _{i}}

, where 




g

i




{\displaystyle g_{i}}

 are Gaussian i.i.d. random variables with zero-mean and variance 1, i.e. 




g

i


∼


N



(
0
,
1
)



{\displaystyle g_{i}\sim {\mathcal {N}}\left(0,1\right)}

."
"Random indexing is a dimension reduction method and computational framework for Distributional semantics, based on the insight that very-high-dimensional Vector Space Model implementations are impractical, that models need not grow in dimensionality when new items (e.g. new terminology) is encountered, and that a high-dimensional model can be projected into a space of lower dimensionality without compromising L2 distance metrics if the resulting dimensions are chosen appropriately, which is the original point of the random projection approach to dimension reduction first formulated as the Johnson–Lindenstrauss lemma. Locality-sensitive hashing has some of the same starting points. Random indexing, as used in representation of language, originates from the work of Pentti Kanerva[1] on Sparse distributed memory, and can be described as an incremental formulation of a random projection.[2]","It can be also verified that random indexing is a random projection technique for the construction of Euclidean spaces---i.e. L2 normed vector spaces.[3] In Euclidean spaces, random projections are elucidated using the Johnson–Lindenstrauss lemma.[4]","TopSig [5] extends the Random Indexing model to produce bit vectors for comparison with the Hamming distance similarity function. It is used for improving the performance of information retrieval and document clustering. In a similar line of research, Random Manhattan Integer Indexing[6] is proposed for improving the performance of the methods that employ the Manhattan distance between text units.",
"In mathematics and statistics, random projection is a technique used to reduce the dimensionality of a set of points which lie in Euclidean space. Random projection methods are powerful methods known for their simplicity and less erroneous output compared with other methods. According to experimental results, random projection preserve distances well, but empirical results are sparse.[1]","Dimensionality reduction, as the name suggests, is reducing the number of random variables using various mathematical methods from statistics and machine learning. Dimensionality reduction is often used to reduce the problem of managing and manipulating large data sets. Dimensionality reduction techniques generally use linear transformations in determining the intrinsic dimensionality of the manifold as well as extracting its principal directions. For this purpose there are various related techniques, including: principal component analysis, linear discriminant analysis, canonical correlation analysis, discrete cosine transform, random projection, etc.",Random projection is a simple and computationally efficient way to reduce the dimensionality of data by trading a controlled amount of error for faster processing times and smaller model sizes. The dimensions and distribution of random projection matrices are controlled so as to approximately preserve the pairwise distances between any two samples of the dataset.,"The core idea behind random projection is given in the Johnson-Lindenstrauss lemma,[2] which states that if points in a vector space are of sufficiently high dimension, then they may be projected into a suitable lower-dimensional space in a way which approximately preserves the distances between the points.","In random projection, the original d-dimensional data is projected to a k-dimensional (k << d) subspace, using a random 



k
×
d


{\displaystyle k\times d}

 - dimensional matrix R whose rows have unit lengths. Using matrix notation: If 




X

d
×
N




{\displaystyle X_{d\times N}}

 is the original set of N d-dimensional observations, then 




X

k
×
N


R
P


=

R

k
×
d



X

d
×
N




{\displaystyle X_{k\times N}^{RP}=R_{k\times d}X_{d\times N}}

 is the projection of the data onto a lower k-dimensional subspace. Random projection is computationally simple: form the random matrix ""R"" and project the 



d
×
N


{\displaystyle d\times N}

 data matrix X onto K dimensions of order 



O
(
d
k
N
)


{\displaystyle O(dkN)}

. If the data matrix X is sparse with about c nonzero entries per column, then the complexity of this operation is of order 



O
(
c
k
N
)


{\displaystyle O(ckN)}

.[3]","The random matrix R can be generated using a Gaussian distribution.[4] The first row is a random unit vector uniformly chosen from 




S

N
−
1




{\displaystyle S^{N-1}}

. The second row is a random unit vector from the space orthogonal to the first row, the third row is a random unit vector from the space orthogonal to the first two rows, and so on. In this way of choosing R, the following properties are satisfied:",Achlioptas[5] has shown that the Gaussian distribution can be replaced by a much simpler distribution such as,This is efficient for database applications because the computations can be performed using integer arithmetic.
"Relational data mining is the data mining technique for relational databases.[1] Unlike traditional data mining algorithms, which look for patterns in a single table (propositional patterns), relational data mining algorithms look for patterns among multiple tables (relational patterns). For most types of propositional patterns, there are corresponding relational patterns. For example, there are relational classification rules (relational classification), relational regression tree, and relational association rules.",There are several approaches to relational data mining:,,,"Multi-Relation Association Rules: Multi-Relation Association Rules (MRAR) is a new class of association rules which in contrast to primitive, simple and even multi-relational association rules (that are usually extracted from multi-relational databases), each rule item consists of one entity but several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations live in, nearby and humid: “Those who live in a place which is near by a city with humid climate type and also are younger than 20 -> their health condition is good”. Such association rules are extractable from RDBMS data or semantic web data.[2]",
"In statistical learning theory, a representer theorem is any of several related results stating that a minimizer 




f

∗




{\displaystyle f^{*}}

 of a regularized empirical risk function defined over a reproducing kernel Hilbert space can be represented as a finite linear combination of kernel products evaluated on the input points in the training set data.",,,"The following Representer Theorem and its proof are due to Schölkopf, Herbrich, and Smola:","Theorem: Let 





X




{\displaystyle {\mathcal {X}}}

 be a nonempty set and 



k


{\displaystyle k}

 a positive-definite real-valued kernel on 





X


×


X




{\displaystyle {\mathcal {X}}\times {\mathcal {X}}}

 with corresponding reproducing kernel Hilbert space 




H

k




{\displaystyle H_{k}}

. Given a training sample 



(

x

1


,

y

1


)
,
…
,
(

x

n


,

y

n


)
∈


X


×

R



{\displaystyle (x_{1},y_{1}),\dotsc ,(x_{n},y_{n})\in {\mathcal {X}}\times \mathbb {R} }

, a strictly monotonically increasing real-valued function 



g
:
[
0
,
∞
)
→

R



{\displaystyle g\colon [0,\infty )\to \mathbb {R} }

, and an arbitrary empirical risk function 



E
:
(


X


×


R


2



)

n


→

R

∪
{
∞
}


{\displaystyle E\colon ({\mathcal {X}}\times \mathbb {R} ^{2})^{n}\to \mathbb {R} \cup \lbrace \infty \rbrace }

, then for any 




f

∗


∈

H

k




{\displaystyle f^{*}\in H_{k}}

 satisfying","




f

∗




{\displaystyle f^{*}}

 admits a representation of the form:","where 




α

i


∈

R



{\displaystyle \alpha _{i}\in \mathbb {R} }

 for all 



1
≤
i
≤
n


{\displaystyle 1\leq i\leq n}

.",Proof: Define a mapping,"(so that 



φ
(
x
)
=
k
(
⋅
,
x
)


{\displaystyle \varphi (x)=k(\cdot ,x)}

 is itself a map 





X


→

R



{\displaystyle {\mathcal {X}}\to \mathbb {R} }

). Since 



k


{\displaystyle k}

 is a reproducing kernel, then","where 



⟨
⋅
,
⋅
⟩


{\displaystyle \langle \cdot ,\cdot \rangle }

 is the inner product on 




H

k




{\displaystyle H_{k}}

.","Given any 




x

1


,
.
.
.
,

x

n




{\displaystyle x_{1},...,x_{n}}

, one can use orthogonal projection to decompose any 



f
∈

H

k




{\displaystyle f\in H_{k}}

 into a sum of two functions, one lying in 



span
⁡

{
φ
(

x

1


)
,
.
.
.
,
φ
(

x

n


)
}



{\displaystyle \operatorname {span} \left\lbrace \varphi (x_{1}),...,\varphi (x_{n})\right\rbrace }

, and the other lying in the orthogonal complement:","where 



⟨
v
,
φ
(

x

i


)
⟩
=
0


{\displaystyle \langle v,\varphi (x_{i})\rangle =0}

 for all 



i


{\displaystyle i}

.","The above orthogonal decomposition and the reproducing property together show that applying 



f


{\displaystyle f}

 to any training point 




x

j




{\displaystyle x_{j}}

 produces","which we observe is independent of 



v


{\displaystyle v}

. Consequently, the value of the empirical risk 



E


{\displaystyle E}

 in (*) is likewise independent of 



v


{\displaystyle v}

. For the second term (the regularization term), since 



v


{\displaystyle v}

 is orthogonal to 




∑

i
=
1


n



α

i


φ
(

x

i


)


{\displaystyle \sum _{i=1}^{n}\alpha _{i}\varphi (x_{i})}

 and 



g


{\displaystyle g}

 is strictly monotonic, we have","Therefore setting 



v
=
0


{\displaystyle v=0}

 does not affect the first term of (*), while it strictly decreasing the second term. Consequently, any minimizer 




f

∗




{\displaystyle f^{*}}

 in (*) must have 



v
=
0


{\displaystyle v=0}

, i.e., it must be of the form",which is the desired result.,"The Theorem stated above is a particular example of a family of results that are collectively referred to as ""representer theorems""; here we describe several such.",The first statement of a representer theorem was due to Kimeldorf and Wahba for the special case in which,"for 



λ
>
0


{\displaystyle \lambda >0}

. Schölkopf, Herbrich, and Smola generalized this result by relaxing the assumption of the squared-loss cost and allowing the regularizer to be any strictly monotonically increasing function 



g
(
⋅
)


{\displaystyle g(\cdot )}

 of the Hilbert space norm.","It is possible to generalize further by augmenting the regularized empirical risk function through the addition of unpenalized offset terms. For example, Schölkopf, Herbrich, and Smola also consider the minimization","i.e., we consider functions of the form 






f
~



=
f
+
h


{\displaystyle {\tilde {f}}=f+h}

, where 



f
∈

H

k




{\displaystyle f\in H_{k}}

 and 



h


{\displaystyle h}

 is an unpenalized function lying in the span of a finite set of real-valued functions 



{

ψ

p


:


X


→

R

∣
1
≤
p
≤
M
}


{\displaystyle \lbrace \psi _{p}\colon {\mathcal {X}}\to \mathbb {R} \mid 1\leq p\leq M\rbrace }

. Under the assumption that the 



m
×
M


{\displaystyle m\times M}

 matrix 





(

ψ

p


(

x

i


)
)


i
p




{\displaystyle \left(\psi _{p}(x_{i})\right)_{ip}}

 has rank 



M


{\displaystyle M}

, they show that the minimizer 







f
~




∗




{\displaystyle {\tilde {f}}^{*}}

 in 



(
†
)


{\displaystyle (\dagger )}

 admits a representation of the form","where 




α

i


,

β

p


∈

R



{\displaystyle \alpha _{i},\beta _{p}\in \mathbb {R} }

 and the 




β

p




{\displaystyle \beta _{p}}

 are all uniquely determined.","The conditions under which a representer theorem exists were investigated by Argyriou, Miccheli, and Pontil, who proved the following:","Theorem: Let 





X




{\displaystyle {\mathcal {X}}}

 be a nonempty set, 



k


{\displaystyle k}

 a positive-definite real-valued kernel on 





X


×


X




{\displaystyle {\mathcal {X}}\times {\mathcal {X}}}

 with corresponding reproducing kernel Hilbert space 




H

k




{\displaystyle H_{k}}

, and let 



R
:

H

k


→

R



{\displaystyle R\colon H_{k}\to \mathbb {R} }

 be a differentiable regularization function. Then given a training sample 



(

x

1


,

y

1


)
,
.
.
.
,
(

x

n


,

y

n


)
∈


X


×

R



{\displaystyle (x_{1},y_{1}),...,(x_{n},y_{n})\in {\mathcal {X}}\times \mathbb {R} }

 and an arbitrary empirical risk function 



E
:
(


X


×


R


2



)

m


→

R

∪
{
∞
}


{\displaystyle E\colon ({\mathcal {X}}\times \mathbb {R} ^{2})^{m}\to \mathbb {R} \cup \lbrace \infty \rbrace }

, a minimizer",of the regularized empirical risk minimization problem admits a representation of the form,"where 




α

i


∈

R



{\displaystyle \alpha _{i}\in \mathbb {R} }

 for all 



1
≤
i
≤
n


{\displaystyle 1\leq i\leq n}

, if and only if there exists a nondecreasing function 



h
:
[
0
,
∞
)
→

R



{\displaystyle h\colon [0,\infty )\to \mathbb {R} }

 for which","Effectively, this result provides a necessary and sufficient condition on a differentiable regularizer 



R
(
⋅
)


{\displaystyle R(\cdot )}

 under which the corresponding regularized empirical risk minimization 



(
‡
)


{\displaystyle (\ddagger )}

 will have a representer theorem. In particular, this shows that a broad class of regularized risk minimizations (much broader than those originally considered by Kimeldorf and Wahba) have representer theorems.","Representer theorems are useful from a practical standpoint because they dramatically simplify the regularized empirical risk minimization problem 



(
‡
)


{\displaystyle (\ddagger )}

. In most interesting applications, the search domain 




H

k




{\displaystyle H_{k}}

 for the minimization will be an infinite-dimensional subspace of 




L

2


(


X


)


{\displaystyle L^{2}({\mathcal {X}})}

, and therefore the search (as written) does not admit implementation on finite-memory and finite-precision computers. In contrast, the representation of 




f

∗


(
⋅
)


{\displaystyle f^{*}(\cdot )}

 afforded by a representer theorem reduces the original (infinite-dimensional) minimization problem to a search for the optimal 



n


{\displaystyle n}

-dimensional vector of coefficients 



α
=
(

α

1


,
.
.
.
,

α

n


)
∈


R


n




{\displaystyle \alpha =(\alpha _{1},...,\alpha _{n})\in \mathbb {R} ^{n}}

; 



α


{\displaystyle \alpha }

 can then be obtained by applying any standard function minimization algorithm. Consequently, representer theorems provide the theoretical basis for the reduction of the general machine learning problem to algorithms that can actually be implemented on computers in practice."
"Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives).","Example of skills that are targeted by learning algorithms include sensorimotor skills such as locomotion, grasping, active object categorization, as well as interactive skills such as joint manipulation of an object with a human peer, and linguistic skills such as the grounded and situated meaning of human language. Learning can happen either through autonomous self-exploration or through guidance from a human teacher, like for example in robot learning by imitation.","Robot learning can be closely related to adaptive control, reinforcement learning as well as developmental robotics which considers the problem of autonomous lifelong acquisition of repertoires of skills. While machine learning is frequently used by computer vision algorithms employed in the context of robotics, these applications are usually not referred to as ""robot learning"".",
"Rule induction is an area of machine learning in which formal rules are extracted from a set of observations. The rules extracted may represent a full scientific model of the data, or merely represent local patterns in the data.",Some major rule induction paradigms are:,Some rule induction algorithms are:
The sample complexity of a machine learning algorithm represents the number of training-samples that it needs in order to successfully learn a target function.,"More precisely, the sample complexity is the number of training-samples that we need to supply to the algorithm, so that the function returned by the algorithm is within an arbitrarily small error of the best possible function, with probability arbitrarily close to 1.",There are two variants of sample complexity:,"The No Free Lunch theorem, discussed below, proves that, in general, the strong sample complexity is infinite. I.e, there is no algorithm that can learn the globally-optimal target function using a finite number of training samples.","However, if we are only interested in a particular class of target functions (e.g, only linear functions) then the sample complexity is finite, and it depends linearly on the VC dimension on the class of target functions. [1]",,,"Let 



X


{\displaystyle X}

 be a space which we call the input space, and 



Y


{\displaystyle Y}

 be a space which we call the output space, and let 



Z


{\displaystyle Z}

 denote the product 



X
×
Y


{\displaystyle X\times Y}

. For example, in the setting of binary classification, 



X


{\displaystyle X}

 is typically a finite-dimensional vector space and 



Y


{\displaystyle Y}

 is the set 



{
−
1
,
1
}


{\displaystyle \{-1,1\}}

.","Fix a hypothesis space 





H




{\displaystyle {\mathcal {H}}}

 of functions 



h
:
X
→
Y


{\displaystyle h\colon X\to Y}

. A learning algorithm over 





H




{\displaystyle {\mathcal {H}}}

 is a computable map from 




Z

∗




{\displaystyle Z^{*}}

 to 





H




{\displaystyle {\mathcal {H}}}

. In other words, it is an algorithm that takes as input a finite sequence of training samples and outputs a function from 



X


{\displaystyle X}

 to 



Y


{\displaystyle Y}

. Typical learning algorithms include empirical risk minimization, without or with Tikhonov regularization.","Fix a loss function 



L
o
s
s
:
Y
×
Y
→


R


≥
0




{\displaystyle Loss\colon Y\times Y\to \mathbb {R} _{\geq 0}}

, for example, the square loss 



L
o
s
s
(
y
,

y
′

)
=
(
y
−

y
′


)

2




{\displaystyle Loss(y,y')=(y-y')^{2}}

. For a given distribution 



ρ


{\displaystyle \rho }

 on 



X
×
Y


{\displaystyle X\times Y}

, the expected risk of a hypothesis (a function) 



h
∈


H




{\displaystyle h\in {\mathcal {H}}}

 is","In our setting, we have 



h
=
A
L
G
(

S

n


)


{\displaystyle h=ALG(S_{n})}

 where 



A
L
G


{\displaystyle ALG}

 is a learning algorithm and 




S

n


=
(
(

x

1


,

y

1


)
,
…
,
(

x

n


,

y

n


)
)
∼

ρ

n




{\displaystyle S_{n}=((x_{1},y_{1}),\ldots ,(x_{n},y_{n}))\sim \rho ^{n}}

 is a sequence of vectors which are all drawn independently from 



ρ


{\displaystyle \rho }

. Define the optimal risk






E




H



∗


=


inf

h
∈


H







E


(
h
)
.


{\displaystyle {\mathcal {E}}_{\mathcal {H}}^{*}={\underset {h\in {\mathcal {H}}}{\inf }}{\mathcal {E}}(h).}

Set 




h

n


=
A
l
g
(

S

n


)


{\displaystyle h_{n}=Alg(S_{n})}

 for each 



n


{\displaystyle n}

. Note that 




h

n




{\displaystyle h_{n}}

 is a random variable and depends on the random variable 




S

n




{\displaystyle S_{n}}

, which is drawn from the distribution 




ρ

n




{\displaystyle \rho ^{n}}

. The algorithm 



A
L
G


{\displaystyle ALG}

 is called consistent if 





E


(

h

n


)


{\displaystyle {\mathcal {E}}(h_{n})}

 probabilistically converges to 






E




H



∗




{\displaystyle {\mathcal {E}}_{\mathcal {H}}^{*}}

, in other words, for all ε, δ > 0, there exists a positive integer N such that for all n ≥ N, we have




Pr


ρ

n




[


E


(

h

n


)
−



E




H



∗


≥
ε
]
<
δ
.


{\displaystyle \Pr _{\rho ^{n}}[{\mathcal {E}}(h_{n})-{\mathcal {E}}_{\mathcal {H}}^{*}\geq \varepsilon ]<\delta .}

The sample complexity of 



A
L
G


{\displaystyle ALG}

 is then the minimum N for which this holds, as a function of ρ, ε, and δ. We write the sample complexity as 



N
(
ρ
,
ϵ
,
δ
)


{\displaystyle N(\rho ,\epsilon ,\delta )}

 to emphasize that this value of N depends on ρ, ε, and δ. If 



A
L
G


{\displaystyle ALG}

 is not consistent, then we set 



N
(
ρ
,
ϵ
,
δ
)
=
∞


{\displaystyle N(\rho ,\epsilon ,\delta )=\infty }

. If there exists an algorithm for which 



N
(
ρ
,
ϵ
,
δ
)


{\displaystyle N(\rho ,\epsilon ,\delta )}

 is finite, then we say that the hypothesis space 





H




{\displaystyle {\mathcal {H}}}

 is learnable.","In words, the sample complexity 



N
(
ρ
,
ϵ
,
δ
)


{\displaystyle N(\rho ,\epsilon ,\delta )}

 defines the rate of consistency of the algorithm: given a desired accuracy ε and confidence δ, one needs to sample 



N
(
ρ
,
ϵ
,
δ
)


{\displaystyle N(\rho ,\epsilon ,\delta )}

 data points to guarantee that the risk of the output function is within ε of the best possible, with probability at least 1 - δ.[2]","In probabilistically approximately correct (PAC) learning, one is concerned with whether the sample complexity is polynomial, that is, whether 



N
(
ρ
,
ϵ
,
δ
)


{\displaystyle N(\rho ,\epsilon ,\delta )}

 is bounded by a polynomial in 1/ε and 1/δ. If 



N
(
ρ
,
ϵ
,
δ
)


{\displaystyle N(\rho ,\epsilon ,\delta )}

 is polynomial for some learning algorithm, then one says that the hypothesis space 





H




{\displaystyle {\mathcal {H}}}

 is PAC-learnable. Note that this is a stronger notion than being learnable."," One can ask whether there exists a learning algorithm so that the sample complexity is finite in the strong sense, that is, there is a bound on the number of samples needed so that the algorithm can learn any distribution over the input-output space with a specified target error. More formally, one asks whether there exists a learning algorithm 



A
L
G


{\displaystyle ALG}

 such that, for all ε, δ > 0, there exists a positive integer N such that for all n ≥ N, we have




sup

ρ



(

Pr


ρ

n




[


E


(

h

n


)
−



E




H



∗


≥
ε
]
)

<
δ
,


{\displaystyle \sup _{\rho }\left(\Pr _{\rho ^{n}}[{\mathcal {E}}(h_{n})-{\mathcal {E}}_{\mathcal {H}}^{*}\geq \varepsilon ]\right)<\delta ,}

where 




h

n


=
A
L
G
(

S

n


)


{\displaystyle h_{n}=ALG(S_{n})}

, with 




S

n


=
(
(

x

1


,

y

1


)
,
…
,
(

x

n


,

y

n


)
)
∼

ρ

n




{\displaystyle S_{n}=((x_{1},y_{1}),\ldots ,(x_{n},y_{n}))\sim \rho ^{n}}

 as above. The No Free Lunch Theorem says that without restrictions on the hypothesis space 





H




{\displaystyle {\mathcal {H}}}

, this is not the case, i.e., there always exist ""bad"" distributions for which the sample complexity is arbitrarily large.[1]","Thus, in order to make statements about the rate of convergence of the quantity 




sup

ρ



(

Pr


ρ

n




[


E


(

f

n


)
−



E




H



∗


≥
ε
]
)

,


{\displaystyle \sup _{\rho }\left(\Pr _{\rho ^{n}}[{\mathcal {E}}(f_{n})-{\mathcal {E}}_{\mathcal {H}}^{*}\geq \varepsilon ]\right),}

 one must either","The latter approach leads to concepts such as VC dimension and Rademacher complexity which control the complexity of the space 





H




{\displaystyle {\mathcal {H}}}

. A smaller hypothesis space introduces more bias into the inference process, meaning that 






E




H



∗




{\displaystyle {\mathcal {E}}_{\mathcal {H}}^{*}}

 may be greater than the best possible risk in a larger space. However, by restricting the complexity of the hypothesis space it becomes possible for an algorithm to produce more uniformly consistent functions. This trade-off leads to the concept of regularization.[2]","It is a theorem from VC theory that the following three statements are equivalent for a hypothesis space 





H




{\displaystyle {\mathcal {H}}}

:","This gives a way to prove that certain hypothesis spaces are PAC learnable, and by extension, learnable.","Let X = Rd, Y = {-1, 1}, and let 





H




{\displaystyle {\mathcal {H}}}

 be the space of affine functions on X, that is, functions of the form 



x
↦
⟨
w
,
x
⟩
+
b


{\displaystyle x\mapsto \langle w,x\rangle +b}

 for some 



w
∈


R


d


,
b
∈

R



{\displaystyle w\in \mathbb {R} ^{d},b\in \mathbb {R} }

. This is the linear classification with offset learning problem. Now, note that four coplanar points in a square cannot be shattered by any affine function, since no affine function can be positive on two diagonally opposite vertices and negative on the remaining two. Thus, the VC dimension of 





H




{\displaystyle {\mathcal {H}}}

 is 3, in particular finite. It follows by the above characterization of PAC-learnable classes that 





H




{\displaystyle {\mathcal {H}}}

 is PAC-learnable, and by extension, learnable."," Suppose 





H




{\displaystyle {\mathcal {H}}}

 is a class of binary functions (functions to {0,1}). Then, 





H




{\displaystyle {\mathcal {H}}}

 is 



(
ϵ
,
δ
)


{\displaystyle (\epsilon ,\delta )}

-PAC-learnable with a sample of size: [3] 



N
=
O


(





V
C
(


H


)
+
ln
⁡


1
δ



ϵ




)




{\displaystyle N=O{\bigg (}{\frac {VC({\mathcal {H}})+\ln {1 \over \delta }}{\epsilon }}{\bigg )}}

 where 



V
C
(


H


)


{\displaystyle VC({\mathcal {H}})}

 is the VC dimension of 





H




{\displaystyle {\mathcal {H}}}

. Moreover, any 



(
ϵ
,
δ
)


{\displaystyle (\epsilon ,\delta )}

-PAC-learning algorithm for 





H




{\displaystyle {\mathcal {H}}}

 must have sample-complexity:[4] 



N
=
Ω


(





V
C
(


H


)
+
ln
⁡


1
δ



ϵ




)




{\displaystyle N=\Omega {\bigg (}{\frac {VC({\mathcal {H}})+\ln {1 \over \delta }}{\epsilon }}{\bigg )}}

 Thus, the sample-complexity is a linear function of the VC dimension of the hypothesis space.","Suppose 





H




{\displaystyle {\mathcal {H}}}

 is a class of real-valued functions with range in [0,T]. Then, 





H




{\displaystyle {\mathcal {H}}}

 is 



(
ϵ
,
δ
)


{\displaystyle (\epsilon ,\delta )}

-PAC-learnable with a sample of size: [5][6] 



N
=
O


(



T

2





P
D
(


H


)
ln
⁡


T
ϵ


+
ln
⁡


1
δ




ϵ

2






)




{\displaystyle N=O{\bigg (}T^{2}{\frac {PD({\mathcal {H}})\ln {T \over \epsilon }+\ln {1 \over \delta }}{\epsilon ^{2}}}{\bigg )}}

 where 



P
D
(


H


)


{\displaystyle PD({\mathcal {H}})}

 is Pollard's pseudo-dimension of 





H




{\displaystyle {\mathcal {H}}}

.","In addition to the supervised learning setting, sample complexity is relevant to semi-supervised learning problems including active learning,[7] where the algorithm can ask for labels to specifically chosen inputs in order to reduce the cost of obtaining many labels. The concept of sample complexity also shows up in reinforcement learning,[8] online learning, and unsupervised algorithms, e.g. for dictionary learning.[9]"
,,"Savi Technology was founded in 1989 and is based in Alexandria, Virginia.",Savi provides the most complete Sensor Analytics solutions for organizations that face critical decisions based on the location and status of their assets.,"Savi Technology offers sensor analytics solutions for logistics and supply chain operations. It tracks shipment locations in real time and applies analytics to accurately predict arrival of goods. The company provides Savi Insight, a solution that offers predictive and prescriptive supply chain analytics to forecast future outcomes, prevent operational disruptions, and reduce risk; Savi Tracking, a solution that monitors and provides operational intelligence for asset tracking, journey management, and electronic cargo tracking assets in motion; ETAaaS, a SaaS analytics solution that processes multiple real-time data sources, enterprise resource planning (ERP), and historical information; and Savi Now, a mobile application for tracking and tracing high-value assets. It also offers tags that enable organizations to access real-time information on the location, condition, and security status of assets and shipments; fixed and mobile readers; radio-frequency identification devices and sensors; and portable deployment kits. In addition, the company provides professional services, including program management, systems integration, system and network design, support, and hosting. It serves the U.S. Department of Defense, the U.S. and allied militaries, civilian governmental organizations, and commercial companies, as well as transportation, pharmaceuticals, retail, life sciences, and manufacturing industries worldwide.",Savi Company Website LinkedIn Savi Technology Overview
"In machine learning, semantic analysis of a corpus is the task of building structures that approximate concepts from a large set of documents. It generally does not involve prior semantic understanding of the documents.","Latent semantic analysis (sometimes latent semantic indexing), is a class of techniques where documents are represented as vectors in term space. A prominent example is PLSI.",Latent Dirichlet allocation involves attributing document terms to topics.,n-grams and hidden Markov models work by representing the term stream as a markov chain where each term is derived from the few terms before it.
Semantic folding theory describes a procedure for encoding the semantics of natural language text in a semantically grounded binary representation. This approach provides a framework for modelling how language data is processed by the neocortex.[1],,,"Semantic folding theory draws inspiration from Douglas R. Hofstadter's Analogy as the Core of Cognition which suggests that the brain makes sense of the world by identifying and applying analogies.[2] The theory hypothesises that semantic data must therefore be introduced to the neocortex in such a form as to allow the application of a similarity measure and offers, as a solution, the sparse binary vector employing a two-dimensional topographic semantic space as a distributional reference frame. The theory builds on the computational theory of the human cortex known as hierarchical temporal memory (HTM), and positions itself as a complementary theory for the representation of language semantics.",A particular strength claimed by this approach is that the resulting binary representation enables complex semantic operations to be performed simply and efficiently at the most basic computational level.,"Analogous to the structure of the neocortex, Semantic Folding theory posits the implementation of a semantic space as a two-dimensional grid. This grid is populated by context-vectors[note 1] in such a way as to place similar context-vectors closer to each other, for instance, by using competitive learning principles. This vector space model is presented in the theory as an equivalence to the well known word space model[3] described in the Information Retrieval literature.",Given a semantic space (implemented as described above) a word-vector[note 2] can be obtained for any given word Y by employing the following algorithm:,For each position X in the semantic map (where X represents cartesian coordinates),"The result of this process will be a word-vector containing all the contexts in which the word Y appears and will therefore be representative of the semantics of that word in the semantic space. It can be seen that the resulting word-vector is also in a sparse distributed representation (SDR) format [Schütze, 1993] & [Sahlgreen, 2006].[3][4] Some properties of word-SDRs that are of particular interest with respect to computational semantics are:[5]",Semantic spaces[note 3][6] in the natural language domain aim to create representations of natural language that are capable of capturing meaning. The original motivation for semantic spaces stems from two core challenges of natural language: Vocabulary mismatch (the fact that the same meaning can be expressed in many ways) and ambiguity of natural language (the fact that the same term can have several meanings).,"The application of semantic spaces in natural language processing (NLP) aims at overcoming limitations of rule-based or model-based approaches operating on the keyword level. The main drawback with these approaches is their brittleness, and the large manual effort required to create either rule-based NLP systems or training corpora for model learning.[7][8] Rule-based and machine learning based models are fixed on the keyword level and break down if the vocabulary differs from that defined in the rules or from the training material used for the statistical models.","Research in semantic spaces dates back more than 20 years. In 1996, two papers were published that raised a lot of attention around the general idea of creating semantic spaces: latent semantic analysis[9] from Microsoft and Hyperspace Analogue to Language[10] from the University of California. However, their adoption was limited by the large computational effort required to construct and use those semantic spaces. A breakthrough with regard to the accuracy of modelling associative relations between words (e.g. ""spider-web"", ""lighter-cigarette"", as opposed to synonymous relations such as ""whale-dolphin"", ""astronaut-driver"") was achieved by explicit semantic analysis (ESA)[11] in 2007. ESA was a novel (non-machine learning) based approach that represented words in the form of vectors with 100,000 dimensions (where each dimension represents an Article in Wikipedia). However practical applications of the approach are limited due to the large number of required dimensions in the vectors.","More recently, advances in neural networking techniques in combination with other new approaches (tensors) led to a host of new recent developments: Word2vec[12] from Google and GloVe[13] from Stanford University.","Semantic folding represents a novel, biologically inspired approach to semantic spaces where each word is represented as a sparse binary vector with 16,000 dimensions (a semantic fingerprint) in a 2D semantic map (the semantic universe). Sparse binary representation are advantageous in terms of computational efficiency, and allow for the storage of very large numbers of possible patterns.[5]"
"Semi-supervised learning is a class of supervised learning tasks and techniques that also make use of unlabeled data for training – typically a small amount of labeled data with a large amount of unlabeled data. Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render a fully labeled training set infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.","As in the supervised learning framework, we are given a set of 



l


{\displaystyle l}

 independently identically distributed examples 




x

1


,
…
,

x

l


∈
X


{\displaystyle x_{1},\dots ,x_{l}\in X}

 with corresponding labels 




y

1


,
…
,

y

l


∈
Y


{\displaystyle y_{1},\dots ,y_{l}\in Y}

. Additionally, we are given 



u


{\displaystyle u}

 unlabeled examples 




x

l
+
1


,
…
,

x

l
+
u


∈
X


{\displaystyle x_{l+1},\dots ,x_{l+u}\in X}

. Semi-supervised learning attempts to make use of this combined information to surpass the classification performance that could be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.","Semi-supervised learning may refer to either transductive learning or inductive learning. The goal of transductive learning is to infer the correct labels for the given unlabeled data 




x

l
+
1


,
…
,

x

l
+
u




{\displaystyle x_{l+1},\dots ,x_{l+u}}

 only. The goal of inductive learning is to infer the correct mapping from 



X


{\displaystyle X}

 to 



Y


{\displaystyle Y}

.","Intuitively, we can think of the learning problem as an exam and labeled data as the few example problems that the teacher solved in class. The teacher also provides a set of unsolved problems. In the transductive setting, these unsolved problems are a take-home exam and you want to do well on them in particular. In the inductive setting, these are practice problems of the sort you will encounter on the in-class exam.","It is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.",,,"In order to make any use of unlabeled data, we must assume some structure to the underlying distribution of data. Semi-supervised learning algorithms make use of at least one of the following assumptions. [1]","Points which are close to each other are more likely to share a label. This is also generally assumed in supervised learning and yields a preference for geometrically simple decision boundaries. In the case of semi-supervised learning, the smoothness assumption additionally yields a preference for decision boundaries in low-density regions, so that there are fewer points close to each other but in different classes.","The data tend to form discrete clusters, and points in the same cluster are more likely to share a label (although data sharing a label may be spread across multiple clusters). This is a special case of the smoothness assumption and gives rise to feature learning with clustering algorithms.",The data lie approximately on a manifold of much lower dimension than the input space. In this case we can attempt to learn the manifold using both the labeled and unlabeled data to avoid the curse of dimensionality. Then learning can proceed using distances and densities defined on the manifold.,"The manifold assumption is practical when high-dimensional data are being generated by some process that may be hard to model directly, but which only has a few degrees of freedom. For instance, human voice is controlled by a few vocal folds,[2] and images of various facial expressions are controlled by a few muscles. We would like in these cases to use distances and smoothness in the natural space of the generating problem, rather than in the space of all possible acoustic waves or images respectively.","The heuristic approach of self-training (also known as self-learning or self-labeling) is historically the oldest approach to semi-supervised learning,[1] with examples of applications starting in the 1960s (see for instance Scudder (1965)[3]).",The transductive learning framework was formally introduced by Vladimir Vapnik in the 1970s.[4] Interest in inductive learning using generative models also began in the 1970s. A probably approximately correct learning bound for semi-supervised learning of a Gaussian mixture was demonstrated by Ratsaby and Venkatesh in 1995 [5],"Semi-supervised learning has recently become more popular and practically relevant due to the variety of problems for which vast quantities of unlabeled data are available—e.g. text on websites, protein sequences, or images. For a review of recent work see a survey article by Zhu (2008).[6]","Generative approaches to statistical learning first seek to estimate 



p
(
x

|

y
)


{\displaystyle p(x|y)}

, the distribution of data points belonging to each class. The probability 



p
(
y

|

x
)


{\displaystyle p(y|x)}

 that a given point 



x


{\displaystyle x}

 has label 



y


{\displaystyle y}

 is then proportional to 



p
(
x

|

y
)
p
(
y
)


{\displaystyle p(x|y)p(y)}

 by Bayes' rule. Semi-supervised learning with generative models can be viewed either as an extension of supervised learning (classification plus information about 



p
(
x
)


{\displaystyle p(x)}

) or as an extension of unsupervised learning (clustering plus some labels).","Generative models assume that the distributions take some particular form 



p
(
x

|

y
,
θ
)


{\displaystyle p(x|y,\theta )}

 parameterized by the vector 



θ


{\displaystyle \theta }

. If these assumptions are incorrect, the unlabeled data may actually decrease the accuracy of the solution relative to what would have been obtained from labeled data alone. [7] However, if the assumptions are correct, then the unlabeled data necessarily improves performance.[5]","The unlabeled data are distributed according to a mixture of individual-class distributions. In order to learn the mixture distribution from the unlabeled data, it must be identifiable, that is, different parameters must yield different summed distributions. Gaussian mixture distributions are identifiable and commonly used for generative models.","The parameterized joint distribution can be written as 



p
(
x
,
y

|

θ
)
=
p
(
y

|

θ
)
p
(
x

|

y
,
θ
)


{\displaystyle p(x,y|\theta )=p(y|\theta )p(x|y,\theta )}

 by using the Chain rule. Each parameter vector 



θ


{\displaystyle \theta }

 is associated with a decision function 




f

θ


(
x
)
=


argmax
y


 
p
(
y

|

x
,
θ
)


{\displaystyle f_{\theta }(x)={\underset {y}{\operatorname {argmax} }}\ p(y|x,\theta )}

. The parameter is then chosen based on fit to both the labeled and unlabeled data, weighted by 



λ


{\displaystyle \lambda }

:",[8],"Another major class of methods attempts to place boundaries in regions where there are few data points (labeled or unlabeled). One of the most commonly used algorithms is the transductive support vector machine, or TSVM (which, despite its name, may be used for inductive learning as well). Whereas support vector machines for supervised learning seek a decision boundary with maximal margin over the labeled data, the goal of TSVM is a labeling of the unlabeled data such that the decision boundary has maximal margin over all of the data. In addition to the standard hinge loss 



(
1
−
y
f
(
x
)

)

+




{\displaystyle (1-yf(x))_{+}}

 for labeled data, a loss function 



(
1
−

|

f
(
x
)

|


)

+




{\displaystyle (1-|f(x)|)_{+}}

 is introduced over the unlabeled data by letting 



y
=
sign
⁡

f
(
x
)



{\displaystyle y=\operatorname {sign} {f(x)}}

. TSVM then selects 




f

∗


(
x
)
=

h

∗


(
x
)
+
b


{\displaystyle f^{*}(x)=h^{*}(x)+b}

 from a reproducing kernel Hilbert space 





H




{\displaystyle {\mathcal {H}}}

 by minimizing the regularized empirical risk:","An exact solution is intractable due to the non-convex term 



(
1
−

|

f
(
x
)

|


)

+




{\displaystyle (1-|f(x)|)_{+}}

, so research has focused on finding useful approximations.[8]","Other approaches that implement low-density separation include Gaussian process models, information regularization, and entropy minimization (of which TSVM is a special case).","Graph-based methods for semi-supervised learning use a graph representation of the data, with a node for each labeled and unlabeled example. The graph may be constructed using domain knowledge or similarity of examples; two common methods are to connect each data point to its 



k


{\displaystyle k}

 nearest neighbors or to examples within some distance 



ϵ


{\displaystyle \epsilon }

. The weight 




W

i
j




{\displaystyle W_{ij}}

 of an edge between 




x

i




{\displaystyle x_{i}}

 and 




x

j




{\displaystyle x_{j}}

 is then set to 




e



−

|


|


x

i


−

x

j



|



|


2



ϵ





{\displaystyle e^{\frac {-||x_{i}-x_{j}||^{2}}{\epsilon }}}

.","Within the framework of manifold regularization, [9] [10] the graph serves as a proxy for the manifold. A term is added to the standard Tikhonov regularization problem to enforce smoothness of the solution relative to the manifold (in the intrinsic space of the problem) as well as relative to the ambient input space. The minimization problem becomes","where 





H




{\displaystyle {\mathcal {H}}}

 is a reproducing kernel Hilbert space and 





M




{\displaystyle {\mathcal {M}}}

 is the manifold on which the data lie. The regularization parameters 




λ

A




{\displaystyle \lambda _{A}}

 and 




λ

I




{\displaystyle \lambda _{I}}

 control smoothness in the ambient and intrinsic spaces respectively. The graph is used to approximate the intrinsic regularization term. Defining the graph Laplacian 



L
=
D
−
W


{\displaystyle L=D-W}

 where 




D

i
i


=

∑

j
=
1


l
+
u



W

i
j




{\displaystyle D_{ii}=\sum _{j=1}^{l+u}W_{ij}}

 and 




f



{\displaystyle \mathbf {f} }

 the vector 



[
f
(

x

1


)
…
f
(

x

l
+
u


)
]


{\displaystyle [f(x_{1})\dots f(x_{l+u})]}

, we have",The Laplacian can also be used to extend the supervised learning algorithms： regularized least squares and support vector machines (SVM) to semi-supervised versions Laplacian regularized least squares and Laplacian SVM.,"Some methods for semi-supervised learning are not intrinsically geared to learning from both unlabeled and labeled data, but instead make use of unlabeled data within a supervised learning framework. For instance, the labeled and unlabeled examples 




x

1


,
…
,

x

l
+
u




{\displaystyle x_{1},\dots ,x_{l+u}}

 may inform a choice of representation, distance metric, or kernel for the data in an unsupervised first step. Then supervised learning proceeds from only the labeled examples.",Self-training is a wrapper method for semi-supervised learning.[11] First a supervised learning algorithm is trained based on the labeled data only. This classifier is then applied to the unlabeled data to generate more labeled examples as input for the supervised learning algorithm. Generally only the labels the classifier is most confident of are added at each step.[12],Co-training is an extension of self-training in which multiple classifiers are trained on different (ideally disjoint) sets of features and generate labeled examples for one another.[13],"Human responses to formal semi-supervised learning problems have yielded varying conclusions about the degree of influence of the unlabeled data (for a summary see [14]). More natural learning problems may also be viewed as instances of semi-supervised learning. Much of human concept learning involves a small amount of direct instruction (e.g. parental labeling of objects during childhood) combined with large amounts of unlabeled experience (e.g. observation of objects without naming or counting them, or at least without feedback).","Human infants are sensitive to the structure of unlabeled natural categories such as images of dogs and cats or male and female faces.[15] More recent work has shown that infants and children take into account not only the unlabeled examples available, but the sampling process from which labeled examples arise.[16][17]"
"In machine learning, sequence labeling is a type of pattern recognition task that involves the algorithmic assignment of a categorical label to each member of a sequence of observed values. A common example of a sequence labeling task is part of speech tagging, which seeks to assign a part of speech to each word in an input sentence or document. Sequence labeling can be treated as a set of independent classification tasks, one per member of the sequence. However, accuracy is generally improved by making the optimal label for a given element dependent on the choices of nearby elements, using special algorithms to choose the globally best set of labels for the entire sequence at once.","As an example of why finding the globally best label sequence might produce better results than labeling one item at a time, consider the part-of-speech tagging task just described. Frequently, many words are members of multiple parts of speech, and the correct label of such a word can often be deduced from the correct label of the word to the immediate left or right. For example, the word ""sets"" can be either a noun or verb. In a phrase like ""he sets the books down"", the word ""he"" is unambiguously a pronoun, and ""the"" unambiguously a determiner, and using either of these labels, ""sets"" can be deduced to be a verb, since nouns very rarely follow pronouns and are less likely to precede determiners than verbs are. But in other cases, only one of the adjacent words is similarly helpful. In ""he sets and then knocks over the table"", only the word ""he"" to the left is helpful (cf. ""...picks up the sets and then knocks over...""). Conversely, in ""... and also sets the table"" only the word ""the"" to the right is helpful (cf. ""... and also sets of books were ...""). An algorithm that proceeds from left to right, labeling one word at a time, can only use the tags of left-adjacent words and might fail in the second example above; vice versa for an algorithm that proceeds from right to left.","Most sequence labeling algorithms are probabilistic in nature, relying on statistical inference to find the best sequence. The most common statistical models in use for sequence labeling make a Markov assumption, i.e. that the choice of label for a particular word is directly dependent only on the immediately adjacent labels; hence the set of labels forms a Markov chain. This leads naturally to the hidden Markov model (HMM), one of the most common statistical models used for sequence labeling. Other common models in use are the maximum entropy Markov model and conditional random field.",,,{{Reflist[1]}}
"Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn from examples a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems,[1] visual identity tracking,[2] face verification[3] and speaker verification.[4]",,,There are four common setups for similarity and metric distance learning.,"A common approach for learning similarity, is to model the similarity function as a bilinear form. For example, in the case of ranking similarity learning, one aims to learn a matrix W that parametrizes the similarity function 




f

W


(
x
,
z
)
=

x

T


W
z


{\displaystyle f_{W}(x,z)=x^{T}Wz}

.","Similarity learning is closely related to distance metric learning. Metric learning is the task of learning a distance function over objects. A metric or distance function has to obey four axioms: non-negativity, Identity of indiscernibles, symmetry and subadditivity / triangle inequality. In practice, metric learning algorithms ignore the condition of identity of indiscernibles and learn a pseudo-metric.","When the objects 




x

i




{\displaystyle x_{i}}

 are vectors in 




R

d




{\displaystyle R^{d}}

, then any matrix 



W


{\displaystyle W}

 in the symmetric positive semi-definite cone 




S

+


d




{\displaystyle S_{+}^{d}}

 defines a distance pseudo-metric of the space of x through the form 




D

W


(

x

1


,

x

2



)

2


=
(

x

1


−

x

2



)

⊤


W
(

x

1


−

x

2


)


{\displaystyle D_{W}(x_{1},x_{2})^{2}=(x_{1}-x_{2})^{\top }W(x_{1}-x_{2})}

. When 



W


{\displaystyle W}

 is a symmetric positive definite matrix, 




D

W




{\displaystyle D_{W}}

 is a metric. Moreover, as any symmetric positive semi-definite matrix 



W
∈

S

+


d




{\displaystyle W\in S_{+}^{d}}

 can be decomposed as 



W
=

L

⊤


L


{\displaystyle W=L^{\top }L}

 where 



L
∈

R

e
×
d




{\displaystyle L\in R^{e\times d}}

 and 



e
≥
r
a
n
k
(
W
)


{\displaystyle e\geq rank(W)}

, the distance function 




D

W




{\displaystyle D_{W}}

 can be rewritten equivalently 




D

W


(

x

1


,

x

2



)

2


=
(

x

1


−

x

2



)

⊤



L

⊤


L
(

x

1


−

x

2


)
=
∥
L
(

x

1


−

x

2


)

∥

2


2




{\displaystyle D_{W}(x_{1},x_{2})^{2}=(x_{1}-x_{2})^{\top }L^{\top }L(x_{1}-x_{2})=\|L(x_{1}-x_{2})\|_{2}^{2}}

. The distance 




D

W


(

x

1


,

x

2



)

2


=
∥

x

1

′

−

x

2

′


∥

2


2




{\displaystyle D_{W}(x_{1},x_{2})^{2}=\|x_{1}'-x_{2}'\|_{2}^{2}}

 corresponds to the Euclidean distance between the projected feature vectors 




x

1

′

=
L

x

1




{\displaystyle x_{1}'=Lx_{1}}

 and 




x

2

′

=
L

x

2




{\displaystyle x_{2}'=Lx_{2}}

. Some well-known approaches for metric learning include Large margin nearest neighbor,[8] Information theoretic metric learning (ITML).[9]","In statistics, the covariance matrix of the data is sometimes used to define a distance metric called Mahalanobis distance.","Similarity learning is used in information retrieval for learning to rank, in face verification or face identification,[10][11] and in recommendation systems. Also, many machine learning approaches rely on some metric. This includes unsupervised learning such as clustering, which groups together close or similar objects. It also includes supervised approaches like K-nearest neighbor algorithm which rely on labels of nearby objects to decide on the label of a new object. Metric learning has been proposed as a preprocessing step for many of these approaches .[12]","Metric and similarity learning naively scale quadraticly with the dimension of the input space, as can easily see when the learned metric has a bilinear form 




f

W


(
x
,
z
)
=

x

T


W
z


{\displaystyle f_{W}(x,z)=x^{T}Wz}

. Scaling to higher dimensions can be achieved by enforcing a sparseness structure over the matrix model, as done with HDSL,[13] and with COMET.[14]",,"For further information on this topic, see the surveys on metric and similarity learning by Bellet et al.[1] and Kulis.[2]"
"Skymind is a machine intelligence company supporting the open source deep learning framework Deeplearning4j and the JVM-based scientific computing library ND4J. The company was founded in 2014 by Adam Gibson and Chris Nicholson, and is headquartered in San Francisco, California. It is privately funded.",,,"Skymind has implemented neural networks such as restricted Boltzmann machines, deep belief networks, deep autoencoders, convolutional networks, recurrent networks (including LSTMs), recursive autoencoders and word2vec.","Skymind's deep neural networks are able to perform dimensionality reduction, classification, regression, collaborative filtering, feature learning and topic modeling. They can be applied to use cases such as machine vision, machine translation, machine transcription, face and voice recognition, time series predictions, business intelligence and econometric analytics.",Deeplearning4j is an open source deep learning library written for Java and the Java Virtual Machine[2][3] and a computing framework with wide support for deep learning algorithms. Its algorithms all include distributed parallel versions that integrate with Hadoop and Spark.[4],"ND4J is a free, open-source extension of the Java programming language operating on the Java Virtual Machine—though it is compatible with both Scala and Clojure.[5] It includes the Scala wrapper ND4S.[6]"
"Ray Solomonoff's theory of universal inductive inference is a theory of prediction based on logical observations, such as predicting the next symbol based upon a given series of symbols. The only assumption that the theory makes is that the environment follows some unknown but computable probability distribution. It is a mathematical formalization of Occam's razor[1][2][3][4][5] and the Principle of Multiple Explanations.[6]",Prediction is done using a completely Bayesian framework. The universal prior is taken over the class of all computable sequences—this is the universal a priori probability distribution; no computable hypothesis will have a zero probability. This means that Bayes rule of causation can be used in predicting the continuation of any particular computable sequence.,,,"The theory is based in philosophical foundations, and was founded by Ray Solomonoff around 1960.[7] It is a mathematically formalized combination of Occam's razor[1][2][3][4][5] and the Principle of Multiple Explanations.[6] All computable theories which perfectly describe previous observations are used to calculate the probability of the next observation, with more weight put on the shorter computable theories. Marcus Hutter's universal artificial intelligence builds upon this to calculate the expected value of an action.","The proof of the ""razor"" is based on the known mathematical properties of a probability distribution over a denumerable set. These properties are relevant because the infinite set of all programs is a denumerable set. The sum S of the probabilities of all programs must be exactly equal to one (as per the definition of probability) thus the probabilities must roughly decrease as we enumerate the infinite set of all programs, otherwise S will be strictly greater than one. To be more precise, for every 



ϵ


{\displaystyle \epsilon }

 > 0, there is some length l such that the probability of all programs longer than l is at most 



ϵ


{\displaystyle \epsilon }

. This does not, however, preclude very long programs from having very high probability.","Fundamental ingredients of the theory are the concepts of algorithmic probability and Kolmogorov complexity. The universal prior probability of any prefix p of a computable sequence x is the sum of the probabilities of all programs (for a universal computer) that compute something starting with p. Given some p and any computable but unknown probability distribution from which x is sampled, the universal prior and Bayes' theorem can be used to predict the yet unseen parts of x in optimal fashion.","Though Solomonoff's inductive inference is not computable, several AIXI-derived algorithms approximate it in order to make it run on a modern computer. The more computing power they are given, the closer their predictions are to the predictions of inductive inference (their mathematical limit is Solomonoff's inductive inference).[8][9][10]","Another direction of inductive inference is based on E. Mark Gold's model of learning in the limit from 1967 and has developed since then more and more models of learning. [11] The general scenario is the following: Given a class S of computable functions, is there a learner (that is, recursive functional) which for any input of the form (f(0),f(1),...,f(n)) outputs a hypothesis (an index e with respect to a previously agreed on acceptable numbering of all computable functions; the indexed function should be consistent with the given values of f). A learner M learns a function f if almost all its hypotheses are the same index e, which generates the function f; M learns S if M learns every f in S. Basic results are that all recursively enumerable classes of functions are learnable while the class REC of all computable functions is not learnable.[citation needed] Many related models have been considered and also the learning of classes of recursively enumerable sets from positive data is a topic studied from Gold's pioneering paper in 1967 onwards. A far reaching extension of the Gold’s approach is developed by Schmidhuber's theory of generalized Kolmogorov complexities,[12] which are kinds of super-recursive algorithms.","The third mathematically based direction of inductive inference makes use of the theory of automata and computation. In this context, the process of inductive inference is performed by an abstract automaton called an inductive Turing machine (Burgin, 2005). Inductive Turing machines represent the next step in the development of computer science providing better models for contemporary computers and computer networks (Burgin, 2001) and forming an important class of super-recursive algorithms as they satisfy all conditions in the definition of algorithm. Namely, each inductive Turing machines is a type of effective method in which a definite list of well-defined instructions for completing a task, when given an initial state, will proceed through a well-defined series of successive states, eventually terminating in an end-state. The difference between an inductive Turing machine and a Turing machine is that to produce the result a Turing machine has to stop, while in some cases an inductive Turing machine can do this without stopping. Kleene called procedures that could run forever without stopping by the name calculation procedure or algorithm (Kleene 1952:137). Kleene also demanded that such an algorithm must eventually exhibit ""some object"" (Kleene 1952:137). This condition is satisfied by inductive Turing machines, as their results are exhibited after a finite number of steps, but inductive Turing machines do not always tell at which step the result has been obtained.","Simple inductive Turing machines are equivalent to other models of computation. More advanced inductive Turing machines are much more powerful. It is proved (Burgin, 2005) that limiting partial recursive functions, trial and error predicates, general Turing machines, and simple inductive Turing machines are equivalent models of computation. However, simple inductive Turing machines and general Turing machines give direct constructions of computing automata, which are thoroughly grounded in physical machines. In contrast, trial and error predicates, limiting recursive functions and limiting partial recursive functions present syntactic systems of symbols with formal rules for their manipulation. Simple inductive Turing machines and general Turing machines are related to limiting partial recursive functions and trial and error predicates as Turing machines are related to partial recursive functions and lambda-calculus.","Note that only simple inductive Turing machines have the same structure (but different functioning semantics of the output mode) as Turing machines. Other types of inductive Turing machines have an essentially more advanced structure due to the structured memory and more powerful instructions. Their utilization for inference and learning allows achieving higher efficiency and better reflects learning of people (Burgin and Klinger, 2004).","Some researchers confuse computations of inductive Turing machines with non-stopping computations or with infinite time computations. First, some of computations of inductive Turing machines halt. As in the case of conventional Turing machines, some halting computations give the result, while others do not give. Second, some non-stopping computations of inductive Turing machines give results, while others do not give. Rules of inductive Turing machines determine when a computation (stopping or non-stopping) gives a result. Namely, an inductive Turing machine produces output from time to time and once this output stops changing, it is considered the result of the computation. It is necessary to know that descriptions of this rule in some papers are incorrect. For instance, Davis (2006: 128) formulates the rule when result is obtained without stopping as ""… once the correct output has been produced any subsequent output will simply repeat this correct result."" Third, in contrast to the widespread misconception, inductive Turing machines give results (when it happens) always after a finite number of steps (in finite time) in contrast to infinite and infinite-time computations. There are two main distinctions between conventional Turing machines and simple inductive Turing machines. The first distinction is that even simple inductive Turing machines can do much more than conventional Turing machines. The second distinction is that a conventional Turing machine always informs (by halting or by coming to a final state) when the result is obtained, while a simple inductive Turing machine in some cases does inform about reaching the result, while in other cases (where the conventional Turing machine is helpless), it does not inform. People have an illusion that a computer always itself informs (by halting or by other means) when the result is obtained. In contrast to this, users themselves have to decide in many cases whether the computed result is what they need or it is necessary to continue computations. Indeed, everyday desktop computer applications like word processors and spreadsheets spend most of their time waiting in event loops, and do not terminate until directed to do so by users.","Evolutionary approach to inductive inference is accomplished by another class of automata called evolutionary inductive Turing machines (Burgin and Eberbach, 2009; 2012). An ‘’’evolutionary inductive Turing machine’’’ is a (possibly infinite) sequence E = {A[t]; t = 1, 2, 3, ... } of inductive Turing machines A[t] each working on generations X[t] which are coded as words in the alphabet of the machines A[t]. The goal is to build a “population” Z satisfying the inference condition. The automaton A[t] called a component, or a level automaton, of E represents (encodes) a one-level evolutionary algorithm that works with input generations X[i] of the population by applying the variation operators v and selection operator s. The first generation X[0] is given as input to E and is processed by the automaton A[1], which generates/produces the first generation X[1] as its transfer output, which goes to the automaton A[2]. For all t = 1, 2, 3, ..., the automaton A[t] receives the generation X[t − 1] as its input from A[t − 1] and then applies the variation operator v and selection operator s, producing the generation X[i + 1] and sending it to A[t + 1] to continue evolution."
Sparse dictionary learning is a representation learning method which aims at finding a sparse representation of the input data (also known as coding) in the form of a linear combination of basis elements as well as those basis elements themselves. These elements are called atoms and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal. This problem setup also allows the dimensionality of representation space to be higher than the one of the input space. The above two properties lead to having seemingly redundant atoms that allow multiple reconstruction ways but also provide an improvement in sparsity and flexibility of the representation.,"One of the key principles of dictionary learning is that the dictionary has to be inferred from the input data. The emergence of sparse dictionary learning methods was stimulated by the fact that in signal processing one typically wants to represent the input data using as few components as possible. Before this approach the general practice was to use predefined dictionaries (such as fourier or wavelet transforms). However, in certain cases the dictionary learned to fit the input data can significantly improve the sparsity and thus the results of signal processing.","This method is applied to data decomposition, compression and analysis and has been used in the fields of image denoising and classification, video and audio processing.",,,"Given the input dataset 



X
=
[

x

1


,
.
.
.
,

x

K


]
,

x

i


∈


R


d




{\displaystyle X=[x_{1},...,x_{K}],x_{i}\in \mathbb {R} ^{d}}

 we wish to find a dictionary 




D

∈


R


d
×
n


:
D
=
[

d

1


,
.
.
.
,

d

n


]


{\displaystyle \mathbf {D} \in \mathbb {R} ^{d\times n}:D=[d_{1},...,d_{n}]}

 and a representation 



R
=
[

r

1


,
.
.
.
,

r

K


]
,

r

i


∈


R


n




{\displaystyle R=[r_{1},...,r_{K}],r_{i}\in \mathbb {R} ^{n}}

 such that both 



∥
X
−

D

R

∥

F


2




{\displaystyle \|X-\mathbf {D} R\|_{F}^{2}}

 is minimized and the representations 




r

i




{\displaystyle r_{i}}

 are sparse enough. This can be formulated as a following optimization problem:","





argmin


D

∈


C


,

r

i


∈


R


n






∑

i
=
1


k


∥

x

i


−

D


r

i



∥

2


2


+
λ
∥

r

i



∥

0




{\displaystyle {\underset {\mathbf {D} \in {\mathcal {C}},r_{i}\in \mathbb {R} ^{n}}{\text{argmin}}}\sum _{i=1}^{k}\|x_{i}-\mathbf {D} r_{i}\|_{2}^{2}+\lambda \|r_{i}\|_{0}}

, where 





C


≡
{

D

∈


R


d
×
n


:
∥

d

i



∥

2


≤
1


∀
i
=
1
,
.
.
.
,
n
}


{\displaystyle {\mathcal {C}}\equiv \{\mathbb {D} \in \mathbb {R} ^{d\times n}:\|d_{i}\|_{2}\leq 1\,\,\forall i=1,...,n\}}

","





C




{\displaystyle {\mathcal {C}}}

 is required to constrain 




D



{\displaystyle \mathbf {D} }

 so that its atoms would not reach arbitrarily high values allowing for arbitrarily low (but non-zero) values of 




r

i




{\displaystyle r_{i}}

.","The minimization problem above is not convex because of the ℓ0-""norm"" and solving this problem is NP-hard.[1] In some cases L1-norm is known to ensure sparsity[2] and so the above becomes a convex optimization problem with respect to each of the variables 




D



{\displaystyle \mathbf {D} }

 and 



R


{\displaystyle R}

 in case the other one of them is currently fixed.","The dictionary 




D



{\displaystyle \mathbf {D} }

 defined above can be ""undercomplete"" if 



n
<
d


{\displaystyle n<d}

 or ""overcomplete"" in case 



n
>
d


{\displaystyle n>d}

 with the latter being a typical assumption for a sparse dictionary learning problem. The case of a complete dictionary does not provide any improvement from a representational point of view and thus isn't considered.","Undercomplete dictionaries represent the setup in which the actual input data lies in a lower-dimensional space. This case is strongly related to dimensionality reduction and techniques like principal component analysis which require atoms 




d

1


,
.
.
.
,

d

n




{\displaystyle d_{1},...,d_{n}}

 to be orthogonal. Orthogonal dictionaries are appealing from a computational point of view because they allow to compute the representation coefficients by calculating a scalar product between the input data and the atoms, though their main downside is limiting the choice of atoms.","Overcomplete dictionaries, however, do not require the atoms to be orthogonal thus allowing for more flexible dictionaries and richer data representations.","As the optimization problem described above can be solved with respect to either dictionary or sparse coding while the other one of the two is fixed, most of the algorithms are based on the idea of iteratively updating one and the other.","The problem of finding an optimal sparse coding 



R


{\displaystyle R}

 with a given dictionary 




D



{\displaystyle \mathbf {D} }

 is known as sparse approximation (or sometimes just sparse coding problem). There has been developed a number of algorithms to solve it (such as orthogonal matching pursuit and LASSO) which are incorporated into the algorithms described below.",The method of optimal directions (or MOD) was one of the first methods introduced to tackle the sparse dictionary learning problem.[3] The core idea of it is to solve the minimization problem subject to the limited number of non-zero components of the representation vector:,"




min


D

,
R


{
∥
X
−

D

R

∥

F


2


}



s.t.



∀
i


∥

r

i



∥

0


≤
T


{\displaystyle \min _{\mathbf {D} ,R}\{\|X-\mathbf {D} R\|_{F}^{2}\}\,\,{\text{s.t.}}\,\,\forall i\,\,\|r_{i}\|_{0}\leq T}

","MOD alternates between getting the sparse coding using one of the methods mentioned above and updating the dictionary by computing the analytical solution of the problem given by 




D

=
X

R

+




{\displaystyle \mathbf {D} =XR^{+}}

 where 




R

+




{\displaystyle R^{+}}

 is a Moore-Penrose pseudoinverse. After this update 




D



{\displaystyle \mathbf {D} }

 is renormalized to fit the constraints and the new sparse coding is obtained again. The process is repeated until convergence.","MOD has proved to be a very efficient method for low-dimensional input data 



X


{\displaystyle X}

 requiring just a few iterations to converge. However, due to the high complexity of the inversion operation, computing the pseudoinverse in high-dimensional cases is in many cases intractable. This shortcoming has inspired the development of other dictionary learning methods.","K-SVD is an algorithm that performs SVD at its core to update the atoms of the dictionary one by one and basically is a generalization of K-means. It enforces that each element of the input data 




x

i




{\displaystyle x_{i}}

 is encoded by a linear combination of not more than 




T

0




{\displaystyle T_{0}}

 elements in a way identical to the MOD approach:","




min


D

,
R


{
∥
X
−

D

R

∥

F


2


}



s.t.



∀
i


∥

r

i



∥

0


≤

T

0




{\displaystyle \min _{\mathbf {D} ,R}\{\|X-\mathbf {D} R\|_{F}^{2}\}\,\,{\text{s.t.}}\,\,\forall i\,\,\|r_{i}\|_{0}\leq T_{0}}

","This algorithm's essence is to first fix the dictionary, find the best possible 



R


{\displaystyle R}

 under the above constraint (using OMP) and then iteratively update the atoms of dictionary 




D



{\displaystyle \mathbf {D} }

 in the following manner:","



∥
X
−

D

R

∥

F


2


=


|
X
−

∑

i
=
1


K



d

i



x

T


i


|


F


2


=
∥

E

k


−

d

k



x

T


k



∥

F


2




{\displaystyle \|X-\mathbf {D} R\|_{F}^{2}=\left|X-\sum _{i=1}^{K}d_{i}x_{T}^{i}\right|_{F}^{2}=\|E_{k}-d_{k}x_{T}^{k}\|_{F}^{2}}

","Next steps of the algorithm include rank-1 approximation of the residual matrix 




E

k




{\displaystyle E_{k}}

, updating 




d

k




{\displaystyle d_{k}}

 and enforcing the sparsity of 




x

k




{\displaystyle x_{k}}

 after the update. This algorithm is considered to be standard for dictionary learning and is used in a variety of applications. However, it shares weaknesses with MOD being efficient only for signals with relatively low dimensionality and having the possibility for a solution to be stuck at local minima.","One can also apply a widespread stochastic gradient descent method with iterative projection to solve this problem.[4][5] The idea of this method is to update the dictionary using the first order stochastic gradient and project it on the constraint set 





C




{\displaystyle {\mathcal {C}}}

. The step that occurs at i-th iteration is described by this expression:","





D


i


=


proj



C



{


D


i
−
1


−

δ

i



∇


D




∑

i
∈
S


∥

x

i


−

D


r

i



∥

2


2


+
λ
∥

r

i



∥

1


}


{\displaystyle \mathbf {D} _{i}={\text{proj}}_{\mathcal {C}}\{\mathbf {D} _{i-1}-\delta _{i}\nabla _{\mathbf {D} }\sum _{i\in S}\|x_{i}-\mathbf {D} r_{i}\|_{2}^{2}+\lambda \|r_{i}\|_{1}\}}

, where 



S


{\displaystyle S}

 is a random subset of 



{
1...
K
}


{\displaystyle \{1...K\}}

 and 




δ

i




{\displaystyle \delta _{i}}

 is a gradient step.",An algorithm based on solving a dual Lagrangian problem provides an efficient way to solve for the dictionary having no complications induced by the sparsity function.[6] Consider the following Lagrangian:,"





L


(

D

,
Λ
)
=

trace


(
(
X
−

D

R

)

T


(
X
−

D

R
)
)

+

∑

j
=
1


n



λ

i


(


∑

i
=
1


d




D


i
j


2


−
c

)


{\displaystyle {\mathcal {L}}(\mathbf {D} ,\Lambda )={\text{trace}}\left((X-\mathbf {D} R)^{T}(X-\mathbf {D} R)\right)+\sum _{j=1}^{n}\lambda _{i}({\sum _{i=1}^{d}\mathbf {D} _{ij}^{2}-c})}

, where 



c


{\displaystyle c}

 is a constraint on the norm of the atoms and 




λ

i




{\displaystyle \lambda _{i}}

 are the so-called dual variables forming the diagonal matrix 



Λ


{\displaystyle \Lambda }

.","We can then provide an analytical expression for the Lagrange dual after minimization over 




D



{\displaystyle \mathbf {D} }

:","





D


(
Λ
)
=

min


D





L


(

D

,
Λ
)
=

trace

(

X

T


X
−
X

R

T


(
R

R

T


+
Λ

)

−
1


(
X

R

T



)

T


−
c
Λ
)


{\displaystyle {\mathcal {D}}(\Lambda )=\min _{\mathbf {D} }{\mathcal {L}}(\mathbf {D} ,\Lambda )={\text{trace}}(X^{T}X-XR^{T}(RR^{T}+\Lambda )^{-1}(XR^{T})^{T}-c\Lambda )}

.","After applying one of the optimization methods to the value of the dual (such as Newton's method or conjugate gradient) we get the value of 




D



{\displaystyle \mathbf {D} }

:","





D


T


=
(
R

R

T


+
Λ

)

−
1


(
X

R

T



)

T




{\displaystyle \mathbf {D} ^{T}=(RR^{T}+\Lambda )^{-1}(XR^{T})^{T}}

","Solving this problem is less computational hard because the amount of dual variables 



n


{\displaystyle n}

 is a lot of times much less than the amount of variables in the primal problem.",Parametric training methods are aimed to incorporate the best of both worlds — the realm of analytically constructed dictionaries and the learned ones.[7] This allows to construct more powerful generalized dictionaries that can potentially be applied to the cases of arbitrary-sized signals. Notable approaches include:,"Many common approaches to sparse dictionary learning rely on the fact that the whole input data 



X


{\displaystyle X}

 is available for the algorithm. However, this might not be the case in the real-world scenario as the size of the input data might be too big to fit it into memory. The other case where this assumption can not be made is when the input data comes in a form of a stream. Such cases lie in the field of study of online learning which essentially suggests iteratively updating the model upon the new data points 



x


{\displaystyle x}

 becoming available.",A dictionary can be learned in an online manner the following way:[11],This method allows us to gradually update the dictionary as new data becomes available for sparse representation learning and helps drastically reduce the amount of memory needed to store the dataset (which often has a huge size).,"The dictionary learning framework, namely the linear decomposition of an input signal using a few basis elements learned from data itself, has led to the state-of-art results in various image and video processing tasks. This technique can be applied to classification problems in a way that if we have built specific dictionaries for each class, the input signal can be classified by finding the dictionary corresponding to the sparsest representation.",It also has properties that are useful for signal denoising since usually one can learn a dictionary to represent the meaningful part of the input signal in a sparse way but the noise in the input will have a much less sparse representation.[12],"Sparse dictionary learning has been successfully applied to various image, video and audio processing tasks as well as to texture synthesis[13] and unsupervised clustering[14]"
Spike-and-slab regression is Bayesian variable selection technique which is particularly useful when the amount of possible predictors is bigger than the amount of observations.[1],"Initially, the idea of the spike-and-slab model was proposed by Mitchell & Beauchamp (1988).[2] The approach was further significantly developed by Madigan & Raftery (1994)[3] and George & McCulloch (1997).[4] The final adjustments to the model were done by Ishwaran & Rao (2005).[5]",,,"Suppose we have P possible predictors in some model. Vector γ has a length equal to P and consists of zeros and ones. This vector indicates whether a particular variable is included in the regression or not. If no specific prior information on initial inclusion probabilities of particular variables is available, a Bernoulli prior distribution is a common default choice.[6] Conditional on a predictor being in the regression, we identify a prior distribution for the model coefficient, which corresponds to that variable (β). A common choice on that step is to use a Normal prior with mean equal to zero and a large variance calculated based on (XT X)−1 matrix (where X is a design matrix of explanatory variables of the model).[7]","A draw of γ from its prior distribution is a list of the variables included in the regression. Conditional on this set of selected variables, we take a draw from the prior distribution of the regression coefficients (if γi = 1 then βi ≠ 0 and if γi = 0 then βi = 0). βγ denotes the subset of β for which γi = 1. In the next step, we calculate a posterior probability distribution for both inclusion and coefficients by applying a standard statistical procedure.[8] All steps of the described algorithm are repeated thousands of times using Markov chain Monte Carlo (MCMC) technique. As a result, we obtain a posterior distribution of γ (variable inclusion in the model), β (regression coefficient values) and the corresponding prediction of y.","The model got its name (spike-and-slab) due to the shape of the two prior distributions. The ""spike"" is the probability of a particular coefficient in the model to be not zero. The ""slab"" is the prior distribution for the regression coefficient values.","An advantage of Bayesian variable selection techniques is that they are able to make use of prior knowledge about the model. In the absence of such knowledge, some reasonable default values can be used: ""For the analyst who prefers simplicity at the cost of some reasonable assumptions, useful prior information can be reduced to an expected model size, an expected R2, and a sample size ν determining the weight given to the guess at R2.""[6] Some researchers suggest the following default values: R2 = 0.5, ν = 0.01, and π = 0.5 (parameter of a prior Bernoulli distribution).[6]","A possible drawback of the Spike-and-Slab model can be its mathematical complexity (in comparison to linear regression). A deep understanding of this model requires sound knowledge in stochastic processes. On the other hand, some modern statistical software (e.g. R-program) have ready-to-use solutions for calculating various Bayesian variable selection models.[9][10][11] In this case, it would be enough for a researcher to know the idea of the method, required model parameters and input variables. The analysis of the model outcomes (distribution of γ, β, and corresponding predictions of y) can be more challenging in comparison to linear regression case. The spike-and-slab model produces inclusion probabilities for each of possible predictors. This can cause difficulties when comparing results to the studies with simple regression (usually only regression coefficients with corresponding statistics are available).","Spike-and-slab regression is a part of Bayesian structural time series model (which is used for feature selection, time series forecasting, nowcasting, inferring causation, and other)."
"Stability, also known as algorithmic stability, is a notion in computational learning theory of how a machine learning algorithm is perturbed by small changes to its inputs. A stable learning algorithm is one for which the prediction does not change much when the training data is modified slightly. For instance, consider a machine learning algorithm that is being trained to recognize handwritten letters of the alphabet, using 1000 examples of handwritten letters and their labels (""A"" to ""Z"") as a training set. One way to modify this training set is to leave out an example, so that only 999 examples of handwritten letters and their labels are available. A stable learning algorithm would produce a similar classifier with both the 1000-element and 999-element training sets.","Stability can be studied for many types of learning problems, from language learning to inverse problems in physics and engineering, as it is a property of the learning process rather than the type of information being learned. The study of stability gained importance in computational learning theory in the 2000s when it was shown to have a connection with generalization. It was shown that for large classes of learning algorithms, notably empirical risk minimization algorithms, certain types of stability ensure good generalization.",,,"A central goal in designing a machine learning system is to guarantee that the learning algorithm will generalize, or perform accurately on new examples after being trained on a finite number of them. In the 1990s, milestones were made in obtaining generalization bounds for supervised learning algorithms. The technique historically used to prove generalization was to show that an algorithm was consistent, using the uniform convergence properties of empirical quantities to their means. This technique was used to obtain generalization bounds for the large class of empirical risk minimization (ERM) algorithms. An ERM algorithm is one that selects a solution from a hypothesis space 



H


{\displaystyle H}

 in such a way to minimize the empirical error on a training set 



S


{\displaystyle S}

.","A general result, proved by Vladimir Vapnik for an ERM binary classification algorithms, is that for any target function and input distribution, any hypothesis space 



H


{\displaystyle H}

 with VC-dimension 



d


{\displaystyle d}

, and 



n


{\displaystyle n}

 training examples, the algorithm is consistent and will produce a training error that is most 



O

(



d
n



)



{\displaystyle O\left({\sqrt {\frac {d}{n}}}\right)}

 (plus logarithmic factors) from the true training error. The result was later extended to almost-ERM algorithms with function classes that do not have unique minimizers.","Vapnik's work, using what became known as VC theory, established a relationship between generalization of a learning algorithm and properties of the hypothesis space 



H


{\displaystyle H}

 of functions being learned. However, these results could not be applied to algorithms with hypothesis spaces of unbounded VC-dimension. Put another way, these results could not be applied when the information being learned had a complexity that was too large to measure. Some of the simplest machine learning algorithms, for instance, for regression have hypothesis spaces with unbounded VC-dimension. Another example is a language learning algorithms that can produce sentences of arbitrary length.","Stability analysis was developed in the 2000s for computational learning theory and is an alternative method for obtaining generalization bounds. The stability of an algorithm is a property of the learning process, rather than a direct property of the hypothesis space 



H


{\displaystyle H}

, and it can be assessed in algorithms that have hypothesis spaces with unbounded or undefined VC-dimension such as nearest neighbor. A stable learning algorithm is one for which the learned function does not change much when the training set is slightly modified, for instance by leaving out an example. A measure of Leave one out error is used in a Cross Validation Leave One Out (CVloo) algorithm to evaluate a learning algorithm's stability with respect to the loss function. As such, stability analysis is the application of sensitivity analysis to machine learning.","We define several terms related to learning algorithms training sets, so that we can then define stability in multiple ways and present theorems from the field.","A machine learning algorithm, also known as a learning map 



L


{\displaystyle L}

, maps a training data set, which is a set of labeled examples 



(
x
,
y
)


{\displaystyle (x,y)}

, onto a function 



f


{\displaystyle f}

 from 



X


{\displaystyle X}

 to 



Y


{\displaystyle Y}

, where 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

 are in the same space of the training examples. The functions 



f


{\displaystyle f}

 are selected from a hypothesis space of functions called 



H


{\displaystyle H}

.",The training set from which an algorithm learns is defined as,"



S
=
{

z

1


=
(

x

1


,
 

y

1


)
 
,
.
.
,
 

z

m


=
(

x

m


,
 

y

m


)
}


{\displaystyle S=\{z_{1}=(x_{1},\ y_{1})\ ,..,\ z_{m}=(x_{m},\ y_{m})\}}

","and is of size 



m


{\displaystyle m}

 in 



Z
=
X
×
Y


{\displaystyle Z=X\times Y}

",drawn i.i.d. from an unknown distribution D.,"Thus, the learning map 



L


{\displaystyle L}

 is defined as a mapping from 




Z

m




{\displaystyle Z_{m}}

 into 



H


{\displaystyle H}

, mapping a training set 



S


{\displaystyle S}

 onto a function 




f

S




{\displaystyle f_{S}}

 from 



X


{\displaystyle X}

 to 



Y


{\displaystyle Y}

. Here, we consider only deterministic algorithms where 



L


{\displaystyle L}

 is symmetric with respect to 



S


{\displaystyle S}

, i.e. it does not depend on the order of the elements in the training set. Furthermore, we assume that all functions are measurable and all sets are countable.","The loss 



V


{\displaystyle V}

 of a hypothesis 



f


{\displaystyle f}

 with respect to an example 



z
=
(
x
,
y
)


{\displaystyle z=(x,y)}

 is then defined as 



V
(
f
,
z
)
=
V
(
f
(
x
)
,
y
)


{\displaystyle V(f,z)=V(f(x),y)}

.","The empirical error of 



f


{\displaystyle f}

 is 




I

S


[
f
]
=


1
n


∑
V
(
f
,

z

i


)


{\displaystyle I_{S}[f]={\frac {1}{n}}\sum V(f,z_{i})}

.","The true error of 



f


{\displaystyle f}

 is 



I
[
f
]
=


E


z


V
(
f
,
z
)


{\displaystyle I[f]=\mathbb {E} _{z}V(f,z)}

","Given a training set S of size m, we will build, for all i = 1....,m, modified training sets as follows:","




S


|

i


=
{

z

1


,
.
.
.
,
 

z

i
−
1


,
 

z

i
+
1


,
.
.
.
,
 

z

m


}


{\displaystyle S^{|i}=\{z_{1},...,\ z_{i-1},\ z_{i+1},...,\ z_{m}\}}

","




S

i


=
{

z

1


,
.
.
.
,
 

z

i
−
1


,
 

z

i




′



,
 

z

i
+
1


,
.
.
.
,
 

z

m


}


{\displaystyle S^{i}=\{z_{1},...,\ z_{i-1},\ z_{i}^{'},\ z_{i+1},...,\ z_{m}\}}

","An algorithm 



L


{\displaystyle L}

 has hypothesis stability β with respect to the loss function V if the following holds:","



∀
i
∈
{
1
,
.
.
.
,
m
}
,


E


S
,
z


[

|

V
(

f

S


,
z
)
−
V
(

f


S


|

i




,
z
)

|

]
≤
β
.


{\displaystyle \forall i\in \{1,...,m\},\mathbb {E} _{S,z}[|V(f_{S},z)-V(f_{S^{|i}},z)|]\leq \beta .}

","An algorithm 



L


{\displaystyle L}

 has point-wise hypothesis stability β with respect to the loss function V if the following holds:","



∀
i
∈
 
{
1
,
.
.
.
,
m
}
,


E


S


[

|

V
(

f

S


,

z

i


)
−
V
(

f


S


|

i




,

z

i


)

|

]
≤
β
.


{\displaystyle \forall i\in \ \{1,...,m\},\mathbb {E} _{S}[|V(f_{S},z_{i})-V(f_{S^{|i}},z_{i})|]\leq \beta .}

","An algorithm 



L


{\displaystyle L}

 has error stability β with respect to the loss function V if the following holds:","



∀
S
∈

Z

m


,
∀
i
∈
{
1
,
.
.
.
,
m
}
,

|



E


z


[
V
(

f

S


,
z
)
]
−


E


z


[
V
(

f


S


|

i




,
z
)
]

|

≤
β


{\displaystyle \forall S\in Z^{m},\forall i\in \{1,...,m\},|\mathbb {E} _{z}[V(f_{S},z)]-\mathbb {E} _{z}[V(f_{S^{|i}},z)]|\leq \beta }

","An algorithm 



L


{\displaystyle L}

 has uniform stability β with respect to the loss function V if the following holds:","



∀
S
∈

Z

m


,
∀
i
∈
{
1
,
.
.
.
,
m
}
,

sup

z
∈
Z



|

V
(

f

S


,
z
)
−
V
(

f


S

i




,
z
)

|

≤
β


{\displaystyle \forall S\in Z^{m},\forall i\in \{1,...,m\},\sup _{z\in Z}|V(f_{S},z)-V(f_{S^{i}},z)|\leq \beta }

",A probabilistic version of uniform stability β is:,"



∀
S
∈

Z

m


,
∀
i
∈
{
1
,
.
.
.
,
m
}
,


P


S


{

sup

z
∈
Z



|

V
(

f

S


,
z
)
−
V
(

f


S

i




,
z
)

|

≤
β
}
≥
1
−
δ


{\displaystyle \forall S\in Z^{m},\forall i\in \{1,...,m\},\mathbb {P} _{S}\{\sup _{z\in Z}|V(f_{S},z)-V(f_{S^{i}},z)|\leq \beta \}\geq 1-\delta }

","An algorithm is said to be stable, when the value of 



β


{\displaystyle \beta }

 decreases as 



O
(


1
m


)


{\displaystyle O({\frac {1}{m}})}

.","An algorithm 



L


{\displaystyle L}

 has CVloo stability β with respect to the loss function V if the following holds:","



∀
i
∈
{
1
,
.
.
.
,
m
}
,


P


S


{

sup

z
∈
Z



|

V
(

f

S


,

z

i


)
−
V
(

f


S


|

i




,

z

i


)

|

≤

β

C
V


}
≥
1
−

δ

C
V




{\displaystyle \forall i\in \{1,...,m\},\mathbb {P} _{S}\{\sup _{z\in Z}|V(f_{S},z_{i})-V(f_{S^{|i}},z_{i})|\leq \beta _{CV}\}\geq 1-\delta _{CV}}

",The definition of (CVloo) Stability is equivalent to Pointwise-hypothesis stability seen earlier.,"An algorithm 



L


{\displaystyle L}

 has 



E
l
o

o

e
r
r




{\displaystyle Eloo_{err}}

 stability if for each n there exists a 




β

E
L


m




{\displaystyle \beta _{EL}^{m}}

 and a 




δ

E
L


m




{\displaystyle \delta _{EL}^{m}}

 such that:","



∀
i
∈
{
1
,
.
.
.
,
m
}
,


P


S


{

|

I
[

f

S


]
−


1
m



∑

i
=
1


m


V
(

f


S


|

i




,

z

i


)

|

≤

β

E
L


m


}
≥
1
−

δ

E
L


m




{\displaystyle \forall i\in \{1,...,m\},\mathbb {P} _{S}\{|I[f_{S}]-{\frac {1}{m}}\sum _{i=1}^{m}V(f_{S^{|i}},z_{i})|\leq \beta _{EL}^{m}\}\geq 1-\delta _{EL}^{m}}

, with 




β

E
L


m




{\displaystyle \beta _{EL}^{m}}

 and 




δ

E
L


m




{\displaystyle \delta _{EL}^{m}}

 going to zero for 



n
→
∞


{\displaystyle n\rightarrow \infty }

",From Bousquet and Elisseeff (02):,"For symmetric learning algorithms with bounded loss, if the algorithm has Uniform Stability with the probabilistic definition above, then the algorithm generalizes.","Uniform Stability is a strong condition which is not met by all algorithms but is, surprisingly, met by the large and important class of Regularization algorithms. The generalization bound is given in the article.",From Mukherjee et al. (06):,"This is an important result for the foundations of learning theory, because it shows that two previously unrelated properties of an algorithm, stability and consistency, are equivalent for ERM (and certain loss functions). The generalization bound is given in the article.","This is a list of algorithms that have been shown to be stable, and the article where the associated generalization bounds are provided."
"In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into ""spam"" or ""non-spam"" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition.","In the terminology of machine learning,[1] classification is considered an instance of supervised learning, i.e. learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance.","Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type), ordinal (e.g. ""large"", ""medium"" or ""small""), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.","An algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term ""classifier"" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.","Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. Other fields may use different terminology: e.g. in community ecology, the term ""classification"" normally refers to cluster analysis, i.e. a type of unsupervised learning, rather than the supervised learning described in this article.",,,"Classification and clustering are examples of the more general problem of pattern recognition, which is the assignment of some sort of output value to a given input value. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence; etc.","A common subclass of classification is probabilistic classification. Algorithms of this nature use statistical inference to find the best class for a given instance. Unlike other algorithms, which simply output a ""best"" class, probabilistic algorithms output a probability of the instance being a member of each of the possible classes. The best class is normally then selected as the one with the highest probability. However, such an algorithm has numerous advantages over non-probabilistic classifiers:","Early work on statistical classification was undertaken by Fisher,[2][3] in the context of two-group problems, leading to Fisher's linear discriminant function as the rule for assigning a group to a new observation.[4] This early work assumed that data-values within each of the two groups had a multivariate normal distribution. The extension of this same context to more than two-groups has also been considered with a restriction imposed that the classification rule should be linear.[4][5] Later work for the multivariate normal distribution allowed the classifier to be nonlinear:[6] several classification rules can be derived based on slight different adjustments of the Mahalanobis distance, with a new observation being assigned to the group whose centre has the lowest adjusted distance from the observation.","Unlike frequentist procedures, Bayesian classification procedures provide a natural way of taking into account any available information about the relative sizes of the sub-populations associated with the different groups within the overall population.[7] Bayesian procedures tend to be computationally expensive and, in the days before Markov chain Monte Carlo computations were developed, approximations for Bayesian clustering rules were devised.[8]",Some Bayesian procedures involve the calculation of group membership probabilities: these can be viewed as providing a more informative outcome of a data analysis than a simple attribution of a single group-label to each new observation.,"Classification can be thought of as two separate problems – binary classification and multiclass classification. In binary classification, a better understood task, only two classes are involved, whereas multiclass classification involves assigning an object to one of several classes.[9] Since many classification methods have been developed specifically for binary classification, multiclass classification often requires the combined use of multiple binary classifiers.","Most algorithms describe an individual instance whose category is to be predicted using a feature vector of individual, measurable properties of the instance. Each property is termed a feature, also known in statistics as an explanatory variable (or independent variable, although features may or may not be statistically independent). Features may variously be binary (e.g. ""male"" or ""female""); categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type); ordinal (e.g. ""large"", ""medium"" or ""small""); integer-valued (e.g. the number of occurrences of a particular word in an email); or real-valued (e.g. a measurement of blood pressure). If the instance is an image, the feature values might correspond to the pixels of an image; if the instance is a piece of text, the feature values might be occurrence frequencies of different words. Some algorithms work only in terms of discrete data and require that real-valued or integer-valued data be discretized into groups (e.g. less than 5, between 5 and 10, or greater than 10)","A large number of algorithms for classification can be phrased in terms of a linear function that assigns a score to each possible category k by combining the feature vector of an instance with a vector of weights, using a dot product. The predicted category is the one with the highest score. This type of score function is known as a linear predictor function and has the following general form:","where Xi is the feature vector for instance i, βk is the vector of weights corresponding to category k, and score(Xi, k) is the score associated with assigning instance i to category k. In discrete choice theory, where instances represent people and categories represent choices, the score is considered the utility associated with person i choosing category k.",Algorithms with this basic setup are known as linear classifiers. What distinguishes them is the procedure for determining (training) the optimal weights/coefficients and the way that the score is interpreted.,Examples of such algorithms are,Examples of classification algorithms include:,Classifier performance depends greatly on the characteristics of the data to be classified. There is no single classifier that works best on all given problems (a phenomenon that may be explained by the no-free-lunch theorem). Various empirical tests have been performed to compare classifier performance and to find the characteristics of data that determine classifier performance. Determining a suitable classifier for a given problem is however still more an art than a science.,"The measures precision and recall are popular metrics used to evaluate the quality of a classification system. More recently, receiver operating characteristic (ROC) curves have been used to evaluate the tradeoff between true- and false-positive rates of classification algorithms.","As a performance metric, the uncertainty coefficient has the advantage over simple accuracy in that it is not affected by the relative sizes of the different classes. [11] Further, it will not penalize an algorithm for simply rearranging the classes.","Classification has many applications. In some of these it is employed as a data mining procedure, while in others more detailed statistical modeling is undertaken."
"Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis.[1] Statistical learning theory deals with the problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, bioinformatics and baseball.[2]",,,"The goal of learning is prediction. Learning falls into many categories, including supervised learning, unsupervised learning, online learning, and reinforcement learning. From the perspective of statistical learning theory, supervised learning is best understood.[3] Supervised learning involves learning from a training set of data. Every point in the training is an input-output pair, where the input maps to an output. The learning problem consists of inferring the function that maps between the input and the output in a predictive fashion, such that the learned function can be used to predict output from future input.","Depending of the type of output, supervised learning problems are either problems of regression or problems of classification. If the output takes a continuous range of values, it is a regression problem. Using Ohm's Law as an example, a regression could be performed with voltage as input and current as output. The regression would find the functional relationship between voltage and current to be 





1
R




{\displaystyle {\frac {1}{R}}}

, such that","Classification problems are those for which the output will be an element from a discrete set of labels. Classification is very common for machine learning applications. In facial recognition, for instance, a picture of a person's face would be the input, and the output label would be that person's name. The input would be represented by a large multidimensional vector whose elements represent pixels in the picture.","After learning a function based on the training set data, that function is validated on a test set of data, data that did not appear in the training set.","Take 



X


{\displaystyle X}

 to be the vector space of all possible inputs, and 



Y


{\displaystyle Y}

 to be the vector space of all possible outputs. Statistical learning theory takes the perspective that there is some unknown probability distribution over the product space 



Z
=
X
×
Y


{\displaystyle Z=X\times Y}

, i.e. there exists some unknown 



p
(
z
)
=
p
(



x
→



,
y
)


{\displaystyle p(z)=p({\vec {x}},y)}

. The training set is made up of 



n


{\displaystyle n}

 samples from this probability distribution, and is notated","Every 







x
→




i




{\displaystyle {\vec {x}}_{i}}

 is an input vector from the training data, and 




y

i




{\displaystyle y_{i}}

 is the output that corresponds to it.","In this formalism, the inference problem consists of finding a function 



f
:
X
↦
Y


{\displaystyle f:X\mapsto Y}

 such that 



f
(



x
→



)
∼
y


{\displaystyle f({\vec {x}})\sim y}

. Let 





H




{\displaystyle {\mathcal {H}}}

 be a space of functions 



f
:
X
→
Y


{\displaystyle f:X\to Y}

 called the hypothesis space. The hypothesis space is the space of functions the algorithm will search through. Let 



V
(
f
(



x
→



)
,
y
)


{\displaystyle V(f({\vec {x}}),y)}

 be the loss functional, a metric for the difference between the predicted value 



f
(



x
→



)


{\displaystyle f({\vec {x}})}

 and the actual value 



y


{\displaystyle y}

. The expected risk is defined to be","The target function, the best possible function 



f


{\displaystyle f}

 that can be chosen, is given by the 



f


{\displaystyle f}

 that satisfies","Because the probability distribution 



p
(



x
→



,
y
)


{\displaystyle p({\vec {x}},y)}

 is unknown, a proxy measure for the expected risk must be used. This measure is based on the training set, a sample from this unknown probability distribution. It is called the empirical risk","A learning algorithm that chooses the function 




f

S




{\displaystyle f_{S}}

 that minimizes the empirical risk is called empirical risk minimization.","The choice of loss function is a determining factor on the function 




f

S




{\displaystyle f_{S}}

 that will be chosen by the learning algorithm. The loss function also affects the convergence rate for an algorithm. It is important for the loss function to be convex.[4]",Different loss functions are used depending on whether the problem is one of regression or one of classification.,The most common loss function for regression is the square loss function (also known as the L2-norm). This familiar loss function is used in ordinary least squares regression. The form is:,The absolute value loss (also known as the L1-norm) is also sometimes used:,"In some sense the 0-1 indicator function is the most natural loss function for classification. It takes the value 0 if the predicted output is the same as the actual output, and it takes the value 1 if the predicted output is different from the actual output. For binary classification with 



Y
=
{
−
1
,
1
}


{\displaystyle Y=\{-1,1\}}

, this is:","where 



θ


{\displaystyle \theta }

 is the Heaviside step function.","In machine learning problems, a major problem that arises is that of overfitting. Because learning is a prediction problem, the goal is not to find a function that most closely fits the (previously observed) data, but to find one that will most accurately predict output from future input. Empirical risk minimization runs this risk of overfitting: finding a function that matches the data exactly but does not predict future output well.","Overfitting is symptomatic of unstable solutions; a small perturbation in the training set data would cause a large variation in the learned function. It can be shown that if the stability for the solution can be guaranteed, generalization and consistency are guaranteed as well.[5][6] Regularization can solve the overfitting problem and give the problem stability.","Regularization can be accomplished by restricting the hypothesis space 





H




{\displaystyle {\mathcal {H}}}

. A common example would be restricting 





H




{\displaystyle {\mathcal {H}}}

 to linear functions: this can be seen as a reduction to the standard problem of linear regression. 





H




{\displaystyle {\mathcal {H}}}

 could also be restricted to polynomial of degree 



p


{\displaystyle p}

, exponentials, or bounded functions on L1. Restriction of the hypothesis space avoids overfitting because the form of the potential functions are limited, and so does not allow for the choice of a function that gives empirical risk arbitrarily close to zero.",One example of regularization is Tikhonov regularization. This consists of minimizing,"where 



γ


{\displaystyle \gamma }

 is a fixed and positive parameter, the regularization parameter. Tikhonov regularization ensures existence, uniqueness, and stability of the solution.[7]"
"Statistical relational learning (SRL) is a subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty; some also build upon the methods of inductive logic programming.[1] Significant contributions to the field have been made since the late 1990s.","As is evident from the characterization above, the field is not strictly limited to learning aspects; it is equally concerned with reasoning (specifically probabilistic inference) and knowledge representation. Therefore, alternative terms that reflect the main foci of the field include statistical relational learning and reasoning (emphasizing the importance of reasoning) and first-order probabilistic languages (emphasizing the key properties of the languages with which models are represented).",,,"A number of canonical tasks are associated with statistical relational learning, the most common ones being[2]","One of the fundamental design goals of the representation formalisms developed in SRL is to abstract away from concrete entities and to represent instead general principles that are intended to be universally applicable. Since there are countless ways in which such principles can be represented, many representation formalisms have been proposed in recent years.[3] In the following, some of the more common ones are listed in alphabetical order:"
"The stochastic block model is a generative model for random graphs. This model tends to produce graphs containing communities, subsets characterized by being connected with one another with particular edge densities. For example, edges may be more common within communities than between communities. The stochastic block model is important in statistics, machine learning, and network science, where it serves as a useful benchmark for the task of recovering community structure in graph data.",,,The stochastic block model takes the following parameters:,"The edge set is then sampled at random as follows: any two vertices 



u
∈

C

i




{\displaystyle u\in C_{i}}

 and 



v
∈

C

j




{\displaystyle v\in C_{j}}

 are connected by an edge with probability 




P

i
j




{\displaystyle P_{ij}}

.","If the probability matrix is a constant, in the sense that 




P

i
j


=
p


{\displaystyle P_{ij}=p}

 for all 



i
,
j


{\displaystyle i,j}

, then the result is the Erdős–Rényi model 



G
(
n
,
p
)


{\displaystyle G(n,p)}

. This case is degenerate—the partition into communities becomes irrelevant—but it illustrates a close relationship to the Erdős–Rényi model.","The planted partition model is the special case that the values of the probability matrix 



P


{\displaystyle P}

 are a constant 



p


{\displaystyle p}

 on the diagonal and another constant 



q


{\displaystyle q}

 off the diagonal. Thus two vertices within the same community share an edge with probability 



p


{\displaystyle p}

, while two vertices in different communities share an edge with probability 



q


{\displaystyle q}

. Sometimes it is this restricted model that is called the stochastic block model.[1] The case where 



p
>
q


{\displaystyle p>q}

 is called an assortative model, while the case 



p
<
q


{\displaystyle p<q}

 is called dissortative.","Returning to the general stochastic block model, a model is called strongly assortative if 




P

i
i


>

P

j
k




{\displaystyle P_{ii}>P_{jk}}

 whenever 



j
≠
k


{\displaystyle j\neq k}

: all diagonal entries dominate all off-diagonal entries. A model is called weakly assortative if 




P

i
i


>

P

i
j




{\displaystyle P_{ii}>P_{ij}}

 whenever 



i
≠
j


{\displaystyle i\neq j}

: each diagonal entry is only required to dominate the rest of its own row and column.[2] Dissortative forms of this terminology exist, by reversing all inequalities. Algorithmic recovery is often easier against block models with assortative or dissortative conditions of this form.[2]","Much of the literature on algorithmic community detection addresses three statistical tasks: detection, partial recovery, and exact recovery.","The goal of detection algorithms is simply to determine, given a sampled graph, whether the graph has latent community structure. More precisely, a graph might be generated, with some known prior probability, from a known stochastic block model, and otherwise from a similar Erdős–Rényi model. The algorithmic task is to correctly identify which of these two underlying models generated the graph.[1]","In partial recovery, the goal is to approximately determine the latent partition into communities, in the sense of finding a partition that is correlated with the true partition significantly better than a random guess.[3]","In exact recovery, the goal is to recover the latent partition into communities exactly. The community sizes and probability matrix may be known[4] or unknown.[5]","Stochastic block models exhibit a sharp threshold effect reminiscent of percolation thresholds.[1][6] Suppose that we allow the size 



n


{\displaystyle n}

 of the graph to grow, keeping the community sizes in fixed proportions. If the probability matrix remains fixed, tasks such as partial and exact recovery become feasible for all non-degenerate parameter settings. However, if we scale down the probability matrix at a suitable rate as 



n


{\displaystyle n}

 increases, we observe a sharp phase transition: for certain settings of the parameters, it will become possible to achieve recovery with probability tending to 1, whereas on the opposite side of the parameter threshold, the probability of recovery tends to 0 no matter what algorithm is used.","For partial recovery, the appropriate scaling is to take 




P

i
j


=




P
~




i
j



/

n


{\displaystyle P_{ij}={\tilde {P}}_{ij}/n}

 for fixed 






P
~





{\displaystyle {\tilde {P}}}

, resulting in graphs of constant average degree. In the case of two equal-sized communities, in the assortative planted partition model with probability matrix 



P
=

(







p
~




/

n





q
~




/

n







q
~




/

n





p
~




/

n




)

,


{\displaystyle P=\left({\begin{array}{cc}{\tilde {p}}/n&{\tilde {q}}/n\\{\tilde {q}}/n&{\tilde {p}}/n\end{array}}\right),}

 partial recovery is feasible[3] with probability 



1
−
o
(
1
)


{\displaystyle 1-o(1)}

 whenever 



(



p
~



−



q
~




)

2


>
2
(



p
~



+



q
~



)


{\displaystyle ({\tilde {p}}-{\tilde {q}})^{2}>2({\tilde {p}}+{\tilde {q}})}

, whereas any estimator fails[1] partial recovery with probability 



1
−
o
(
1
)


{\displaystyle 1-o(1)}

 whenever 



(



p
~



−



q
~




)

2


<
2
(



p
~



+



q
~



)


{\displaystyle ({\tilde {p}}-{\tilde {q}})^{2}<2({\tilde {p}}+{\tilde {q}})}

.","For exact recovery, the appropriate scaling is to take 




P

i
j


=




P
~




i
j


log
⁡
n

/

n


{\displaystyle P_{ij}={\tilde {P}}_{ij}\log n/n}

, resulting in graphs of logarithmic average degree. Here a similar threshold exists: for the assortative planted partition model with 



r


{\displaystyle r}

 equal-sized communities, the threshold lies at 







p
~




−




q
~




=


r




{\displaystyle {\sqrt {\tilde {p}}}-{\sqrt {\tilde {q}}}={\sqrt {r}}}

. In fact, the exact recovery threshold is known for the fully general stochastic block model.[4]","In principle, exact recovery can be solved in its feasible range using maximum likelihood, but this amounts to solving a constrained or regularized cut problem such as minimum bisection that is typically NP-complete. Hence, no known efficient algorithms will correctly compute the maximum-likelihood estimate in the worst case.","However, a wide variety of algorithms perform well in the average case, and many high-probability performance guarantees have been proven for algorithms in both the partial and exact recovery settings. Successful algorithms include spectral clustering of the vertices,[3][4][7] semidefinite programming,[2][6] and forms of belief propagation,[8] among others.","Several variants of the model exist. One minor tweak allocates vertices to communities randomly, according to a categorical distribution, rather than in a fixed partition.[4] More significant variants include the censored block model and the mixed-membership block model."
"Structural risk minimization (SRM) is an inductive principle of use in machine learning. Commonly in machine learning, a generalized model must be selected from a finite data set, with the consequent problem of overfitting – the model becoming too strongly tailored to the particularities of the training set and generalizing poorly to new data. The SRM principle addresses this problem by balancing the model's complexity against its success at fitting the training data.",The SRM principle was first set out in a 1974 paper by Vladimir Vapnik and Alexey Chervonenkis and uses the VC dimension.
"Structured sparsity regularization is a class of methods, and an area of research in statistical learning theory, that extend and generalize sparsity regularization learning methods.[1] Both sparsity and structured sparsity regularization methods seek to exploit the assumption that the output variable 



Y


{\displaystyle Y}

 (i.e., response, or dependent variable) to be learned can be described by a reduced number of variables in the input space 



X


{\displaystyle X}

 (i.e., the domain, space of features or explanatory variables). Sparsity regularization methods focus on selecting the input variables that best describe the output. Structured sparsity regularization methods generalize and extend sparsity regularization methods, by allowing for optimal selection over structures like groups or networks of input variables in 



X


{\displaystyle X}

.[2][3]","Common motivation for the use of structured sparsity methods are model interpretability, high-dimensional learning (where dimensionality of 



X


{\displaystyle X}

 may be higher than the number of observations 



n


{\displaystyle n}

), and reduction of computational complexity.[4] Moreover, structured sparsity methods allow to incorporate prior assumptions on the structure of the input variables, such as overlapping groups,[2] non-overlapping groups, and acyclic graphs.[3] Examples of uses of structured sparsity methods include face recognition,[5] magnetic resonance image (MRI) processing,[6] socio-linguistic analysis in natural language processing,[7] and analysis of genetic expression in breast cancer.[8]",,,"Consider the linear kernel regularized empirical risk minimization problem with a loss function 



V
(

y

i


,
f
(
x
)
)


{\displaystyle V(y_{i},f(x))}

 and the 




ℓ

0




{\displaystyle \ell _{0}}

 ""norm"" as the regularization penalty:","where 



x
,
w
∈


R

d





{\displaystyle x,w\in \mathbb {R^{d}} }

, and 



∥
w

∥

0




{\displaystyle \|w\|_{0}}

 denotes the 




ℓ

0




{\displaystyle \ell _{0}}

 ""norm"", defined as the number of nonzero entries of the vector 



w


{\displaystyle w}

. 



f
(
x
)
=
⟨
w
,

x

i


⟩


{\displaystyle f(x)=\langle w,x_{i}\rangle }

 is said to be sparse if 



∥
w

∥

0


=
s
<
d


{\displaystyle \|w\|_{0}=s<d}

. Which means that the output 



Y


{\displaystyle Y}

 can be described by a small subset of input variables.","More generally, assume a dictionary 




ϕ

j


:
X
→

R



{\displaystyle \phi _{j}:X\rightarrow \mathbb {R} }

 with 



j
=
1
,
.
.
.
,
p


{\displaystyle j=1,...,p}

 is given, such that the target function 



f
(
x
)


{\displaystyle f(x)}

 of a learning problem can be written as:","The 




ℓ

0




{\displaystyle \ell _{0}}

 norm 



∥
f

∥

0


=
∥
w

∥

0




{\displaystyle \|f\|_{0}=\|w\|_{0}}

 as the number of non-zero components of 



w


{\displaystyle w}

 is defined as","



f


{\displaystyle f}

 is said to be sparse if 



∥
f

∥

0


=
∥
w

∥

0


=
s
<
d


{\displaystyle \|f\|_{0}=\|w\|_{0}=s<d}

.","However, while using the 




ℓ

0




{\displaystyle \ell _{0}}

 norm for regularization favors sparser solutions, it is computationally difficult to use and additionally is not convex. A computationally more feasible norm that favors sparser solutions is the 




ℓ

1




{\displaystyle \ell _{1}}

 norm; this has been shown to still favor sparser solutions and is additionally convex.[4]","Structured sparsity regularization extends and generalizes the variable selection problem that characterizes sparsity regularization.[2][3] Consider the above regularized empirical risk minimization problem with a general kernel and associated feature map 




ϕ

j


:
X
→

R



{\displaystyle \phi _{j}:X\rightarrow \mathbb {R} }

 with 



j
=
1
,
.
.
.
,
p


{\displaystyle j=1,...,p}

.","The regularization term 



λ
∥
w

∥

0




{\displaystyle \lambda \|w\|_{0}}

 penalizes each 




w

j




{\displaystyle w_{j}}

 component independently, which means that the algorithm will suppress input variables independently from each other.","In several situations we may want to impose more structure in the regularization process, so that, for example, input variables are suppressed according to predefined groups. Structured sparsity regularization methods allow to impose such structure by adding structure to the norms defining the regularization term.","The non-overlapping group case is the most basic instance of structured sparsity. In it, an a priori partition of the coefficient vector 



w


{\displaystyle w}

 in 



G


{\displaystyle G}

 non-overlapping groups is assumed. Let 




w

g




{\displaystyle w_{g}}

 be the vector of coefficients in group 



g


{\displaystyle g}

, we can define a regularization term and its group norm as","where 



∥

w

g



∥

g




{\displaystyle \|w_{g}\|_{g}}

 is the group 




ℓ

2




{\displaystyle \ell _{2}}

 norm 



∥

w

g



∥

g


=



∑

j
=
1



|


G

g



|



(

w

g


j



)

2






{\displaystyle \|w_{g}\|_{g}={\sqrt {\sum _{j=1}^{|G_{g}|}(w_{g}^{j})^{2}}}}

 , 




G

g




{\displaystyle G_{g}}

 is group 



g


{\displaystyle g}

, and 




w

g


j




{\displaystyle w_{g}^{j}}

 is the j-th component of group 




G

g




{\displaystyle G_{g}}

.","The above norm is also referred to as group Lasso.[2] This regularizer will force entire coefficient groups towards zero, rather than individual coefficients. As the groups are non-overlapping, the set of non-zero coefficients can be obtained as the union of the groups that were not set to zero, and conversely for the set of zero coefficients.","Overlapping groups is the structure sparsity case where a variable can belong to more than one group 



g


{\displaystyle g}

. This case is often of interest as it can represent a more general class of relationships among variables than non-overlapping groups can, such as tree structures or other type of graphs.[3][8]","There are two types of overlapping group sparsity regularization approaches, which are used to model different types of input variable relationships:",The intersection of compliments approach is used in cases when we want to select only those input variables that have positive coefficients in all groups they belong to. Consider again the group Lasso for a regularized empirical risk minimization problem:,"where 



∥

w

g



∥

g




{\displaystyle \|w_{g}\|_{g}}

 is the group 




ℓ

2




{\displaystyle \ell _{2}}

 norm, 




G

g




{\displaystyle G_{g}}

 is group 



g


{\displaystyle g}

, and 




w

g


j




{\displaystyle w_{g}^{j}}

 is the j-th component of group 




G

g




{\displaystyle G_{g}}

.","As in the non-overlapping groups case, the group Lasso regularizer will potentially set entire groups of coefficients to zero. Selected variables are those with coefficients 




w

j


>
0


{\displaystyle w_{j}>0}

. However, as in this case groups may overlap, we take the intersection of the compliments of those groups that are not set to zero.","This intersection of complements selection criteria implies the modeling choice that we allow some coefficients within a particular group 



g


{\displaystyle g}

 to be set to zero, while others within the same group 



g


{\displaystyle g}

 may remain positive. In other words, coefficients within a group may differ depending on the several group memberships that each variable within the group may have.",A different approach is to consider union of groups for variable selection. This approach captures the modeling situation where variables can be selected as long as they belong at least to one group with positive coefficients. This modeling perspective implies that we want to preserve group structure.,"The formulation of the union of groups approach is also referred to as latent group Lasso, and requires to modify the group 




ℓ

2




{\displaystyle \ell _{2}}

 norm considered above and introduce the following regularizer [3]","where 



w
∈



R

d






{\displaystyle w\in {\mathbb {R^{d}} }}

, 




w

g


∈

G

g




{\displaystyle w_{g}\in G_{g}}

 is the vector of coefficients of group g, and 







w
¯




g


∈



R

d






{\displaystyle {\bar {w}}_{g}\in {\mathbb {R^{d}} }}

 is a vector with coefficients 




w

g


j




{\displaystyle w_{g}^{j}}

 for all variables 



j


{\displaystyle j}

 in group 



g


{\displaystyle g}

 , and 



0


{\displaystyle 0}

 in all others, i.e., 







w
¯




g


j


=

w

g


j




{\displaystyle {\bar {w}}_{g}^{j}=w_{g}^{j}}

 if 



j


{\displaystyle j}

 in group 



g


{\displaystyle g}

 and 







w
¯




g


j


=
0


{\displaystyle {\bar {w}}_{g}^{j}=0}

 otherwise.","This regularizer can be interpreted as effectively replicating variables that belong to more than one group, therefore conserving group structure. As intended by the union of groups approach, requiring 



w
=

∑

g
=
1


G






w
¯




g




{\displaystyle w=\sum _{g=1}^{G}{\bar {w}}_{g}}

 produces a vector of weights w that effectively sums up the weights of all variables across all groups they belong to.","The objective function using group lasso consists of an error function, which is generally required to be convex but not necessarily strongly convex, and a group 




ℓ

1




{\displaystyle \ell _{1}}

 regularization term. An issue with this objective function is that it is convex but not necessarily strongly convex, and thus generally does not lead to unique solutions.[9]","An example of a way to fix this is to introduce the squared 




ℓ

2




{\displaystyle \ell _{2}}

 norm of the weight vector as an additional regularization term while keeping the 




ℓ

1




{\displaystyle \ell _{1}}

 regularization term from the group lasso approach.[9] If the coefficient of the squared 




ℓ

2




{\displaystyle \ell _{2}}

 norm term is greater than 



0


{\displaystyle 0}

, then because the squared 




ℓ

2




{\displaystyle \ell _{2}}

 norm term is strongly convex, the resulting objective function will also be strongly convex.[9] Provided that the 




ℓ

2




{\displaystyle \ell _{2}}

 coefficient is suitably small but still positive, the weight vector minimizing the resulting objective function is generally very close to a weight vector that minimizes the objective function that would result from removing the group 




ℓ

2




{\displaystyle \ell _{2}}

 regularization term altogether from the original objective function; the latter scenario corresponds to the group Lasso approach.[9] Thus this approach allows for simpler optimization while maintaining sparsity.[9]",See: Submodular set function,"Besides the norms discussed above, other norms used in structured sparsity methods include hierarchical norms and norms defined on grids. These norms arise from submodular functions and allow the incorporation of prior assumptions on the structure of the input variables. In the context of hierarchical norms, this structure can be represented as a directed acyclic graph over the variables while in the context of grid-based norms, the structure can be represented using a grid.[10][11][12][13][14][15]",See: Unsupervised learning,"Unsupervised learning methods are often used to learn the parameters of latent variable models. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. Often in such models, ""hierarchies"" are assumed between the variables of the system; this system of hierarchies can be represented using directed acyclic graphs.","Hierarchies of latent variables have emerged as a natural structure in several applications, notably to model text documents.[11] Hierarchical models using Bayesian non-parametric methods have been used to learn topic models,[10] which are statistical models for discovering the abstract ""topics"" that occur in a collection of documents. Hierarchies have also been considered in the context of kernel methods.[13] Hierarchical norms have been applied to bioinformatics,[12] computer vision and topic models.[14]","If the structure assumed over variables is in the form of a 1D, 2D or 3D grid, then submodular functions based on overlapping groups can be considered as norms, leading to stable sets equal to rectangular or convex shapes.[13] Such methods have applications in computer vision[15]",The problem of choosing the best subset of input variables can be naturally formulated under a penalization framework as:[4],"Where 



∥
w

∥

0




{\displaystyle \|w\|_{0}}

 denotes the 




ℓ

0




{\displaystyle \ell _{0}}

 ""norm"", defined as the number of nonzero entries of the vector 



w


{\displaystyle w}

.","Although this formulation makes sense from a modeling perspective, it is computationally unfeasible, as it is equivalent to an exhaustive search evaluating all possible subsets of variables.[4]","Two main approaches for solving the optimization problem are: 1) greedy methods, such as step-wise regression in statistics, or matching pursuit in signal processing; and 2) convex relaxation formulation approaches and proximal gradient optimization methods.","A natural approximation for the best subset selection problem is the 




ℓ

1




{\displaystyle \ell _{1}}

 norm regularization:[4]","Such as scheme is called basis pursuit or the Lasso, which substitutes the 




ℓ

0




{\displaystyle \ell _{0}}

 ""norm"" for the convex, non-differentiable 




ℓ

1




{\displaystyle \ell _{1}}

 norm.","Proximal gradient methods, also called forward-backward splitting, are optimization methods useful for minimizing functions with a convex and differentiable component, and a convex potentially non-differentiable component.","As such, proximal gradient methods are useful for solving sparsity and structured sparsity regularization problems[9] of the following form:","Where 



V
(

y

i


,
w
,

x

i


)


{\displaystyle V(y_{i},w,x_{i})}

 is a convex and differetiable loss function like the quadratic loss, and 



R
(
w
)


{\displaystyle R(w)}

 is a convex potentially non-differentiable regularizer such as the 




ℓ

1




{\displaystyle \ell _{1}}

 norm.",Main article: Multiple kernel learning,Structured Sparsity regularization can be applied in the context of multiple kernel learning.[16] Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm.,"In the algorithms mentioned above, a whole space was taken into consideration at once and was partitioned into groups, i.e. subspaces. A complementary point of view is to consider the case in which distinct spaces are combined to obtain a new one. It is useful to discuss this idea considering finite dictionaries. Finite dictionaries with linearly independent elements - these elements are also known as atoms - refer to finite sets of linearly independent basis functions, the linear combinations of which define hypothesis spaces. Finite dictionaries can be used to define specific kernels, as will be shown.[16] Assume for this example that rather than only one dictionary, several finite dictionaries are considered.","For simplicity, the case in which there are only two dictionaries 



A
=
{

a

j


:
X
→

R

,
j
=
1
,
.
.
.
,
p
}


{\displaystyle A=\{a_{j}:X\rightarrow \mathbb {R} ,j=1,...,p\}}

 and 



B
=
{

b

t


:
X
→

R

,
t
=
1
,
.
.
.
,
q
}


{\displaystyle B=\{b_{t}:X\rightarrow \mathbb {R} ,t=1,...,q\}}

 where 



q


{\displaystyle q}

 and 



p


{\displaystyle p}

 are integers, will be considered. The atoms in 



A


{\displaystyle A}

 as well as the atoms in 



B


{\displaystyle B}

 are assumed to be linearly independent. Let 



D
=
{

d

k


:
X
→

R

,
k
=
1
,
.
.
.
,
p
+
q
}
=
A
∪
B


{\displaystyle D=\{d_{k}:X\rightarrow \mathbb {R} ,k=1,...,p+q\}=A\cup B}

 be the union of the two dictionaries. Consider the linear space of functions 



H


{\displaystyle H}

 given by linear combinations of the form","



f
(
x
)
=

∑

i
=
1


p
+
q




w

j



d

j


(
x
)

=

∑

j
=
1


p




w

A


j



a

j


(
x
)

+

∑

t
=
1


q




w

B


t



b

t


(
x
)

,
x
∈
X


{\displaystyle f(x)=\sum _{i=1}^{p+q}{w^{j}d_{j}(x)}=\sum _{j=1}^{p}{w_{A}^{j}a_{j}(x)}+\sum _{t=1}^{q}{w_{B}^{t}b_{t}(x)},x\in X}

","for some coefficient vectors 




w

A


∈


R


p


,

w

B


∈


R


q




{\displaystyle w_{A}\in \mathbb {R} ^{p},w_{B}\in \mathbb {R} ^{q}}

, where 



w
=
(

w

A


,

w

B


)


{\displaystyle w=(w_{A},w_{B})}

. Assume the atoms in 



D


{\displaystyle D}

 to still be linearly independent, or equivalently, that the map 



w
=
(

w

A


,

w

B


)
↦
f


{\displaystyle w=(w_{A},w_{B})\mapsto f}

 is one to one. The functions in the space 



H


{\displaystyle H}

 can be seen as the sums of two components, one in the space 




H

A




{\displaystyle H_{A}}

, the linear combinations of atoms in 



A


{\displaystyle A}

 and one in 




H

B




{\displaystyle H_{B}}

, the linear combinations of the atoms in 



B


{\displaystyle B}

.","One choice of norm on this space is 




|


|

f

|


|

=

|


|


w

A



|


|

+

|


|


w

B



|


|



{\displaystyle ||f||=||w_{A}||+||w_{B}||}

. Note that we can now view 



H


{\displaystyle H}

 as a function space in which 




H

A




{\displaystyle H_{A}}

, 




H

B




{\displaystyle H_{B}}

 are subspaces. In view of the linear independence assumption, 



H


{\displaystyle H}

 can be identified with 





R


p
+
q




{\displaystyle \mathbb {R} ^{p+q}}

 and 




H

A


,

H

B




{\displaystyle H_{A},H_{B}}

 with 





R


p


,


R


q




{\displaystyle \mathbb {R} ^{p},\mathbb {R} ^{q}}

 respectively. The norm mentioned above can be seen as the group norm in 



H


{\displaystyle H}

associated to the subspaces 




H

A




{\displaystyle H_{A}}

, 




H

B




{\displaystyle H_{B}}

, providing a connection to structured sparsity regularization.","Here, 




H

A




{\displaystyle H_{A}}

, 




H

B




{\displaystyle H_{B}}

 and 



H


{\displaystyle H}

 can be seen to be the reproducing kernel Hilbert spaces with corresponding feature maps 




Φ

A


:
X
→


R


p




{\displaystyle \Phi _{A}:X\rightarrow \mathbb {R} ^{p}}

, given by 




Φ

A


(
x
)
=
(

a

1


(
x
)
,
.
.
.
,

a

p


(
x
)
)


{\displaystyle \Phi _{A}(x)=(a_{1}(x),...,a_{p}(x))}

, 




Φ

B


:
X
→


R


q




{\displaystyle \Phi _{B}:X\rightarrow \mathbb {R} ^{q}}

, given by 




Φ

B


(
x
)
=
(

b

1


(
x
)
,
.
.
.
,

b

q


(
x
)
)


{\displaystyle \Phi _{B}(x)=(b_{1}(x),...,b_{q}(x))}

, and 



Φ
:
X
→


R


p
+
q




{\displaystyle \Phi :X\rightarrow \mathbb {R} ^{p+q}}

, given by the concatenation of 




Φ

A


,

Φ

B




{\displaystyle \Phi _{A},\Phi _{B}}

, respectively.","In the structured sparsity regularization approach to this scenario, the relevant groups of variables which the group norms consider correspond to the subspaces 




H

A




{\displaystyle H_{A}}

 and 




H

B




{\displaystyle H_{B}}

. This approach promotes setting the groups of coefficients corresponding to these subspaces to zero as opposed to only individual coefficients, promoting sparse multiple kernel learning.","The above reasoning directly generalizes to any finite number of dictionaries, or feature maps. It can be extended to feature maps inducing infinite dimensional hypothesis",spaces.[16],Considering sparse multiple kernel learning is useful in several situations including the following:,• Data fusion: When each kernel corresponds to a different kind of modality/feature.,"• Nonlinear variable selection: Consider kernels 




K

g




{\displaystyle K_{g}}

 depending only one dimension of the input.",Generally sparse multiple kernel learning is particularly useful when there are many kernels and model selection and interpretability are important.[16],Structured sparsity regularization methods have been used in a number of settings where it is desired to impose an a priori input variable structure to the regularization process. Some such applications are:
"In computational learning theory in mathematics, given a class of concepts C, a subclass D is reachable if there exists a partial approximation S of some concept such that D contains exactly those concepts in C that are extensions to S (i.e., D=C|S).",
"Supervised learning is the machine learning task of inferring a function from labeled training data.,.[1] The training data consist of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias).",The parallel task in human and animal psychology is often referred to as concept learning.,,,"In order to solve a given problem of supervised learning, one has to perform the following steps:","A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).",There are four major issues to consider in supervised learning:,"A first issue is the tradeoff between bias and variance.[2] Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input 



x


{\displaystyle x}

 if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for 



x


{\displaystyle x}

. A learning algorithm has high variance for a particular input 



x


{\displaystyle x}

 if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm.[3] Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be ""flexible"" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).","The second issue is the amount of training data available relative to the complexity of the ""true"" function (classifier or regression function). If the true function is simple, then an ""inflexible"" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be learnable from a very large amount of training data and using a ""flexible"" learning algorithm with low bias and high variance. Good learning algorithms therefore automatically adjust the bias/variance tradeoff based on the amount of data available and the apparent complexity of the function to be learned.","A third issue is the dimensionality of the input space. If the input feature vectors have very high dimension, the learning problem can be difficult even if the true function only depends on a small number of those features. This is because the many ""extra"" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, high input dimensionality typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, this is likely to improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.","A fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation that part of the target function that cannot be modeled ""corrupts"" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.","In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.[4][5]",Other factors to consider when choosing and applying a learning algorithm include the following:,"When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.","The most widely used learning algorithms are Support Vector Machines, linear regression, logistic regression, naive Bayes, linear discriminant analysis, decision trees, k-nearest neighbor algorithm, and Neural Networks (Multilayer perceptron).","Given a set of 



N


{\displaystyle N}

 training examples of the form 



{
(

x

1


,

y

1


)
,
.
.
.
,
(

x

N


,


y

N


)
}


{\displaystyle \{(x_{1},y_{1}),...,(x_{N},\;y_{N})\}}

 such that 




x

i




{\displaystyle x_{i}}

 is the feature vector of the i-th example and 




y

i




{\displaystyle y_{i}}

 is its label (i.e., class), a learning algorithm seeks a function 



g
:
X
→
Y


{\displaystyle g:X\to Y}

, where 



X


{\displaystyle X}

 is the input space and 



Y


{\displaystyle Y}

 is the output space. The function 



g


{\displaystyle g}

 is an element of some space of possible functions 



G


{\displaystyle G}

, usually called the hypothesis space. It is sometimes convenient to represent 



g


{\displaystyle g}

 using a scoring function 



f
:
X
×
Y
→


R




{\displaystyle f:X\times Y\to {\mathbb {R}}}

 such that 



g


{\displaystyle g}

 is defined as returning the 



y


{\displaystyle y}

 value that gives the highest score: 



g
(
x
)
=
arg
⁡

max

y



f
(
x
,
y
)


{\displaystyle g(x)=\arg \max _{y}\;f(x,y)}

. Let 



F


{\displaystyle F}

 denote the space of scoring functions.","Although 



G


{\displaystyle G}

 and 



F


{\displaystyle F}

 can be any space of functions, many learning algorithms are probabilistic models where 



g


{\displaystyle g}

 takes the form of a conditional probability model 



g
(
x
)
=
P
(
y

|

x
)


{\displaystyle g(x)=P(y|x)}

, or 



f


{\displaystyle f}

 takes the form of a joint probability model 



f
(
x
,
y
)
=
P
(
x
,
y
)


{\displaystyle f(x,y)=P(x,y)}

. For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.","There are two basic approaches to choosing 



f


{\displaystyle f}

 or 



g


{\displaystyle g}

: empirical risk minimization and structural risk minimization.[6] Empirical risk minimization seeks the function that best fits the training data. Structural risk minimize includes a penalty function that controls the bias/variance tradeoff.","In both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, 



(

x

i


,


y

i


)


{\displaystyle (x_{i},\;y_{i})}

. In order to measure how well a function fits the training data, a loss function 



L
:
Y
×
Y
→



R



≥
0




{\displaystyle L:Y\times Y\to {\mathbb {R}}^{\geq 0}}

 is defined. For training example 



(

x

i


,


y

i


)


{\displaystyle (x_{i},\;y_{i})}

, the loss of predicting the value 






y
^





{\displaystyle {\hat {y}}}

 is 



L
(

y

i


,



y
^



)


{\displaystyle L(y_{i},{\hat {y}})}

.","The risk 



R
(
g
)


{\displaystyle R(g)}

 of function 



g


{\displaystyle g}

 is defined as the expected loss of 



g


{\displaystyle g}

. This can be estimated from the training data as","In empirical risk minimization, the supervised learning algorithm seeks the function 



g


{\displaystyle g}

 that minimizes 



R
(
g
)


{\displaystyle R(g)}

. Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find 



g


{\displaystyle g}

.","When 



g


{\displaystyle g}

 is a conditional probability distribution 



P
(
y

|

x
)


{\displaystyle P(y|x)}

 and the loss function is the negative log likelihood: 



L
(
y
,



y
^



)
=
−
log
⁡
P
(
y

|

x
)


{\displaystyle L(y,{\hat {y}})=-\log P(y|x)}

, then empirical risk minimization is equivalent to maximum likelihood estimation.","When 



G


{\displaystyle G}

 contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well. This is called overfitting.",Structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones.,"A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function 



g


{\displaystyle g}

 is a linear function of the form","A popular regularization penalty is 




∑

j



β

j


2




{\displaystyle \sum _{j}\beta _{j}^{2}}

, which is the squared Euclidean norm of the weights, also known as the 




L

2




{\displaystyle L_{2}}

 norm. Other norms include the 




L

1




{\displaystyle L_{1}}

 norm, 




∑

j



|


β

j



|



{\displaystyle \sum _{j}|\beta _{j}|}

, and the 




L

0




{\displaystyle L_{0}}

 norm, which is the number of non-zero 




β

j




{\displaystyle \beta _{j}}

s. The penalty will be denoted by 



C
(
g
)


{\displaystyle C(g)}

.","The supervised learning optimization problem is to find the function 



g


{\displaystyle g}

 that minimizes","The parameter 



λ


{\displaystyle \lambda }

 controls the bias-variance tradeoff. When 



λ
=
0


{\displaystyle \lambda =0}

, this gives empirical risk minimization with low bias and high variance. When 



λ


{\displaystyle \lambda }

 is large, the learning algorithm will have high bias and low variance. The value of 



λ


{\displaystyle \lambda }

 can be chosen empirically via cross validation.","The complexity penalty has a Bayesian interpretation as the negative log prior probability of 



g


{\displaystyle g}

, 



−
log
⁡
P
(
g
)


{\displaystyle -\log P(g)}

, in which case 



J
(
g
)


{\displaystyle J(g)}

 is the posterior probabability of 



g


{\displaystyle g}

.","The training methods described above are discriminative training methods, because they seek to find a function 



g


{\displaystyle g}

 that discriminates well between the different output values (see discriminative model). For the special case where 



f
(
x
,
y
)
=
P
(
x
,
y
)


{\displaystyle f(x,y)=P(x,y)}

 is a joint probability distribution and the loss function is the negative log likelihood 



−

∑

i


log
⁡
P
(

x

i


,

y

i


)
,


{\displaystyle -\sum _{i}\log P(x_{i},y_{i}),}

 a risk minimization algorithm is said to perform generative training, because 



f


{\displaystyle f}

 can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.",There are several ways in which the standard supervised learning problem can be generalized:
"In many areas of information science, finding predictive relationships from data is a very important task. Initial discovery of relationships is usually done with a training set while a test set and validation set are used for evaluating whether the discovered relationships hold. More formally, a training set is a set of data used to discover potentially predictive relationships. A test set is a set of data used to assess the strength and utility of a predictive relationship. Test and training sets are used in intelligent systems, machine learning, genetic programming and statistics.",,,"Regression analysis was one of the earliest such approaches to be developed. The data used to construct or discover a predictive relationship are called the training data set. Most approaches that search through training data for empirical relationships tend to overfit the data, meaning that they can identify apparent relationships in the training data that do not hold in general. A test set is a set of data that is independent of the training data, but that follows the same probability distribution as the training data. If a model fit to the training set also fits the test set well, minimal overfitting has taken place. A better fitting of the training set as opposed to the test set usually points to overfitting.","In order to avoid overfitting, when any classification parameter needs to be adjusted, it is necessary to have a validation set in addition to the training and test sets. For example if the most suitable classifier for the problem is sought, the training set is used to train the candidate algorithms, the validation set is used to compare their performances and decide which one to take, and finally, the test set is used to obtain the performance characteristics such as accuracy, sensitivity, specificity, F-measure and so on. The validation set functions as a hybrid: it is training data used by testing, but neither as part of the low-level training, nor as part of the final testing.","Most simply, part of the training set can be set aside and used as a validation set; this is known as the holdout method, and common proportions are 70%/30% training/validation. Alternatively, this process can be repeated, repeatedly partitioning the original training set into a training set and a validation set; this is known as cross-validation. These repeated partitions can be done in various ways, such as dividing into 2 equal sets and using them as training/validation and then validation/training, or repeatedly selecting a random subset as a validation set.",These can be defined as:[1][2],"The basic process of using a validation set for model selection (as part of training set, validation set, and test set) is:[3][2]","Since our goal is to find the network having the best performance on new data, the simplest approach to the comparison of different networks is to evaluate the error function using data which is independent of that used for training. Various networks are trained by minimization of an appropriate error function defined with respect to a training data set. The performance of the networks is then compared by evaluating the error function using an independent validation set, and the network having the smallest error with respect to the validation set is selected. This approach is called the hold out method. Since this procedure can itself lead to some overfitting to the validation set, the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set.","An application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error).","Sometimes the training set and validation set are referred to collectively as design set: the first part of the design set is the training set, the second part is the validation step.[4]","Another example of parameter adjustment is hierarchical classification (sometimes referred to as instance space decomposition [5]), which splits a complete multi-class problem into a set of smaller classiﬁcation problems. It serves for learning more accurate concepts due to simpler classiﬁcation boundaries in subtasks and individual feature selection procedures for subtasks. When doing classiﬁcation decomposition, the central choice is the order of combination of smaller classiﬁcation steps, called the classiﬁcation path. Depending on the application, it can be derived from the confusion matrix and, uncovering the reasons for typical errors and finding ways to prevent the system make those in the future. For example,[6] on the validation set one can see which classes are most frequently mutually confused by the system and then the instance space decomposition is done as follows: firstly, the classification is done among well recognizable classes, and the difficult to separate classes are treated as a single joint class, and finally, as a second classification step the joint class is classified into the two initially mutually confused classes.","In artificial intelligence or machine learning, a training set consists of an input vector and an answer vector, and is used together with a supervised learning method to train a knowledge database (e.g. a neural net or a naive Bayes classifier) used by an AI machine. Validation sets can be used for regularization by early stopping: stop training when the error on the validation set increases, as this is a sign of overfitting to the training set.[7]","This simple procedure is complicated in practice by the fact that the validation error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when overfitting has truly begun.[7]","In statistical modeling, a training set is used to fit a model that can be used to predict a ""response value"" from one or more ""predictors."" The fitting can include both variable selection and parameter estimation. Statistical models used for prediction are often called regression models, of which linear regression and logistic regression are two examples.","In these fields, a major emphasis is placed on avoiding overfitting, so as to achieve the best possible performance on an independent test set that follows the same probability distribution as the training set.","In general, an intelligent system consists of a function taking one or more arguments and results in an output vector, and the learning method's task is to run the system once with the input vector as the arguments, calculating the output vector, comparing it with the answer vector and then changing somewhat in order to get an output vector more like the answer vector next time the system is simulated.",
"This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events are included.",,,
"In logic, statistical inference, and supervised learning, transduction or transductive inference is reasoning from observed, specific (training) cases to specific (test) cases. In contrast, induction is reasoning from observed training cases to general rules, which are then applied to the test cases. The distinction is most interesting in cases where the predictions of the transductive model are not achievable by any inductive model. Note that this is caused by transductive inference on different test sets producing mutually inconsistent predictions.","Transduction was introduced by Vladimir Vapnik in the 1990s, motivated by his view that transduction is preferable to induction since, according to him, induction requires solving a more general problem (inferring a function) before solving a more specific problem (computing outputs for new cases): ""When solving a problem of interest, do not solve a more general problem as an intermediate step. Try to get the answer that you really need but not a more general one."" A similar observation had been made earlier by Bertrand Russell: ""we shall reach the conclusion that Socrates is mortal with a greater approach to certainty if we make our argument purely inductive than if we go by way of 'all men are mortal' and then use deduction"" (Russell 1912, chap VII).","An example of learning which is not inductive would be in the case of binary classification, where the inputs tend to cluster in two groups. A large set of test inputs may help in finding the clusters, thus providing useful information about the classification labels. The same predictions would not be obtainable from a model which induces a function based only on the training cases. Some people may call this an example of the closely related semi-supervised learning, since Vapnik's motivation is quite different. An example of an algorithm in this category is the Transductive Support Vector Machine (TSVM).","A third possible motivation which leads to transduction arises through the need to approximate. If exact inference is computationally prohibitive, one may at least try to make sure that the approximations are good at the test inputs. In this case, the test inputs could come from an arbitrary distribution (not necessarily related to the distribution of the training inputs), which wouldn't be allowed in semi-supervised learning. An example of an algorithm falling in this category is the Bayesian Committee Machine (BCM).",,,The following example problem contrasts some of the unique properties of transduction against induction.,,"A collection of points is given, such that some of the points are labeled (A, B, or C), but most of the points are unlabeled (?). The goal is to predict appropriate labels for all of the unlabeled points.","The inductive approach to solving this problem is to use the labeled points to train a supervised learning algorithm, and then have it predict labels for all of the unlabeled points. With this problem, however, the supervised learning algorithm will only have five labeled points to use as a basis for building a predictive model. It will certainly struggle to build a model that captures the structure of this data. For example, if a nearest-neighbor algorithm is used, then the points near the middle will be labeled ""A"" or ""C"", even though it is apparent that they belong to the same cluster as the point labeled ""B"".","Transduction has the advantage of being able to consider all of the points, not just the labeled points, while performing the labeling task. In this case, transductive algorithms would label the unlabeled points according to the clusters to which they naturally belong. The points in the middle, therefore, would most likely be labeled ""B"", because they are packed very close to that cluster.","An advantage of transduction is that it may be able to make better predictions with fewer labeled points, because it uses the natural breaks found in the unlabeled points. One disadvantage of transduction is that it builds no predictive model. If a previously unknown point is added to the set, the entire transductive algorithm would need to be repeated with all of the points in order to predict a label. This can be computationally expensive if the data is made available incrementally in a stream. Further, this might cause the predictions of some of the old points to change (which may be good or bad, depending on the application). A supervised learning algorithm, on the other hand, can label new points instantly, with very little computational cost.","Transduction algorithms can be broadly divided into two categories: those that seek to assign discrete labels to unlabeled points, and those that seek to regress continuous labels for unlabeled points. Algorithms that seek to predict discrete labels tend to be derived by adding partial supervision to a clustering algorithm. These can be further subdivided into two categories: those that cluster by partitioning, and those that cluster by agglomerating. Algorithms that seek to predict continuous labels tend to be derived by adding partial supervision to a manifold learning algorithm.",Partitioning transduction can be thought of as top-down transduction. It is a semi-supervised extension of partition-based clustering. It is typically performed as follows:,"Of course, any reasonable partitioning technique could be used with this algorithm. Max flow min cut partitioning schemes are very popular for this purpose.",Agglomerative transduction can be thought of as bottom-up transduction. It is a semi-supervised extension of agglomerative clustering. It is typically performed as follows:,Manifold-learning-based transduction is still a very young field of research.
"Joel Bar-El
(CEO),",Trax Image Recognition is a technology company based in Singapore. Its computer vision technology is used by FMCG companies such as Coca-Cola[1] for monitoring and analysing retail shelves across large numbers of stores.,"The service reduces the time an employee needs to spend on audits to check inventory, shelf display and product promotions. It is also gathers more extensive data such as product assortment, shelf space, pricing, promotions, shelf location and arrangement of products on display. This market intelligence is valuable to FMCG manufacturers because they pay large sums for space in supermarkets and stores. For example, in the US companies pay approximately $18 billion for shelf space.","The computer vision technology is able to recognise products that are similar or identical such as branded drinks or shampoo bottles whilst also being able to differentiate between them based on variety and size. It piloted its machine learning algorithms with initial customers, allowing its algorithm to learn about different products. As the company processes more images, the better it gets at recognising the same products in different shapes and sizes.[2]","Founded in 2010, Trax has over 40 customers in the FMCG industry, including beverage giant Coca-Cola and brewer Anheuser-Busch InBev. Its service is available in 32 markets and the company's development centre is located in Tel Aviv.[2] In December 2014 it announced its fourth round of investment of US$15 million.[3]"
"The Ugly Duckling theorem is an argument asserting that classification is impossible without some sort of bias. It is named for Hans Christian Andersen's story ""The Ugly Duckling."" It gets its name because it shows that, all things being equal, an ugly duckling is just as similar to a swan as two swans are to each other, although it is only a theorem in a very informal sense. It was proposed by Satosi Watanabe in 1969.[1]",,,"Watanabe came to realize there is an unquantifiable number of shared properties between all objects, making any classification biased. Murphy and Medin (1985) give an example of two putative classified things, plums and lawnmowers:","""Suppose that one is to list the attributes that plums and lawnmowers have in common in order to judge their similarity. It is easy to see that the list could be infinite: Both weigh less than 10,000 kg (and less than 10,001 kg), both did not exist 10,000,000 years ago (and 10,000,001 years ago), both cannot hear well, both can be dropped, both take up space, and so on. Likewise, the list of differences could be infinite… any two entities can be arbitrarily similar or dissimilar by changing the criterion of what counts as a relevant attribute.""[2]","Unless some properties are considered more salient, or ‘weighted’ more important than others, everything will appear equally similar, hence Watanabe (1986) wrote: “any objects, in so far as they are distinguishable, are equally similar"".[3] However, since there is an unlimited number of properties to choose from, it remains an arbitrary choice what properties to select/deselect. This makes classification biased. Watanabe named this the ""Ugly Duckling theorem"" because a swan is as similar to a duckling as to another swan (there are no constraints or fixes on what constitutes similarity).","Suppose there are n things in the universe, and one wants to put them into classes or categories. One has no preconceived ideas or biases about what sorts of categories are ""natural"" or ""normal"" and what are not. So one has to consider all the possible classes that could be, all the possible ways of making sets out of the n objects. There are 




2

n




{\displaystyle 2^{n}}

 such ways, the size of the power set of n objects. One can use that to measure the similarity between two objects: and one would see how many sets they have in common. However one can not. Any two objects have exactly the same number of classes in common if we can form any possible class, namely 




2

n
−
1




{\displaystyle 2^{n-1}}

 (half the total number of classes there are). To see this is so, one may imagine each class is a represented by an n-bit string (or binary encoded integer), with a zero for each element not in the class and a one for each element in the class. As one finds, there are 




2

n




{\displaystyle 2^{n}}

 such strings.","As all possible choices of zeros and ones are there, any two bit-positions will agree exactly half the time. One may pick two elements and reorder the bits so they are the first two, and imagine the numbers sorted lexicographically. The first 




2

n



/

2


{\displaystyle 2^{n}/2}

 numbers will have bit #1 set to zero, and the second 




2

n



/

2


{\displaystyle 2^{n}/2}

 will have it set to one. Within each of those blocks, the top 




2

n



/

4


{\displaystyle 2^{n}/4}

 will have bit #2 set to zero and the other 




2

n



/

4


{\displaystyle 2^{n}/4}

 will have it as one, so they agree on two blocks of 




2

n



/

4


{\displaystyle 2^{n}/4}

 or on half of all the cases. No matter which two elements one picks. So if we have no preconceived bias about which categories are better, everything is then equally similar (or equally dissimilar). The number of predicates simultaneously satisfied by two non-identical elements is constant over all such pairs and is the same[citation needed] as the number of those satisfied by one. Thus, some kind of inductive[citation needed] bias is needed to make judgements; i.e. to prefer certain categories over others.","Let 




x

1


,

x

2


,
…
,

x

n




{\displaystyle x_{1},x_{2},\dots ,x_{n}}

 be a set of vectors of 



k


{\displaystyle k}

 booleans each. The ugly duckling is the vector which is least like the others. Given the booleans, this can be computed using Hamming distance.","However, the choice of boolean features to consider could have been somewhat arbitrary. Perhaps there were features derivable from the original features that were important for identifying the ugly duckling. The set of booleans in the vector can be extended with new features computed as boolean functions of the 



k


{\displaystyle k}

 original features. The only canonical way to do this is to extend it with all possible Boolean functions. The resulting completed vectors have 




2

k




{\displaystyle 2^{k}}

 features. The Ugly Duckling Theorem states that there is no ugly duckling because any two completed vectors will either be equal or differ in exactly half of the features.","Proof. Let x and y be two vectors. If they are the same, then their completed vectors must also be the same because any Boolean function of x will agree with the same Boolean function of y. If x and y are different, then there exists a coordinate 



i


{\displaystyle i}

 where the 



i


{\displaystyle i}

-th coordinate of 



x


{\displaystyle x}

 differs from the 



i


{\displaystyle i}

-th coordinate of 



y


{\displaystyle y}

. Now the completed features contain every Boolean function on 



k


{\displaystyle k}

 Boolean variables, with each one exactly once. Viewing these Boolean functions as polynomials in 



k


{\displaystyle k}

 variables over GF(2), segregate the functions into pairs 



(
f
,
g
)


{\displaystyle (f,g)}

 where 



f


{\displaystyle f}

 contains the 



i


{\displaystyle i}

-th coordinate as a linear term and 



g


{\displaystyle g}

 is 



f


{\displaystyle f}

 without that linear term. Now, for every such pair 



(
f
,
g
)


{\displaystyle (f,g)}

, 



x


{\displaystyle x}

 and 



y


{\displaystyle y}

 will agree on exactly one of the two functions. If they agree on one, they must disagree on the other and vice versa. (This proof is believed to be due to Watanabe.)","A solution to the Ugly Ducking Theorem would be to introduce a constraint on how similarity is measured by limiting the properties involved in classification, say between A and B. However Medin et al. (1993) point out that this does not actually resolve the arbitrariness or bias problem since in what respects A is similar to B: “varies with the stimulus context and task, so that there is no unique answer, to the question of how similar is one object to another”.[4][5] For example, ""a barberpole and a zebra would be more similar than a horse and a zebra if the feature striped had sufficient weight. Of course, if these feature weights were fixed, then these similarity relations would be constrained"". Yet the property ""striped"" as a weight 'fix' or constraint is arbitrary itself, meaning: ""unless one can specify such criteria, then the claim that categorization is based on attribute matching is almost entirely vacuous"".",Stamos (2003) has attempted to solve the Ugly Ducking Theorem by showing some judgments of overall similarity are non-arbitrary in the sense they are useful:,"""Presumably, people's perceptual and conceptual processes have evolved that information that matters to human needs and goals can be roughly approximated by a similarity heuristic... If you are in the jungle and you see a tiger but you decide not to stereotype (perhaps because you believe that similarity is a false friend), then you will probably be eaten. In other words, in the biological world stereotyping based on veridical judgments of overall similarity statistically results in greater survival and reproductive success.""[6]"
"In computer science, uncertain data is data that contains noise that makes it deviate from the correct, intended or original values. In the age of big data, uncertainty or data veracity is one of the defining characteristics of data. Data is constantly growing in volume, variety, velocity and uncertainty (1/veracity). Uncertain data is found in abundance today on the web, in sensor networks, within enterprises both in their structured and unstructured sources. For example, there may be uncertainty regarding the address of a customer in an enterprise dataset, or the temperature readings captured by a sensor due to aging of the sensor. In 2012 IBM called out managing uncertain data at scale in its global technology outlook report[1] that presents a comprehensive analysis looking three to ten years into the future seeking to identify significant, disruptive technologies that will change the world. In order to make confident business decisions based on real-world data, analyses must necessarily account for many different kinds of uncertainty present in very large amounts of data. Analyses based on uncertain data will have an effect on the quality of subsequent decisions, so the degree and types of inaccuracies in this uncertain data cannot be ignored.","Uncertain data is found in the area of sensor networks; text where noisy text is found in abundance on social media, web and within enterprises where the structured and unstructured data may be old, outdated, or plain incorrect; in modeling where the mathematical model may only be an approximation of the actual process. When representing such data in a database, some indication of the probability of the correctness of the various values also needs to be estimated.","There are three main models of uncertain data in databases. In attribute uncertainty, each uncertain attribute in a tuple is subject to its own independent probability distribution.[2] For example, if readings are taken of temperature and wind speed, each would be described by its own probability distribution, as knowing the reading for one measurement would not provide any information about the other.","In correlated uncertainty, multiple attributes may be described by a joint probability distribution.[2] For example, if readings are taken of the position of an object, and the x- and y-coordinates stored, the probability of different values may depend on the distance from the recorded coordinates. As distance depends on both coordinates, it may be appropriate to use a joint distribution for these coordinates, as they are not independent.","In tuple uncertainty, all the attributes of a tuple are subject to a joint probability distribution. This covers the case of correlated uncertainty, but also includes the case where there is a probability of a tuple not belonging in the relevant relation, which is indicated by all the probabilities not summing to one.[2] For example, assume we have the following tuple from a probabilistic database:","Then, the tuple has 10% chance of not existing in the database.",
"For a class of predicates 



H




{\displaystyle H\,\!}

 defined on a set 



X




{\displaystyle X\,\!}

 and a set of samples 



x
=
(

x

1


,

x

2


,
…
,

x

m


)




{\displaystyle x=(x_{1},x_{2},\dots ,x_{m})\,\!}

, where 




x

i


∈
X




{\displaystyle x_{i}\in X\,\!}

, the empirical frequency of 



h
∈
H




{\displaystyle h\in H\,\!}

 on 



x




{\displaystyle x\,\!}

 is 







Q

x


^



(
h
)
=


1
m



|

{
i
:
1
≤
i
≤
m
,
h
(

x

i


)
=
1
}

|





{\displaystyle {\widehat {Q_{x}}}(h)={\frac {1}{m}}|\{i:1\leq i\leq m,h(x_{i})=1\}|\,\!}

. The Uniform Convergence Theorem states, roughly,that if 



H




{\displaystyle H\,\!}

 is ""simple"" and we draw samples independently (with replacement) from 



X




{\displaystyle X\,\!}

 according to a distribution 



P




{\displaystyle P\,\!}

, then with high probability all the empirical frequency will be close to its expectation, where the expectation is given by 




Q

P


(
h
)
=
P
{
y
∈
X
:
h
(
y
)
=
1
}




{\displaystyle Q_{P}(h)=P\{y\in X:h(y)=1\}\,\!}

. Here ""simple"" means that the Vapnik-Chernovenkis dimension of the class 



H




{\displaystyle H\,\!}

 is small relative to the size of the sample.
In other words, a sufficiently simple collection of functions behaves roughly the same on a small random sample as it does on the distribution as a whole.",,,"If 



H




{\displaystyle H\,\!}

 is a set of 



{
0
,
1
}




{\displaystyle \{0,1\}\,\!}

-valued functions defined on a set 



X




{\displaystyle X\,\!}

 and 



P




{\displaystyle P\,\!}

 is a probability distribution on 



X




{\displaystyle X\,\!}

 then for 



ϵ
>
0




{\displaystyle \epsilon >0\,\!}

 and 



m




{\displaystyle m\,\!}

 a positive integer, we have,","where, for any 



x
∈

X

m






{\displaystyle x\in X^{m}\,\!}

,","




Π

H






{\displaystyle \Pi _{H}\,\!}

 is defined as: For any 



{
0
,
1
}




{\displaystyle \{0,1\}\,\!}

-valued functions 



H




{\displaystyle H\,\!}

 over 



X




{\displaystyle X\,\!}

 and 



D
⊆
X




{\displaystyle D\subseteq X\,\!}

,","And for any natural number 



m




{\displaystyle m\,\!}

 the shattering number 




Π

H


(
m
)




{\displaystyle \Pi _{H}(m)\,\!}

 is defined as.","From the point of Learning Theory one can consider 



H




{\displaystyle H\,\!}

 to be the Concept/Hypothesis class defined over the instance set 



X




{\displaystyle X\,\!}

. Before getting into the details of the proof of the theorem we will state Sauer's Lemma which we will need in our proof.","The Sauer–Shelah lemma[2] relates the shattering number 




Π

h


(
m
)




{\displaystyle \Pi _{h}(m)\,\!}

 to the VC Dimension.","Lemma: 




Π

H


(
m
)
≤


(



e
m

d


)


d






{\displaystyle \Pi _{H}(m)\leq \left({\frac {em}{d}}\right)^{d}\,\!}

, where 



d




{\displaystyle d\,\!}

 is the VC Dimension of the concept class 



H




{\displaystyle H\,\!}

.","Corollary: 




Π

H


(
m
)
≤

m

d






{\displaystyle \Pi _{H}(m)\leq m^{d}\,\!}

.",Before we get into the details of the proof of the Uniform Convergence Theorem we will present a high level overview of the proof.,We present the technical details of the proof.,"Lemma: Let 



V
=
{
x
∈

X

m


:

|


Q

P


(
h
)
−




Q

x


^



(
h
)

|

≥
ϵ




{\displaystyle V=\{x\in X^{m}:|Q_{P}(h)-{\widehat {Q_{x}}}(h)|\geq \epsilon \,\!}

 for some 



h
∈
H
}




{\displaystyle h\in H\}\,\!}

 and","Then for 



m
≥


2

ϵ

2








{\displaystyle m\geq {\frac {2}{\epsilon ^{2}}}\,\!}

, 




P

m


(
V
)
≤
2

P

2
m


(
R
)




{\displaystyle P^{m}(V)\leq 2P^{2m}(R)\,\!}

.","Proof: By the triangle inequality,
if 




|


Q

P


(
h
)
−




Q

r


^



(
h
)

|

≥
ϵ




{\displaystyle |Q_{P}(h)-{\widehat {Q_{r}}}(h)|\geq \epsilon \,\!}

 and 




|


Q

P


(
h
)
−




Q

s


^



(
h
)

|

≤
ϵ

/

2




{\displaystyle |Q_{P}(h)-{\widehat {Q_{s}}}(h)|\leq \epsilon /2\,\!}

 then 




|





Q

r


^



(
h
)
−




Q

s


^



(
h
)

|

≥
ϵ

/

2




{\displaystyle |{\widehat {Q_{r}}}(h)-{\widehat {Q_{s}}}(h)|\geq \epsilon /2\,\!}

.
Therefore,","Now for 



r
∈
V




{\displaystyle r\in V\,\!}

 fix an 



h
∈
H




{\displaystyle h\in H\,\!}

 such that 




|


Q

P


(
h
)
−




Q

r


^



(
h
)

|

≥
ϵ




{\displaystyle |Q_{P}(h)-{\widehat {Q_{r}}}(h)|\geq \epsilon \,\!}

. For this 



h




{\displaystyle h\,\!}

, we shall show that","Thus for any 



r
∈
V




{\displaystyle r\in V\,\!}

, 



A
≥




P

m


(
V
)

2






{\displaystyle A\geq {\frac {P^{m}(V)}{2}}\,\!}

 and hence 




P

2
m


(
R
)
≥




P

m


(
V
)

2






{\displaystyle P^{2m}(R)\geq {\frac {P^{m}(V)}{2}}\,\!}

. And hence we perform the first step of our high level idea.
Notice, 



m
⋅




Q

s


^



(
h
)




{\displaystyle m\cdot {\widehat {Q_{s}}}(h)\,\!}

 is a binomial random variable with expectation 



m
⋅

Q

P


(
h
)




{\displaystyle m\cdot Q_{P}(h)\,\!}

 and variance 



m
⋅

Q

P


(
h
)
(
1
−

Q

P


(
h
)
)




{\displaystyle m\cdot Q_{P}(h)(1-Q_{P}(h))\,\!}

. By Chebyshev's inequality we get,","Let 




Γ

m






{\displaystyle \Gamma _{m}\,\!}

 be the set of all permutations of 



{
1
,
2
,
3
,
…
,
2
m
}




{\displaystyle \{1,2,3,\dots ,2m\}\,\!}

 that swaps 



i




{\displaystyle i\,\!}

 and 



m
+
i




{\displaystyle m+i\,\!}

 



∀
i




{\displaystyle \forall i\,\!}

 in some subset of 



{
1
,
2
,
3
,
.
.
.
,
2
m
}




{\displaystyle \{1,2,3,...,2m\}\,\!}

.","Lemma: Let 



R




{\displaystyle R\,\!}

 be any subset of 




X

2
m






{\displaystyle X^{2m}\,\!}

 and 



P




{\displaystyle P\,\!}

 any probability distribution on 



X




{\displaystyle X\,\!}

. Then,","where the expectation is over 



x




{\displaystyle x\,\!}

 chosen according to 




P

2
m






{\displaystyle P^{2m}\,\!}

, and the probability is over 



σ




{\displaystyle \sigma \,\!}

 chosen uniformly from 




Γ

m






{\displaystyle \Gamma _{m}\,\!}

.","Proof: For any 



σ
∈

Γ

m


,




{\displaystyle \sigma \in \Gamma _{m},\,\!}

",The maximum is guaranteed to exist since there is only a finite set of values that probability under a random permutation can take.,"Lemma: Basing on the previous lemma,","Proof: Let us define 



x
=
(

x

1


,

x

2


,
.
.
.
,

x

2
m


)




{\displaystyle x=(x_{1},x_{2},...,x_{2m})\,\!}

 and 



t
=

|

H


|


x



|





{\displaystyle t=|H|_{x}|\,\!}

 which is atmost 




Π

H


(
2
m
)




{\displaystyle \Pi _{H}(2m)\,\!}

. This means there are functions 




h

1


,

h

2


,
.
.
.
,

h

t


∈
H




{\displaystyle h_{1},h_{2},...,h_{t}\in H\,\!}

 such that for any 



h
∈
H
,
∃
i




{\displaystyle h\in H,\exists i\,\!}

 between 



1




{\displaystyle 1\,\!}

 and 



t




{\displaystyle t\,\!}

 with 




h

i


(

x

k


)
=
h
(

x

k


)




{\displaystyle h_{i}(x_{k})=h(x_{k})\,\!}

 for 



1
≤
k
≤
2
m




{\displaystyle 1\leq k\leq 2m\,\!}

.
We see that 



σ
(
x
)
∈
R




{\displaystyle \sigma (x)\in R\,\!}

 iff for some 



h




{\displaystyle h\,\!}

 in 



H




{\displaystyle H\,\!}

 satisfies, 




|



1
m



|

{
1
≤
i
≤
m
:
h
(

x


σ

i




)
=
1
}

|

−


1
m



|

{
m
+
1
≤
i
≤
2
m
:
h
(

x


σ

i




)
=
1
}

|


|

≥


ϵ
2






{\displaystyle |{\frac {1}{m}}|\{1\leq i\leq m:h(x_{\sigma _{i}})=1\}|-{\frac {1}{m}}|\{m+1\leq i\leq 2m:h(x_{\sigma _{i}})=1\}||\geq {\frac {\epsilon }{2}}\,\!}

. Hence if we define 




w

i


j


=
1




{\displaystyle w_{i}^{j}=1\,\!}

 if 




h

j


(

x

i


)
=
1




{\displaystyle h_{j}(x_{i})=1\,\!}

 and 




w

i


j


=
0




{\displaystyle w_{i}^{j}=0\,\!}

 otherwise.
For 



1
≤
i
≤
m




{\displaystyle 1\leq i\leq m\,\!}

 and 



1
≤
j
≤
t




{\displaystyle 1\leq j\leq t\,\!}

, we have that 



σ
(
x
)
∈
R




{\displaystyle \sigma (x)\in R\,\!}

 iff for some 



j




{\displaystyle j\,\!}

 in 




1
,
.
.
.
,
t





{\displaystyle {1,...,t}\,\!}

 satisfies 




|



1
m



(

∑

i



w

σ
(
i
)


j


−

∑

i



w

σ
(
m
+
i
)


j


)


|

≥


ϵ
2






{\displaystyle |{\frac {1}{m}}\left(\sum _{i}w_{\sigma (i)}^{j}-\sum _{i}w_{\sigma (m+i)}^{j}\right)|\geq {\frac {\epsilon }{2}}\,\!}

. By union bound we get,","Since, the distribution over the permutations 



σ




{\displaystyle \sigma \,\!}

 is uniform for each 



i




{\displaystyle i\,\!}

, so 




w


σ

i




j


−

w


σ

m
+
i




j






{\displaystyle w_{\sigma _{i}}^{j}-w_{\sigma _{m+i}}^{j}\,\!}

 equals 



±

|


w

i


j


−

w

m
+
i


j



|





{\displaystyle \pm |w_{i}^{j}-w_{m+i}^{j}|\,\!}

, with equal probability.
Thus,","where the probability on the right is over 




β

i






{\displaystyle \beta _{i}\,\!}

 and both the possibilities are equally likely. By Hoeffding's inequality, this is at most 



2

e

−



m

ϵ

2



8








{\displaystyle 2e^{-{\frac {m\epsilon ^{2}}{8}}}\,\!}

.

Finally, combining all the three parts of the proof we get the Uniform Convergence Theorem."
The universal portfolio algorithm is a portfolio selection algorithm from the field of machine learning and information theory. The algorithm learns adaptively from historical data and maximizes the log-optimal growth rate in the long run. It was introduced by the late Stanford University information theorist Thomas M. Cover.[1],The algorithm rebalances the portfolio at the beginning of each trading period. At the beginning of the first trading period it starts with a naive diversification. In the following trading periods the portfolio composition depends on the historical total return of all possible constant-rebalanced portfolios.
"Unsupervised learning is the machine learning task of inferring a function to describe hidden structure from unlabeled data. Since the examples given to the learner are unlabeled, there is no error or reward signal to evaluate a potential solution. This distinguishes unsupervised learning from supervised learning and reinforcement learning.",Unsupervised learning is closely related to the problem of density estimation in statistics.[1] However unsupervised learning also encompasses many other techniques that seek to summarize and explain key features of the data.,Approaches to unsupervised learning include:,,,"The classical example of unsupervised learning in the study of both natural and artificial neural networks is subsumed by Donald Hebb's principle, that is, neurons that fire together wire together. In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons. A similar version exists that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.","Among neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are also used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing. The first version of ART was ""ART1"", developed by Carpenter and Grossberg (1988).[4]","One of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.","In particular, the method of moments is shown to be effective in learning the parameters of latent variable models.[5] Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.[5]","The Expectation–maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. Alternatively, for the method of moments, the global convergence is guaranteed under some conditions.[5]"
"User behavior analytics (""UBA"") as defined by Gartner, is a cybersecurity process about detection of insider threats, targeted attacks, and financial fraud. UBA solutions look at patterns of human behavior, and then apply algorithms and statistical analysis to detect meaningful anomalies from those patterns—anomalies that indicate potential threats.'[1][2] Instead of tracking devices or security events, UBA tracks a system's users.[3] Big data platforms like Apache Hadoop are increasing UBA functionality by allowing them to analyze petabytes worth of data to detect insider threats and advanced persistent threats.[4][5]","The problem UBA responds to, as described by Nemertes Research CEO Johna Till Johnson, is that ""Security systems provide so much information that it's tough to uncover information that truly indicates a potential for real attack. Analytics tools help make sense of the vast amount of data that SIEM, IDS/IPS, system logs, and other tools gather. UBA tools use a specialized type of security analytics that focuses on the behavior of systems and the people using them. UBA technology first evolved in the field of marketing, to help companies understand and predict consumer-buying patterns. But as it turns out, UBA can be extraordinarily useful in the security context too."" [6]","Developments in UBA technology led Gartner to also recognize user and entity behavior analytics (""UEBA""). This expanded definition includes devices, applications, servers, data, or anything with an IP address. ""When end users have been compromised, malware can lay dormant and go undetected for months. Rather than trying to find where the outsider entered, UEBAs allow for quicker detection by using algorithms to detect insider threats.""[7]","Particularly in the computer security market, there are many vendors for UEBA applications. They can be ""differentiated by whether they are designed to monitor on-premises or cloud-based software as a service (SaaS) applications; the methods in which they obtain the source data; the type of analytics they use (i.e., packaged analytics, user-driven or vendor-written), and the service delivery method (i.e., on-premises or a cloud-based).""[8] Though not intended to be an exhaustive list, representative vendors include:[9]","According to the 2015 market guide released by Gartner, ""the UEBA market grew substantially in 2015; UEBA vendors grew their customer base, market consolidation began, and Gartner client interest in UEBA and security analytics increased.""[10] The report further projected, ""Over the next three years, leading UEBA platforms will become preferred systems for security operations and investigations at some of the organizations they serve. It will be—and in some cases already is—much easier to discover some security events and analyze individual offenders in UEBA than it is in many legacy security monitoring systems.""[10]"
"In many areas of information science, finding predictive relationships from data is a very important task. Initial discovery of relationships is usually done with a training set while a test set and validation set are used for evaluating whether the discovered relationships hold. More formally, a training set is a set of data used to discover potentially predictive relationships. A test set is a set of data used to assess the strength and utility of a predictive relationship. Test and training sets are used in intelligent systems, machine learning, genetic programming and statistics.",,,"Regression analysis was one of the earliest such approaches to be developed. The data used to construct or discover a predictive relationship are called the training data set. Most approaches that search through training data for empirical relationships tend to overfit the data, meaning that they can identify apparent relationships in the training data that do not hold in general. A test set is a set of data that is independent of the training data, but that follows the same probability distribution as the training data. If a model fit to the training set also fits the test set well, minimal overfitting has taken place. A better fitting of the training set as opposed to the test set usually points to overfitting.","In order to avoid overfitting, when any classification parameter needs to be adjusted, it is necessary to have a validation set in addition to the training and test sets. For example if the most suitable classifier for the problem is sought, the training set is used to train the candidate algorithms, the validation set is used to compare their performances and decide which one to take, and finally, the test set is used to obtain the performance characteristics such as accuracy, sensitivity, specificity, F-measure and so on. The validation set functions as a hybrid: it is training data used by testing, but neither as part of the low-level training, nor as part of the final testing.","Most simply, part of the training set can be set aside and used as a validation set; this is known as the holdout method, and common proportions are 70%/30% training/validation. Alternatively, this process can be repeated, repeatedly partitioning the original training set into a training set and a validation set; this is known as cross-validation. These repeated partitions can be done in various ways, such as dividing into 2 equal sets and using them as training/validation and then validation/training, or repeatedly selecting a random subset as a validation set.",These can be defined as:[1][2],"The basic process of using a validation set for model selection (as part of training set, validation set, and test set) is:[3][2]","Since our goal is to find the network having the best performance on new data, the simplest approach to the comparison of different networks is to evaluate the error function using data which is independent of that used for training. Various networks are trained by minimization of an appropriate error function defined with respect to a training data set. The performance of the networks is then compared by evaluating the error function using an independent validation set, and the network having the smallest error with respect to the validation set is selected. This approach is called the hold out method. Since this procedure can itself lead to some overfitting to the validation set, the performance of the selected network should be confirmed by measuring its performance on a third independent set of data called a test set.","An application of this process is in early stopping, where the candidate models are successive iterations of the same network, and training stops when the error on the validation set grows, choosing the previous model (the one with minimum error).","Sometimes the training set and validation set are referred to collectively as design set: the first part of the design set is the training set, the second part is the validation step.[4]","Another example of parameter adjustment is hierarchical classification (sometimes referred to as instance space decomposition [5]), which splits a complete multi-class problem into a set of smaller classiﬁcation problems. It serves for learning more accurate concepts due to simpler classiﬁcation boundaries in subtasks and individual feature selection procedures for subtasks. When doing classiﬁcation decomposition, the central choice is the order of combination of smaller classiﬁcation steps, called the classiﬁcation path. Depending on the application, it can be derived from the confusion matrix and, uncovering the reasons for typical errors and finding ways to prevent the system make those in the future. For example,[6] on the validation set one can see which classes are most frequently mutually confused by the system and then the instance space decomposition is done as follows: firstly, the classification is done among well recognizable classes, and the difficult to separate classes are treated as a single joint class, and finally, as a second classification step the joint class is classified into the two initially mutually confused classes.","In artificial intelligence or machine learning, a training set consists of an input vector and an answer vector, and is used together with a supervised learning method to train a knowledge database (e.g. a neural net or a naive Bayes classifier) used by an AI machine. Validation sets can be used for regularization by early stopping: stop training when the error on the validation set increases, as this is a sign of overfitting to the training set.[7]","This simple procedure is complicated in practice by the fact that the validation error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when overfitting has truly begun.[7]","In statistical modeling, a training set is used to fit a model that can be used to predict a ""response value"" from one or more ""predictors."" The fitting can include both variable selection and parameter estimation. Statistical models used for prediction are often called regression models, of which linear regression and logistic regression are two examples.","In these fields, a major emphasis is placed on avoiding overfitting, so as to achieve the best possible performance on an independent test set that follows the same probability distribution as the training set.","In general, an intelligent system consists of a function taking one or more arguments and results in an output vector, and the learning method's task is to run the system once with the input vector as the arguments, calculating the output vector, comparing it with the answer vector and then changing somewhat in order to get an output vector more like the answer vector next time the system is simulated.",
"In machine learning, the vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the gradient of the error function with respect to the current weight in each iteration of training. Traditional activation functions such as the hyperbolic tangent function have gradients in the range (−1, 1) or [0, 1), and backpropagation computes gradients by the chain rule. This has the effect of multiplying n of these small numbers to compute gradients of the ""front"" layers in an n-layer network, meaning that the gradient (error signal) decreases exponentially with n and the front layers train very slowly.","With the advent of the back-propagation algorithm in the 1970s, many researchers tried to train supervised deep artificial neural networks from scratch, initially with little success. Sepp Hochreiter's diploma thesis of 1991[1][2] formally identified the reason for this failure in the ""vanishing gradient problem"", which not only affects many-layered feedforward networks, but also recurrent neural networks. The latter are trained by unfolding them into very deep feedforward networks, where a new layer is created for each time step of an input sequence processed by the network.","When activation functions are used whose derivatives can take on larger values, one risks encountering the related exploding gradient problem.",,,"To overcome this problem, several methods were proposed. One is Jürgen Schmidhuber's multi-level hierarchy of networks (1992) pre-trained one level at a time through unsupervised learning, fine-tuned through backpropagation.[3] Here each level learns a compressed representation of the observations that is fed to the next level.","Another method is the long short term memory (LSTM) network of 1997 by Hochreiter & Schmidhuber.[4] In 2009, deep multidimensional LSTM networks demonstrated the power of deep learning with many nonlinear layers, by winning three ICDAR 2009 competitions in connected handwriting recognition, without any prior knowledge about the three different languages to be learned.[5][6]",Sven Behnke relied only on the sign of the gradient (Rprop) when training his Neural Abstraction Pyramid[7] to solve problems like image reconstruction and face localization.,"Other methods also use unsupervised pre-training to structure a neural network, making it first learn generally useful feature detectors. Then the network is trained further by supervised back-propagation to classify labeled data. The deep model of Hinton et al. (2006) involves learning the distribution of a high level representation using successive layers of binary or real-valued latent variables. It uses a restricted Boltzmann machine to model each new layer of higher level features. Each new layer guarantees an increase on the lower-bound of the log likelihood of the data, thus improving the model, if trained properly. Once sufficiently many layers have been learned the deep architecture may be used as a generative model by reproducing the data when sampling down the model (an ""ancestral pass"") from the top level feature activations.[8] Hinton reports that his models are effective feature extractors over high-dimensional, structured data.[9]","Hardware advances have meant that from 1991 to 2015, computer power (especially as delivered by GPUs) has increased around a million-fold, making standard backpropagation feasible for networks several layers deeper than when the vanishing gradient problem was recognized. Schmidhuber notes that this ""is basically what is winning many of the image recognition competitions now"", but that it ""does not really overcome the problem in a fundamental way"".[10]"
"Version space learning is a logical approach to machine learning, specifically binary classification. Version space learning algorithms search a predefined space of hypotheses, viewed as a set of logical sentences. Formally, the hypothesis space is a disjunction[1]","(i.e., either hypothesis 1 is true, or hypothesis 2, or any subset of the hypotheses 1 through n). A version space learning algorithm is presented with examples, which it will use to restrict its hypothesis space; for each example x, the hypotheses that are inconsistent with x are removed from the space.[2] This iterative refining of the hypothesis space is called the candidate elimination algorithm, the hypothesis space maintained inside the algorithm its version space.[1]",,,"In settings where there is a generality-ordering on hypotheses, it is possible to represent the version space by two sets of hypotheses: (1) the most specific consistent hypotheses, and (2) the most general consistent hypotheses, where ""consistent"" indicates agreement with observed data.","The most specific hypotheses (i.e., the specific boundary SB) cover the observed positive training examples, and as little of the remaining feature space as possible. These hypotheses, if reduced any further, exclude a positive training example, and hence become inconsistent. These minimal hypotheses essentially constitute a (pessimistic) claim that the true concept is defined just by the positive data already observed: Thus, if a novel (never-before-seen) data point is observed, it should be assumed to be negative. (I.e., if data has not previously been ruled in, then it's ruled out.)","The most general hypotheses (i.e., the general boundary GB) cover the observed positive training examples, but also cover as much of the remaining feature space without including any negative training examples. These, if enlarged any further, include a negative training example, and hence become inconsistent. These maximal hypotheses essentially constitute a (optimistic) claim that the true concept is defined just by the negative data already observed: Thus, if a novel (never-before-seen) data point is observed, it should be assumed to be positive. (I.e., if data has not previously been ruled out, then it's ruled in.)","Thus, during learning, the version space (which itself is a set – possibly infinite – containing all consistent hypotheses) can be represented by just its lower and upper bounds (maximally general and maximally specific hypothesis sets), and learning operations can be performed just on these representative sets.","After learning, classification can be performed on unseen examples by testing the hypothesis learned by the algorithm. If the example is consistent with multiple hypotheses, a majority vote rule can be applied.[1]","The notion of version spaces was introduced by Mitchell in the early 1980s[2] as a framework for understanding the basic problem of supervised learning within the context of solution search. Although the basic ""candidate elimination"" search method that accompanies the version space framework is not a popular learning algorithm, there are some practical implementations that have been developed (e.g., Sverdlik & Reynolds 1992, Hong & Tsang 1997, Dubois & Quafafou 2002).","A major drawback of version space learning is its inability to deal with noise: any pair of inconsistent examples can cause the version space to collapse, i.e., become empty, so that classification becomes impossible.[1]"
"Word2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes as its input a large corpus of text and produces a high-dimensional space (typically of several hundred dimensions), with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space.[1]",Word2vec was created by a team of researchers led by Tomas Mikolov at Google. The algorithm has been subsequently analysed and explained by other researchers[2][3] and a Bayesian version of the algorithm is proposed as well.[4],,,"Word2vec can utilize either of two model architectures to produce a distributed representation of words: continuous bag-of-words (CBOW) or continuous skip-gram. By the continuous bag-of-words architecture, the model predicts the current word by using a window of surrounding context words. The order of context words does not influence prediction (bag-of-words assumption). By the continuous skip-gram architecture, the model uses the current word to predict the surrounding window of context words. The skip-gram architecture weights nearby context words more heavily than more distant context words.[1][5] According to the authors' note,[6] CBOW is faster while skip-gram is slower but does a better job for infrequent words.",Results of word2vec training can be sensitive to parametrization. The followings are some important parameters in word2vec training.,"A Word2vec model can be trained with hierarchical softmax and/or negative sampling. To approximate the conditional log-likelihood a model seeks to maximize, the hierarchical softmax method uses a Huffman tree to reduce calculation. The negative sampling method, on the other hand, approaches to the maximization problem by minimizing the log-likelihood of sampled negative instances. According to the authors, hierarchical softmax works better for infrequent words while negative sampling works better for frequent words and better with low dimensional vectors.[6] As training epochs increase, hierarchical softmax stops being useful.[7]",High frequency words often provide little information. Words with frequency above a certain threshold may be subsampled to increase training speed. A useful range for the threshold is 1e-3 to 1e-5.[6],"Quality of word embedding increases with higher dimensionality. But after reaching some point, marginal gain will diminish.[1]","Context window determines how many words before and after a given word would be included as context words of the given word. According to the authors' note, the recommended value is 10 for skip-gram and 5 for CBOW.[6]","An extension of word2vec to construct embeddings from entire documents (rather than the individual words) has been proposed.[8] This extension is called paragraph2vec or doc2vec and has been implemented in the C, Python[9][10] and Java/Scala[11] tools (see below), with the Java and Python versions also supporting inference of document embeddings on new, unseen documents.","A method named Item2Vec[12] was introduced for scalable item-item collaborative filtering. Item2Vec is based on word2vec with minor modifications and produces low dimensional representation for items, where the affinity between items can be measured by cosine similarity.","An extension of word vectors for n-grams in biological sequences (e.g. DNA, RNA, and Proteins) for bioinformatics applications have been proposed by Asgari and Mofrad.[13] Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of machine learning in proteomics and genomics. The results presented by[13] suggest that BioVectors can characterize biological sequences in terms of biochemical and biophysical interpretations of the underlying patterns.","The reasons for successful word embedding learning in the word2vec framework are poorly understood. Goldberg and Levy point out that the word2vec objective function causes words that occur in similar contexts to have similar embeddings (as measured by cosine similarity) and note that this is in line with J. R. Firth's distributional hypothesis. However, they note that this explanation is ""very hand-wavy"" and argue that a more formal explanation would be preferable.[2]","Levy et al. (2015)[14] show that much of the superior performance of word2vec or similar embeddings in downstream tasks is not a result of the models per se, but of the choice of specific hyperparameters. Transferring these hyperparameters to more 'traditional' approaches yields similar performances in downstream tasks.",The word embedding approach is able to capture multiple different degrees of similarity between words. Mikolov et al (2013)[15] found that semantic and syntactic patterns can be reproduced using vector arithmetic. Patterns such as “Man is to Woman as Brother is to Sister” can be generated through algebraic operations on the vector representations of these words such that the vector representation of “Brother” - ”Man” + ”Woman” produces a result which is closest to the vector representation of “Sister” in the model. Such relationships can be generated for a range of semantic relations (such as Country—Capital) as well as syntactic relations (e.g. present tense—past tense),"Mikolov et al (2013)[1] develop an approach to assessing the quality of a word2vec model which draws on the semantic and syntactic patterns discussed above. They develop a set of 8869 semantic relations and 10675 syntactic relations which they use as a benchmark to test the accuracy of a model. When assessing the quality of a vector model, a user may draw on this accuracy test which is implemented in word2vec,[16] or develop their own test set which is meaningful to the corpora which make up the model. This approach offers a more challenging test than simply arguing that the words most similar to a given test word are intuitively plausible.[1]","The use of different model parameters and different corpora sizes can greatly affect the quality of a word2vec model. Accuracy can be improved in a number of ways, including the choice of model architecture (CBOW or Skip-Gram), increasing the training data set, increasing the number of vector dimensions, and increasing the window size of words considered by the algorithm. Each of these improvements comes with the cost of increased computational complexity and therefore increased model generation time.[1]","In models using large corpora and a high number of dimensions, the skip-gram model yields the highest overall accuracy, and consistently produces the highest accuracy on semantic relationships, as well as yielding the highest syntactic accuracy in most cases. However, the CBOW is less computationally expensive and yields similar accuracy results.[1]","Accuracy increases overall as the number of words used increase, and as the number of dimensions increases. Mikolov et al[1] report that doubling the amount of training data results in an equivalent increase in computational complexity as doubling the number of vector dimensions."
"Zeroth is a platform for brain-inspired computing from Qualcomm. It is based around a neural processing unit (NPU) AI accelerator chip and a software API to interact with the platform. It makes a form of machine learning known as deep learning available to mobile devices. It is used for image and sound processing, including speech recognition. The software operates locally rather than as a cloud application.[1]","Mobile chip maker Qualcomm announced in March 2015 that it would bundle the software with its next major mobile device chip, the Snapdragon 820 processor.[1]",Qualcomm demonstrated that the system could recognize human faces[1] and gestures[2] that it had seen before and detect and then search for different types of photo scenes.[1],Another potential application is to extend battery life by analyzing phone usage and powering down all or part of its capabilities without affecting the user experience.[1]
"Learning is the acquisition and development of memories and behaviors, including skills, knowledge, understanding, values, and wisdom.","This category has the following 20 subcategories, out of 20 total.
","The following 169 pages are in this category, out of 169 total. This list may not reflect recent changes (learn more).
"
"This is a hidden category. It is not shown on its member pages, unless the corresponding user preference 'Show hidden categories' is set.",This is a tracking category. It builds and maintains a list of pages primarily for the sake of the list itself. Pages are added to tracking categories through templates.,Pages are added to this category by Template:Commons category.,"This category has the following 200 subcategories, out of 203,347 total.
","The following 200 pages are in this category, out of 256,333 total. This list may not reflect recent changes (learn more).
"
"Machine learning is a branch of statistics and computer science, which studies algorithms and architectures that learn from observed facts.","This category has the following 27 subcategories, out of 27 total.
","The following 183 pages are in this category, out of 183 total. This list may not reflect recent changes (learn more).
"
"This category is within the scope of the WikiProject Statistics, a collaborative effort to improve the coverage of statistics on Wikipedia. If you would like to participate, please visit the project page or join the discussion.","Could we maybe create a sub-category Category:Machine learning algorithms and move most of the algorithms there? Maybe even with sub-categories: learning algorithms, Category:Classification algorithms and such. --87.174.70.235 (talk) 07:05, 27 October 2011 (UTC)"
