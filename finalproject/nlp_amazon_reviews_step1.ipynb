{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import bs4 as BeautifulSoup\n",
    "import numpy as np\n",
    "import itertools\n",
    "np.set_printoptions(precision=9)\n",
    "import math\n",
    "import timeit\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process the amazon review text\n",
    "\n",
    "\n",
    "path = '/home/hadoop/data_node/Data Analysis/'\n",
    "text_destination = '/home/hadoop/data_node/Data Analysis/datafiles/chunk' \n",
    "global debug \n",
    "debug = True\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path + 'item_dedup.json.gz', 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "\n",
    "def Load_Review_Chunk(path, destination, chunksize):\n",
    "    start_time = timeit.default_timer()\n",
    "    dfDict = {}\n",
    "    i = 0\n",
    "    chunk = 1\n",
    "    total_chunks = 82680000/chunksize\n",
    "    for d in parse(path):\n",
    "        dfDict[i] = d\n",
    "        i += 1\n",
    "        if i > chunksize * chunk:\n",
    "            # save via function\n",
    "            review_to_csv(dfDict, destination + str(chunk) + \".csv\")\n",
    "            time_e = int(timeit.default_timer() - start_time)\n",
    "            time_rem = (time_e * total_chunks / chunk) - time_e\n",
    "            print(\"Chunk # \", chunk, \" saved. Time elapsed: \", time_e/60, \" minutess. Time remaining: \", time_rem/60, end='\\r')\n",
    "            dfDict = {}\n",
    "            chunk += 1\n",
    "            if debug == True and chunk == 3: break\n",
    "    # last save via function if needed\n",
    "    if len(dfDict)>0:\n",
    "        review_to_csv(dfDict, destination + str(chunk) + \".csv\")\n",
    "    print(\"Last Chunk # \", chunk, \"Total time elapsed: \", int(timeit.default_timer() - start_time) / 60, \" minutes\", end='\\r')\n",
    "    return\n",
    "\n",
    "def review_to_csv(dfDict, destination):\n",
    "    # cols: reviewer_id, item_id (asin), helpful\n",
    "    df = pd.DataFrame.from_dict(dfDict, orient='index')\n",
    "    if debug == True:\n",
    "        print(df.head())\n",
    "    # col_list = ['reviewerID', 'asin', 'reviewText','overall', 'unixReviewTime']\n",
    "    df.drop(['helpful', 'reviewerName', 'summary', 'reviewTime'], axis=1, inplace = True)\n",
    "    df.to_csv(destination, header = True, sep='~', index=False)\n",
    "    return\n",
    "  \n",
    "\n",
    " \n",
    "Load_Review_Chunk(path, text_destination, 50000)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dirty_data = []\n",
    "with open(\"/home/hadoop/data_node/Data Analysis/review500kdata.txt\", \"r\") as dirty_file:\n",
    "    dirty_data = dirty_file.read().split(\"\\n\")\n",
    "\n",
    "\n",
    "# Create the empty file to append to\n",
    "with open(\"/home/hadoop/data_node/Data Analysis/cleaned_reviews.csv\", \"w\") as cleaner_file:\n",
    "    wr = csv.writer(cleaner_file, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_words(raw_review, stops):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_review) \n",
    "    #\n",
    "    # 2. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 3. Remove stop words\n",
    "    meaningful_words = [w for w in words if (not w in stops and len(w) > 2)]   \n",
    "    #\n",
    "    # 4. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))\n",
    "\n",
    "\n",
    "#  In Python, searching a set is much faster than searching\n",
    "#  a list, so convert the stop words to a set\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "reviews = []\n",
    "count = 1\n",
    "for raw_review in dirty_data:\n",
    "    reviews.append(review_to_words(raw_review, stops))\n",
    "    if count % 10 == 0:\n",
    "        print(\"current_review: \")\n",
    "        print(reviews[2])\n",
    "        print(\"Reviews processed: \", count)\n",
    "        df = pd.DataFrame(reviews)\n",
    "        df.to_csv(\"/home/hadoop/data_node/Data Analysis/cleaned_reviews.csv\", mode = 'a', header = False, sep='\\n', index=False)\n",
    "        reviews = []\n",
    "    count += 1\n",
    "    # debugging only!\n",
    "    if count == 100 and debug == True: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if debug == True:\n",
    "    check_reviews = pd.read_csv(\"/home/hadoop/data_node/Data Analysis/cleaned_reviews.csv\", sep='\\n')\n",
    "    print(check_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?pd.DataFrame.to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
