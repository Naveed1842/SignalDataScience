{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import timeit\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from numpy import nan\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/nathan/Documents/Stats/Data Analysis/\"\n",
    "path2 = \"/Users/nathan/Documents/Stats/SignalDataScience/finalproject/\"\n",
    "destination = path2 + \"datafiles/\"\n",
    "global debug \n",
    "debug = False   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process the amazon review text\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path + 'item_dedup.json.gz', 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "\n",
    "def Load_Review_Chunk(path, destination, chunksize, start_chunk, end_chunk):\n",
    "    start_time = timeit.default_timer()\n",
    "    database = destination + \"reviews.db\"\n",
    "    disk_engine = sqlalchemy.create_engine('sqlite:///' + database)\n",
    "    dfDict = {}\n",
    "    i = (start_chunk - 1) * chunksize\n",
    "    max_chunks = int(82680000/chunksize)\n",
    "    if end_chunk > max_chunks: end_chunk = max_chunks\n",
    "    chunk = start_chunk\n",
    "    total_chunks = end_chunk - start_chunk\n",
    "    for d in parse(path):\n",
    "        dfDict[i] = d\n",
    "        i += 1\n",
    "        if i >= chunksize * chunk:\n",
    "            # save via function\n",
    "            review_to_file(dfDict, destination + \"chunk\" + str(chunk), disk_engine)\n",
    "            time_e = int(timeit.default_timer() - start_time)\n",
    "            time_rem = (time_e * total_chunks / chunk) - time_e\n",
    "            print(\"\\rChunk # \", chunk, \" saved. Time elapsed: \", round(time_e/60, 3),\n",
    "                  \" minutes. Time remaining: \", round(time_rem/60,3), end = \"\")\n",
    "            dfDict = {}\n",
    "            if chunk == end_chunk: break\n",
    "            chunk += 1\n",
    "    # last save via function if needed\n",
    "    if len(dfDict)>0:\n",
    "        review_to_file(dfDict, destination + \"finalchunk\" + str(chunk), disk_engine)\n",
    "        chunk += 1\n",
    "    print(\"\")\n",
    "    print(\"Last Chunk # \", chunk, \"Total time elapsed: \", int(timeit.default_timer() - start_time) / 60, \" minutes\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_file(dfDict, destination, disk_engine):\n",
    "    # cols: reviewer_id, item_id (asin), helpful\n",
    "    df = pd.DataFrame.from_dict(dfDict, orient='index')\n",
    "    df = df_sentiment(df)\n",
    "    user_grader(df, destination, disk_engine)\n",
    "    df = df.drop(['reviewText', 'summary', 'helpful'], axis=1)\n",
    "    df = df.rename(columns={})\n",
    "    df.to_sql('review_full',\n",
    "              disk_engine,\n",
    "              if_exists='append',\n",
    "              index= False,\n",
    "              dtype={'asin':sqlalchemy.types.NVARCHAR(length=10),\n",
    "                     'overall':sqlalchemy.types.INTEGER(),\n",
    "                     'reviewTime':sqlalchemy.NVARCHAR(length=10),\n",
    "                     'reviewerID':sqlalchemy.types.NVARCHAR(length=14),\n",
    "                     'unixReviewTime':sqlalchemy.types.INTEGER(),\n",
    "                     'reviewerName':sqlalchemy.types.NVARCHAR(length=30),\n",
    "                     'vader':sqlalchemy.types.Float(precision=4, asdecimal=True)})\n",
    "    # df.to_json(destination + \".json\")\n",
    " \n",
    "    # df.to_csv(destination + \".csv\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def df_sentiment(df):\n",
    "    stops = set(nltk.corpus.stopwords.words(\"english\"))  \n",
    "    # if debug == True, only try for try_minutes\n",
    "    try_minutes = 4\n",
    "    count = 0\n",
    "    start_time = int(timeit.default_timer())\n",
    "    old_count = 1\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    df['vader'] = nan\n",
    "    for row in df.index.get_values():\n",
    "        # df.set_value(row,\"pros_text\",(review_to_words(df.ix[row, \"reviewText\"], stops)))\n",
    "        sentiment_list = 0\n",
    "        try:\n",
    "            current_text = df.ix[row, \"reviewText\"]\n",
    "        except:\n",
    "            print(row, \"row error\")\n",
    "        if len(current_text) > 5:\n",
    "            try:\n",
    "                lines_list = nltk.tokenize.sent_tokenize(current_text)\n",
    "            except:\n",
    "                lines_list = current_text\n",
    "                print(lines_list, \"text tokenization failure\")\n",
    "                print(\"\")\n",
    "            for sentence in lines_list:\n",
    "                ss = sid.polarity_scores(sentence)\n",
    "                sentiment_list += ss['compound']\n",
    "        df.set_value(row,\"vader\",sentiment_list)\n",
    "        count += 1\n",
    "        time_e = (int(timeit.default_timer()) - start_time)/60\n",
    "        if time_e >= try_minutes and debug == True:\n",
    "            # df.to_csv(path + destination_file)\n",
    "            # print(df.head())\n",
    "            print(\"Out of time. Total reviews sentiment-processed: \", count)\n",
    "            print(\"\")\n",
    "            return df\n",
    "        if count % 1000 == 0 and debug == True:\n",
    "            if old_count > 1: print(\"\", end = \"\\r\")\n",
    "            print(\"Reviews sentiment-processed: \", count, end = \"\")\n",
    "            old_count = count\n",
    "            sys.stdout.flush()\n",
    "    print(\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def user_grader(df, destination, disk_engine):\n",
    "    # columns=[\"reviewerID\", \"reviewerScore\", \"asin\"]\n",
    "    reviewerID = []\n",
    "    reviewerScore = []\n",
    "    asins = []\n",
    "    for row in df.index.get_values():\n",
    "\n",
    "        try:\n",
    "            current_ID = df.ix[row, 'reviewerID']\n",
    "            helpful = df.ix[row, 'helpful']\n",
    "            asin = df.ix[row, 'asin']\n",
    "        except:\n",
    "            print(row, \"row error\")\n",
    "        helpful_score = (10*helpful[0])-(10*(helpful[1]-helpful[0]))\n",
    "        reviewerID.append(current_ID)\n",
    "        reviewerScore.append(helpful_score)\n",
    "        asins.append(asin)\n",
    "\n",
    "    new_df = pd.DataFrame({'reviewerID':reviewerID,\n",
    "                           'reviewerScore':reviewerScore,\n",
    "                          'asin':asins})\n",
    "    new_df.to_sql('reviewer_scores',\n",
    "              disk_engine,\n",
    "              if_exists='append',\n",
    "              index = False,\n",
    "              dtype={'reviewerID':sqlalchemy.types.NVARCHAR(length=14),\n",
    "                    'reviewerScore':sqlalchemy.types.INTEGER(),\n",
    "                    'asin':sqlalchemy.types.NVARCHAR(length=10)})\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "database = destination + \"reviews.db\"\n",
    "disk_engine = sqlalchemy.create_engine('sqlite:///' + database)\n",
    "query = \"\"\"\n",
    "SELECT COUNT(*) FROM review_full\n",
    "\"\"\"\n",
    "df = pd.read_sql_query(query, disk_engine)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   COUNT(*)\n",
      "0   4090000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2001.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.head())\n",
    "2001000/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk #  4091  saved. Time elapsed:  0.03333333333333333  minutes. Time remaining:  0.14110649392976451\n",
      "Chunk #  4092  saved. Time elapsed:  0.1  minutes. Time remaining:  0.42319159335288364\n",
      "Chunk #  4093  saved. Time elapsed:  0.15  minutes. Time remaining:  0.6345956511116541\n",
      "Chunk #  4094  saved. Time elapsed:  0.2  minutes. Time remaining:  0.8458720078163166\n",
      "Chunk #  4095  saved. Time elapsed:  0.25  minutes. Time remaining:  1.057020757020757\n",
      "Chunk #  4096  saved. Time elapsed:  0.3  minutes. Time remaining:  1.2680419921875\n",
      "Chunk #  4097  saved. Time elapsed:  0.35  minutes. Time remaining:  1.4789358066878202\n",
      "Chunk #  4098  saved. Time elapsed:  0.4166666666666667  minutes. Time remaining:  1.7601065560435984\n",
      "Chunk #  4099  saved. Time elapsed:  0.5166666666666667  minutes. Time remaining:  2.1818736277140767\n",
      "Chunk #  4100  saved. Time elapsed:  0.6  minutes. Time remaining:  2.5330243902439022\n",
      "Chunk #  4101  saved. Time elapsed:  0.6833333333333333  minutes. Time remaining:  2.8839632609932537\n",
      "Chunk #  4102  saved. Time elapsed:  0.75  minutes. Time remaining:  3.1643710385177966\n",
      "Chunk #  4103  saved. Time elapsed:  0.8166666666666667  minutes. Time remaining:  3.444609635226257\n",
      "Chunk #  4104  saved. Time elapsed:  0.9  minutes. Time remaining:  3.7949561403508776\n",
      "Chunk #  4105  saved. Time elapsed:  0.9666666666666667  minutes. Time remaining:  4.07483556638246\n",
      "Chunk #  4106  saved. Time elapsed:  1.05  minutes. Time remaining:  4.42478080857282\n",
      "Chunk #  4107  saved. Time elapsed:  1.1166666666666667  minutes. Time remaining:  4.704301598896193\n",
      "Chunk #  4108  saved. Time elapsed:  1.1833333333333333  minutes. Time remaining:  4.983653846153846\n",
      "Chunk #  4109  saved. Time elapsed:  1.2833333333333334  minutes. Time remaining:  5.4031800113571835\n",
      "Chunk #  4110  saved. Time elapsed:  1.35  minutes. Time remaining:  5.682153284671533"
     ]
    }
   ],
   "source": [
    "Load_Review_Chunk(path, destination, chunksize = 1000, start_chunk = 4091, end_chunk = 25500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Make new table with just the first 20 chronological reviews for only products with at least 1000 reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
