{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "import timeit\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from numpy import nan\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/nathan/Documents/Stats/Data Analysis/\"\n",
    "path2 = \"/Users/nathan/Documents/Stats/SignalDataScience/finalproject/\"\n",
    "text_destination = path2 + \"datafiles/\"\n",
    "global debug \n",
    "debug = False   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Process the amazon review text\n",
    "\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path + 'item_dedup.json.gz', 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "\n",
    "def Load_Review_Chunk(path, destination, chunksize, start_chunk, end_chunk):\n",
    "    start_time = timeit.default_timer()\n",
    "    database = destination + \"reviews.db\"\n",
    "    disk_engine = sqlalchemy.create_engine('sqlite:///' + database)\n",
    "    dfDict = {}\n",
    "    i = 0\n",
    "    max_chunks = int(82680000/chunksize)\n",
    "    if end_chunk > max_chunks: end_chunk = max_chunks\n",
    "    chunk = start_chunk\n",
    "    total_chunks = end_chunk - start_chunk\n",
    "    for d in parse(path):\n",
    "        dfDict[i] = d\n",
    "        i += 1\n",
    "        if i >= chunksize * chunk:\n",
    "            # save via function\n",
    "            review_to_file(dfDict, destination + \"chunk\" + str(chunk), disk_engine)\n",
    "            time_e = int(timeit.default_timer() - start_time)\n",
    "            time_rem = (time_e * total_chunks / chunk) - time_e\n",
    "            sys.stdout.flush()\n",
    "            print(\"\", end=\"\\r\")\n",
    "            print(\"Chunk # \", chunk, \" saved. Time elapsed: \", time_e/60,\n",
    "                  \" minutes. Time remaining: \", time_rem/60, end = \"\")\n",
    "            dfDict = {}\n",
    "            if chunk == end_chunk: break\n",
    "            chunk += 1\n",
    "    # last save via function if needed\n",
    "    if len(dfDict)>0:\n",
    "        review_to_file(dfDict, destination + \"finalchunk\" + str(chunk), disk_engine)\n",
    "        chunk += 1\n",
    "    print(\"\")\n",
    "    print(\"Last Chunk # \", chunk, \"Total time elapsed: \", int(timeit.default_timer() - start_time) / 60, \" minutes\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_file(dfDict, destination, disk_engine):\n",
    "    # cols: reviewer_id, item_id (asin), helpful\n",
    "    df = pd.DataFrame.from_dict(dfDict, orient='index')\n",
    "    df = df_sentiment(df)\n",
    "    user_grader(df, destination, disk_engine)\n",
    "    df = df.drop(['reviewText', 'summary', 'helpful'], axis=1)\n",
    "    df = df.rename(columns={})\n",
    "    df.to_sql('review_full',\n",
    "              disk_engine,\n",
    "              if_exists='append',\n",
    "              index= False,\n",
    "              dtype={'asin':sqlalchemy.types.NVARCHAR(length=10),\n",
    "                     'overall':sqlalchemy.types.INTEGER(),\n",
    "                     'reviewTime':sqlalchemy.NVARCHAR(length=10),\n",
    "                     'reviewerID':sqlalchemy.types.NVARCHAR(length=14),\n",
    "                     'unixReviewTime':sqlalchemy.types.INTEGER(),\n",
    "                     'reviewerName':sqlalchemy.types.NVARCHAR(length=30),\n",
    "                     'vader':sqlalchemy.types.Float(precision=4, asdecimal=True)})\n",
    "    # df.to_json(destination + \".json\")\n",
    " \n",
    "    # df.to_csv(destination + \".csv\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def df_sentiment(df):\n",
    "    stops = set(nltk.corpus.stopwords.words(\"english\"))  \n",
    "    # if debug == True, only try for try_minutes\n",
    "    try_minutes = 4\n",
    "    count = 0\n",
    "    start_time = int(timeit.default_timer())\n",
    "    old_count = 1\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    df['vader'] = nan\n",
    "    for row in df.index.get_values():\n",
    "        # df.set_value(row,\"pros_text\",(review_to_words(df.ix[row, \"reviewText\"], stops)))\n",
    "        sentiment_list = 0\n",
    "        try:\n",
    "            current_text = df.ix[row, \"reviewText\"]\n",
    "        except:\n",
    "            print(row, \"row error\")\n",
    "        if len(current_text) > 5:\n",
    "            try:\n",
    "                lines_list = nltk.tokenize.sent_tokenize(current_text)\n",
    "            except:\n",
    "                lines_list = current_text\n",
    "                print(lines_list, \"text tokenization failure\")\n",
    "                print(\"\")\n",
    "            for sentence in lines_list:\n",
    "                ss = sid.polarity_scores(sentence)\n",
    "                sentiment_list += ss['compound']\n",
    "        df.set_value(row,\"vader\",sentiment_list)\n",
    "        count += 1\n",
    "        time_e = (int(timeit.default_timer()) - start_time)/60\n",
    "        if time_e >= try_minutes and debug == True:\n",
    "            # df.to_csv(path + destination_file)\n",
    "            # print(df.head())\n",
    "            print(\"Out of time. Total reviews sentiment-processed: \", count)\n",
    "            print(\"\")\n",
    "            return df\n",
    "        if count % 1000 == 0 and debug == True:\n",
    "            if old_count > 1: print(\"\", end = \"\\r\")\n",
    "            print(\"Reviews sentiment-processed: \", count, end = \"\")\n",
    "            old_count = count\n",
    "            sys.stdout.flush()\n",
    "    print(\"\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def user_grader(df, destination, disk_engine):\n",
    "    # columns=[\"reviewerID\", \"reviewerScore\", \"asin\"]\n",
    "    reviewerID = []\n",
    "    reviewerScore = []\n",
    "    asins = []\n",
    "    for row in df.index.get_values():\n",
    "\n",
    "        try:\n",
    "            current_ID = df.ix[row, 'reviewerID']\n",
    "            helpful = df.ix[row, 'helpful']\n",
    "            asin = df.ix[row, 'asin']\n",
    "        except:\n",
    "            print(row, \"row error\")\n",
    "        helpful_score = (10*helpful[0])-(10*(helpful[1]-helpful[0]))\n",
    "        reviewerID.append(current_ID)\n",
    "        reviewerScore.append(helpful_score)\n",
    "        asins.append(asin)\n",
    "\n",
    "    new_df = pd.DataFrame({'reviewerID':reviewerID,\n",
    "                           'reviewerScore':reviewerScore,\n",
    "                          'asin':asins})\n",
    "    new_df.to_sql('reviewer_scores',\n",
    "              disk_engine,\n",
    "              if_exists='append',\n",
    "              index = False,\n",
    "              dtype={'reviewerID':sqlalchemy.types.NVARCHAR(length=14),\n",
    "                    'reviewerScore':sqlalchemy.types.INTEGER(),\n",
    "                    'asin':sqlalchemy.types.NVARCHAR(length=10)})\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews sentiment-processed:  1000\n",
      "['asin', 'overall', 'reviewTime', 'reviewerID', 'unixReviewTime', 'reviewerName', 'vader']\n",
      "Chunk #  1  saved. Time elapsed:  0.03333333333333333  minutes. Time remaining:  0.26666666666666666Reviews sentiment-processed:  1000\n",
      "['asin', 'overall', 'reviewTime', 'reviewerID', 'reviewerName', 'unixReviewTime', 'vader']\n",
      "Chunk #  2  saved. Time elapsed:  0.1  minutes. Time remaining:  0.35Reviews sentiment-processed:  1000\n",
      "['asin', 'overall', 'reviewTime', 'reviewerID', 'reviewerName', 'unixReviewTime', 'vader']\n",
      "Chunk #  3  saved. Time elapsed:  0.13333333333333333  minutes. Time remaining:  0.26666666666666666Reviews sentiment-processed:  1000\n",
      "['asin', 'overall', 'reviewTime', 'reviewerID', 'reviewerName', 'unixReviewTime', 'vader']\n",
      "Chunk #  4  saved. Time elapsed:  0.18333333333333332  minutes. Time remaining:  0.22916666666666666Reviews sentiment-processed:  1000\n",
      "['asin', 'overall', 'reviewTime', 'reviewerID', 'reviewerName', 'unixReviewTime', 'vader']\n",
      "Chunk #  5  saved. Time elapsed:  0.23333333333333334  minutes. Time remaining:  0.18666666666666665Reviews sentiment-processed:  1000\n",
      "['asin', 'overall', 'reviewTime', 'reviewerID', 'reviewerName', 'unixReviewTime', 'vader']\n",
      "Chunk #  6  saved. Time elapsed:  0.26666666666666666  minutes. Time remaining:  0.13333333333333333Reviews sentiment-processed:  1000\n",
      "['asin', 'overall', 'reviewTime', 'reviewerID', 'reviewerName', 'unixReviewTime', 'vader']\n",
      "Chunk #  7  saved. Time elapsed:  0.31666666666666665  minutes. Time remaining:  0.09047619047619045Reviews sentiment-processed:  1000\n",
      "['asin', 'overall', 'reviewTime', 'reviewerID', 'reviewerName', 'unixReviewTime', 'vader']\n",
      "Chunk #  8  saved. Time elapsed:  0.38333333333333336  minutes. Time remaining:  0.04791666666666667Reviews sentiment-processed:  1000\n",
      "['asin', 'overall', 'reviewTime', 'reviewerID', 'reviewerName', 'unixReviewTime', 'vader']\n",
      "Chunk #  9  saved. Time elapsed:  0.4666666666666667  minutes. Time remaining:  0.0Reviews sentiment-processed:  1000\n",
      "['asin', 'overall', 'reviewTime', 'reviewerID', 'reviewerName', 'unixReviewTime', 'vader']\n",
      "Chunk #  10  saved. Time elapsed:  0.5333333333333333  minutes. Time remaining:  -0.05333333333333332\n",
      "Last Chunk #  10 Total time elapsed:  0.5333333333333333  minutes\n"
     ]
    }
   ],
   "source": [
    "Load_Review_Chunk(path, text_destination, chunksize = 1000, start_chunk = 1, end_chunk = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Make new table with just the first 20 chronological reviews for only products with at least 1000 reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
